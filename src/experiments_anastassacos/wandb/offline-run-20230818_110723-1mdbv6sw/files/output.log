config= {'b_value': 5.0, 'c_value': 1.0, 'd_value': 0.0, 'n_agents': 10, 'algorithm': 'reinforce', 'wandb_mode': 'offline', 'num_game_iterations': 1, 'n_epochs': 2000, 'obs_size': 4, 'action_size': 2, 'random_baseline': False, 'embedding_dim': 1, 'binary_reputation': None, 'other_reputation_threshold': 0.4, 'cooperation_threshold': 0.4, 'optuna_': 0, 'device': 'cpu', 'reputation_in_reward': 0, 'lr_actor': 0.01, 'n_hidden_act': 2, 'hidden_size_act': 16, 'batch_size': 1, 'decayRate': 0.999}
DD= tensor(1.)
Dc= tensor(6.)
Cd= tensor(0.)
CC= tensor(5.)
mv= tensor(6.)
mat= tensor([[1., 6.],
        [0., 5.]])
norm mat= tensor([[0.1667, 1.0000],
        [0.0000, 0.8333]])
is_dummy= [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Agent 0
Agent 1
Agent 2
Agent 3
Agent 4
Agent 5
Agent 6
Agent 7
Agent 8
Agent 9
==========>Epoch= 0
agent= agent_5 distrib= tensor([0.3298, 0.6702], grad_fn=<SoftmaxBackward0>)
agent= agent_4 distrib= tensor([0.4123, 0.5877], grad_fn=<SoftmaxBackward0>)
Epoch : 0 	 Measure: 1.0
==========>Epoch= 1
agent= agent_5 distrib= tensor([0.2636, 0.7364], grad_fn=<SoftmaxBackward0>)
agent= agent_7 distrib= tensor([0.6397, 0.3603], grad_fn=<SoftmaxBackward0>)
==========>Epoch= 2
agent= agent_5 distrib= tensor([0.2850, 0.7150], grad_fn=<SoftmaxBackward0>)
agent= agent_7 distrib= tensor([0.6389, 0.3611], grad_fn=<SoftmaxBackward0>)
==========>Epoch= 3
agent= agent_6 distrib= tensor([0.5294, 0.4706], grad_fn=<SoftmaxBackward0>)
agent= agent_0 distrib= tensor([0.4505, 0.5495], grad_fn=<SoftmaxBackward0>)
==========>Epoch= 4
agent= agent_6 distrib= tensor([0.4283, 0.5717], grad_fn=<SoftmaxBackward0>)
agent= agent_3 distrib= tensor([0.5697, 0.4303], grad_fn=<SoftmaxBackward0>)
==========>Epoch= 5
agent= agent_4 distrib= tensor([0.3375, 0.6625], grad_fn=<SoftmaxBackward0>)
agent= agent_3 distrib= tensor([0.6388, 0.3612], grad_fn=<SoftmaxBackward0>)
==========>Epoch= 6
agent= agent_7 distrib= tensor([0.6998, 0.3002], grad_fn=<SoftmaxBackward0>)
agent= agent_2 distrib= tensor([0.5280, 0.4720], grad_fn=<SoftmaxBackward0>)
==========>Epoch= 7
agent= agent_3 distrib= tensor([0.6550, 0.3450], grad_fn=<SoftmaxBackward0>)
agent= agent_7 distrib= tensor([0.6998, 0.3002], grad_fn=<SoftmaxBackward0>)
==========>Epoch= 8
agent= agent_0 distrib= tensor([0.3802, 0.6198], grad_fn=<SoftmaxBackward0>)
agent= agent_2 distrib= tensor([0.5894, 0.4106], grad_fn=<SoftmaxBackward0>)
==========>Epoch= 9
agent= agent_8 distrib= tensor([0.4041, 0.5959], grad_fn=<SoftmaxBackward0>)
agent= agent_5 distrib= tensor([0.3172, 0.6828], grad_fn=<SoftmaxBackward0>)
==========>Epoch= 10
agent= agent_7 distrib= tensor([0.7972, 0.2028], grad_fn=<SoftmaxBackward0>)
agent= agent_2 distrib= tensor([0.5654, 0.4346], grad_fn=<SoftmaxBackward0>)
Epoch : 10 	 Measure: 0.5
==========>Epoch= 11
agent= agent_8 distrib= tensor([0.4041, 0.5959], grad_fn=<SoftmaxBackward0>)
agent= agent_1 distrib= tensor([0.5007, 0.4993], grad_fn=<SoftmaxBackward0>)
==========>Epoch= 12
agent= agent_2 distrib= tensor([0.5302, 0.4698], grad_fn=<SoftmaxBackward0>)
agent= agent_3 distrib= tensor([0.7368, 0.2632], grad_fn=<SoftmaxBackward0>)
==========>Epoch= 13
agent= agent_7 distrib= tensor([0.7896, 0.2104], grad_fn=<SoftmaxBackward0>)
agent= agent_5 distrib= tensor([0.3327, 0.6673], grad_fn=<SoftmaxBackward0>)
==========>Epoch= 14
agent= agent_7 distrib= tensor([0.8788, 0.1212], grad_fn=<SoftmaxBackward0>)
agent= agent_8 distrib= tensor([0.3371, 0.6629], grad_fn=<SoftmaxBackward0>)
==========>Epoch= 15
agent= agent_7 distrib= tensor([0.8942, 0.1058], grad_fn=<SoftmaxBackward0>)
agent= agent_1 distrib= tensor([0.4270, 0.5730], grad_fn=<SoftmaxBackward0>)
==========>Epoch= 16
agent= agent_9 distrib= tensor([0.5648, 0.4352], grad_fn=<SoftmaxBackward0>)
agent= agent_1 distrib= tensor([0.3565, 0.6435], grad_fn=<SoftmaxBackward0>)
==========>Epoch= 17
agent= agent_1 distrib= tensor([0.3059, 0.6941], grad_fn=<SoftmaxBackward0>)
agent= agent_9 distrib= tensor([0.6002, 0.3998], grad_fn=<SoftmaxBackward0>)
==========>Epoch= 18
agent= agent_4 distrib= tensor([0.3227, 0.6773], grad_fn=<SoftmaxBackward0>)
agent= agent_8 distrib= tensor([0.2914, 0.7086], grad_fn=<SoftmaxBackward0>)
==========>Epoch= 19
agent= agent_4 distrib= tensor([0.2900, 0.7100], grad_fn=<SoftmaxBackward0>)
agent= agent_5 distrib= tensor([0.3647, 0.6353], grad_fn=<SoftmaxBackward0>)
==========>Epoch= 20
agent= agent_4 distrib= tensor([0.2638, 0.7362], grad_fn=<SoftmaxBackward0>)
agent= agent_8 distrib= tensor([0.2439, 0.7561], grad_fn=<SoftmaxBackward0>)
Epoch : 20 	 Measure: 0.20000000298023224
==========>Epoch= 21
agent= agent_9 distrib= tensor([0.6795, 0.3205], grad_fn=<SoftmaxBackward0>)
agent= agent_2 distrib= tensor([0.4686, 0.5314], grad_fn=<SoftmaxBackward0>)
==========>Epoch= 22
agent= agent_2 distrib= tensor([0.4468, 0.5532], grad_fn=<SoftmaxBackward0>)
agent= agent_3 distrib= tensor([0.7444, 0.2556], grad_fn=<SoftmaxBackward0>)
==========>Epoch= 23
agent= agent_6 distrib= tensor([0.3708, 0.6292], grad_fn=<SoftmaxBackward0>)
agent= agent_4 distrib= tensor([0.2340, 0.7660], grad_fn=<SoftmaxBackward0>)
==========>Epoch= 24
agent= agent_2 distrib= tensor([0.4288, 0.5712], grad_fn=<SoftmaxBackward0>)
agent= agent_0 distrib= tensor([0.3041, 0.6959], grad_fn=<SoftmaxBackward0>)
==========>Epoch= 25
agent= agent_7 distrib= tensor([0.8982, 0.1018], grad_fn=<SoftmaxBackward0>)
agent= agent_8 distrib= tensor([0.2039, 0.7961], grad_fn=<SoftmaxBackward0>)
==========>Epoch= 26
agent= agent_8 distrib= tensor([0.1712, 0.8288], grad_fn=<SoftmaxBackward0>)
agent= agent_5 distrib= tensor([0.3872, 0.6128], grad_fn=<SoftmaxBackward0>)
==========>Epoch= 27
agent= agent_0 distrib= tensor([0.2556, 0.7444], grad_fn=<SoftmaxBackward0>)
agent= agent_9 distrib= tensor([0.7523, 0.2477], grad_fn=<SoftmaxBackward0>)
==========>Epoch= 28
agent= agent_8 distrib= tensor([0.1364, 0.8636], grad_fn=<SoftmaxBackward0>)
agent= agent_7 distrib= tensor([0.9398, 0.0602], grad_fn=<SoftmaxBackward0>)
==========>Epoch= 29
agent= agent_8 distrib= tensor([0.1038, 0.8962], grad_fn=<SoftmaxBackward0>)
agent= agent_9 distrib= tensor([0.7816, 0.2184], grad_fn=<SoftmaxBackward0>)
==========>Epoch= 30
agent= agent_0 distrib= tensor([0.2223, 0.7777], grad_fn=<SoftmaxBackward0>)
agent= agent_8 distrib= tensor([0.0882, 0.9118], grad_fn=<SoftmaxBackward0>)
Epoch : 30 	 Measure: 0.800000011920929
==========>Epoch= 31
agent= agent_3 distrib= tensor([0.8085, 0.1915], grad_fn=<SoftmaxBackward0>)
agent= agent_9 distrib= tensor([0.8079, 0.1921], grad_fn=<SoftmaxBackward0>)
==========>Epoch= 32
agent= agent_9 distrib= tensor([0.8508, 0.1492], grad_fn=<SoftmaxBackward0>)
agent= agent_7 distrib= tensor([0.9552, 0.0448], grad_fn=<SoftmaxBackward0>)
==========>Epoch= 33
agent= agent_6 distrib= tensor([0.3278, 0.6722], grad_fn=<SoftmaxBackward0>)
agent= agent_7 distrib= tensor([0.9726, 0.0274], grad_fn=<SoftmaxBackward0>)
==========>Epoch= 34
agent= agent_5 distrib= tensor([0.4404, 0.5596], grad_fn=<SoftmaxBackward0>)
agent= agent_7 distrib= tensor([0.9757, 0.0243], grad_fn=<SoftmaxBackward0>)
==========>Epoch= 35
agent= agent_8 distrib= tensor([0.0784, 0.9216], grad_fn=<SoftmaxBackward0>)
agent= agent_9 distrib= tensor([0.8829, 0.1171], grad_fn=<SoftmaxBackward0>)
==========>Epoch= 36
agent= agent_6 distrib= tensor([0.2909, 0.7091], grad_fn=<SoftmaxBackward0>)
agent= agent_7 distrib= tensor([0.9733, 0.0267], grad_fn=<SoftmaxBackward0>)
==========>Epoch= 37
agent= agent_4 distrib= tensor([0.2127, 0.7873], grad_fn=<SoftmaxBackward0>)
agent= agent_8 distrib= tensor([0.0777, 0.9223], grad_fn=<SoftmaxBackward0>)
==========>Epoch= 38
agent= agent_8 distrib= tensor([0.0626, 0.9374], grad_fn=<SoftmaxBackward0>)
agent= agent_4 distrib= tensor([0.1763, 0.8237], grad_fn=<SoftmaxBackward0>)
==========>Epoch= 39
agent= agent_4 distrib= tensor([0.1298, 0.8702], grad_fn=<SoftmaxBackward0>)
agent= agent_5 distrib= tensor([0.4713, 0.5287], grad_fn=<SoftmaxBackward0>)
==========>Epoch= 40
agent= agent_5 distrib= tensor([0.4980, 0.5020], grad_fn=<SoftmaxBackward0>)
agent= agent_2 distrib= tensor([0.4909, 0.5091], grad_fn=<SoftmaxBackward0>)
Epoch : 40 	 Measure: 0.5
==========>Epoch= 41
agent= agent_5 distrib= tensor([0.5313, 0.4687], grad_fn=<SoftmaxBackward0>)
agent= agent_9 distrib= tensor([0.9038, 0.0962], grad_fn=<SoftmaxBackward0>)
==========>Epoch= 42
agent= agent_2 distrib= tensor([0.4515, 0.5485], grad_fn=<SoftmaxBackward0>)
agent= agent_7 distrib= tensor([0.9830, 0.0170], grad_fn=<SoftmaxBackward0>)
==========>Epoch= 43
agent= agent_9 distrib= tensor([0.9001, 0.0999], grad_fn=<SoftmaxBackward0>)
agent= agent_1 distrib= tensor([0.2734, 0.7266], grad_fn=<SoftmaxBackward0>)
==========>Epoch= 44
agent= agent_4 distrib= tensor([0.1456, 0.8544], grad_fn=<SoftmaxBackward0>)
agent= agent_9 distrib= tensor([0.8963, 0.1037], grad_fn=<SoftmaxBackward0>)
Traceback (most recent call last):
  File "caller.py", line 63, in <module>
    train_reinforce(args)
  File "/home/nicole/marl-emecom/src/experiments_anastassacos/train_reinforce.py", line 213, in train_reinforce
    objective(args, repo_name)
  File "/home/nicole/marl-emecom/src/experiments_anastassacos/train_reinforce.py", line 138, in objective
    returns_eval = eval_anast(parallel_env, active_agents, active_agents_idxs, config.num_game_iterations, social_norm, 0.99)
  File "/home/nicole/marl-emecom/src/utils/utils.py", line 92, in eval_anast
    a, d = active_agents[agent].select_action()#states[idx_agent])
  File "/home/nicole/marl-emecom/src/algos/anast/agent_anast.py", line 49, in select_action
    action, action_logprob, entropy, distrib = self.policy_act.act(state=state_to_act, greedy=False, get_distrib=True)
  File "/home/nicole/marl-emecom/src/algos/anast/Actor.py", line 41, in act
    out = self.actor(state)
  File "/home/nicole/marl-emecom/env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/nicole/marl-emecom/env1/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/home/nicole/marl-emecom/env1/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/nicole/marl-emecom/env1/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
  File "/usr/lib/python3.8/traceback.py", line 197, in format_stack
    return format_list(extract_stack(f, limit=limit))
  File "/usr/lib/python3.8/traceback.py", line 212, in extract_stack
    stack.reverse()
KeyboardInterrupt