config= {'b_value': 5.0, 'c_value': 1.0, 'd_value': 0.0, 'n_agents': 2, 'algorithm': 'reinforce', 'wandb_mode': 'offline', 'num_game_iterations': 10, 'n_epochs': 2000, 'obs_size': 4, 'action_size': 2, 'random_baseline': False, 'embedding_dim': 1, 'binary_reputation': 1, 'other_reputation_threshold': 0.4, 'cooperation_threshold': 0.4, 'optuna_': 0, 'device': 'cpu', 'reputation_in_reward': 0, 'lr_actor': 0.01, 'n_hidden_act': 2, 'hidden_size_act': 16, 'batch_size': 1, 'decayRate': 0.999}
DD= tensor(1.)
Dc= tensor(6.)
Cd= tensor(0.)
CC= tensor(5.)
mv= tensor(6.)
mat= tensor([[1., 6.],
        [0., 5.]])
norm mat= tensor([[0.1667, 1.0000],
        [0.0000, 0.8333]])
is_dummy= [0, 0]
Agent 0
Agent 1
==========>Epoch= 0
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.5910, 0.4090], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.4711, 0.5289], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.4711, 0.5289], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.5520, 0.4480], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.4751, 0.5249], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.4751, 0.5249], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 1., 0., 0.])
agent.state_act= tensor([1., 0., 0., 0.])
states= {'agent_1': tensor([0., 1., 0., 0.]), 'agent_0': tensor([1., 0., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.5563, 0.4437], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 0., 0.]), 'agent_0': tensor([1., 0., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.4886, 0.5114], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.4886, 0.5114], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 1., 1., 0.])
agent.state_act= tensor([1., 0., 0., 1.])
states= {'agent_1': tensor([0., 1., 1., 0.]), 'agent_0': tensor([1., 0., 0., 1.])}
agent.state_act= tensor([0., 1., 1., 0.])
 action, distrib= tensor(0) tensor([0.5271, 0.4729], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 0.]), 'agent_0': tensor([1., 0., 0., 1.])}
agent.state_act= tensor([1., 0., 0., 1.])
 action, distrib= tensor(0) tensor([0.4985, 0.5015], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.4985, 0.5015], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 1., 0., 0.])
agent.state_act= tensor([1., 0., 0., 0.])
states= {'agent_1': tensor([0., 1., 0., 0.]), 'agent_0': tensor([1., 0., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.5563, 0.4437], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 0., 0.]), 'agent_0': tensor([1., 0., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.4886, 0.5114], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.4886, 0.5114], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 1., 0., 0.])
agent.state_act= tensor([1., 0., 0., 0.])
states= {'agent_1': tensor([0., 1., 0., 0.]), 'agent_0': tensor([1., 0., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.5563, 0.4437], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 0., 0.]), 'agent_0': tensor([1., 0., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.4886, 0.5114], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.4886, 0.5114], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 1., 0., 0.])
agent.state_act= tensor([1., 0., 0., 0.])
states= {'agent_1': tensor([0., 1., 0., 0.]), 'agent_0': tensor([1., 0., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.5563, 0.4437], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 0., 0.]), 'agent_0': tensor([1., 0., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(1) tensor([0.4886, 0.5114], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.4886, 0.5114], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 1., 0., 0.])
agent.state_act= tensor([1., 1., 0., 0.])
states= {'agent_1': tensor([1., 1., 0., 0.]), 'agent_0': tensor([1., 1., 0., 0.])}
agent.state_act= tensor([1., 1., 0., 0.])
 action, distrib= tensor(1) tensor([0.5904, 0.4096], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 0., 0.]), 'agent_0': tensor([1., 1., 0., 0.])}
agent.state_act= tensor([1., 1., 0., 0.])
 action, distrib= tensor(1) tensor([0.4859, 0.5141], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
actions= {'agent_1': tensor(1), 'agent_0': tensor(1)}
d= tensor([0.4859, 0.5141], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 1., 1., 0.])
agent.state_act= tensor([1., 1., 0., 1.])
states= {'agent_1': tensor([1., 1., 1., 0.]), 'agent_0': tensor([1., 1., 0., 1.])}
agent.state_act= tensor([1., 1., 1., 0.])
 action, distrib= tensor(1) tensor([0.5648, 0.4352], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 0.]), 'agent_0': tensor([1., 1., 0., 1.])}
agent.state_act= tensor([1., 1., 0., 1.])
 action, distrib= tensor(1) tensor([0.4981, 0.5019], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
actions= {'agent_1': tensor(1), 'agent_0': tensor(1)}
d= tensor([0.4981, 0.5019], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.5910, 0.4090], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.4711, 0.5289], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(0.), 'agent_0': tensor(6.)}
actions= {'agent_1': tensor(1), 'agent_0': tensor(0)}
d= tensor([0.4711, 0.5289], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
UPDATE
inside update
inside update
done
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6259, 0.3741], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.4744, 0.5256], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.6259, 0.3741], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.4744, 0.5256], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6259, 0.3741], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.4744, 0.5256], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.6259, 0.3741], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.4744, 0.5256], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6259, 0.3741], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.4744, 0.5256], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6259, 0.3741], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.4744, 0.5256], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6259, 0.3741], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.4744, 0.5256], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6259, 0.3741], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.4744, 0.5256], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6259, 0.3741], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.4744, 0.5256], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6259, 0.3741], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.4744, 0.5256], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
R eval= {'agent_1': tensor(36.2397), 'agent_0': tensor(13.6356)}
Epoch : 0 	 Measure: nan
==========>Epoch= 1
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.4744, 0.5256], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6259, 0.3741], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.6259, 0.3741], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.4671, 0.5329], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.6757, 0.3243], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(1)}
d= tensor([0.6757, 0.3243], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 1., 0., 1.])
agent.state_act= tensor([1., 1., 1., 0.])
states= {'agent_0': tensor([1., 1., 0., 1.]), 'agent_1': tensor([1., 1., 1., 0.])}
agent.state_act= tensor([1., 1., 0., 1.])
 action, distrib= tensor(0) tensor([0.5180, 0.4820], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 0., 1.]), 'agent_1': tensor([1., 1., 1., 0.])}
agent.state_act= tensor([1., 1., 1., 0.])
 action, distrib= tensor(0) tensor([0.6534, 0.3466], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(0)}
d= tensor([0.6534, 0.3466], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.4665, 0.5335], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6109, 0.3891], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(0)}
d= tensor([0.6109, 0.3891], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 1., 0., 0.])
agent.state_act= tensor([1., 1., 0., 0.])
states= {'agent_0': tensor([1., 1., 0., 0.]), 'agent_1': tensor([1., 1., 0., 0.])}
agent.state_act= tensor([1., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.5090, 0.4910], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 0., 0.]), 'agent_1': tensor([1., 1., 0., 0.])}
agent.state_act= tensor([1., 1., 0., 0.])
 action, distrib= tensor(1) tensor([0.6685, 0.3315], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(6.), 'agent_1': tensor(0.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(1)}
d= tensor([0.6685, 0.3315], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 0., 0., 0.])
agent.state_act= tensor([0., 1., 0., 0.])
states= {'agent_0': tensor([1., 0., 0., 0.]), 'agent_1': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(1) tensor([0.5219, 0.4781], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 0., 0.]), 'agent_1': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.6166, 0.3834], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.6166, 0.3834], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 1., 1., 0.])
agent.state_act= tensor([1., 1., 0., 1.])
states= {'agent_0': tensor([1., 1., 1., 0.]), 'agent_1': tensor([1., 1., 0., 1.])}
agent.state_act= tensor([1., 1., 1., 0.])
 action, distrib= tensor(1) tensor([0.4552, 0.5448], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 1., 0.]), 'agent_1': tensor([1., 1., 0., 1.])}
agent.state_act= tensor([1., 1., 0., 1.])
 action, distrib= tensor(0) tensor([0.6885, 0.3115], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.6885, 0.3115], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 1., 0., 1.])
agent.state_act= tensor([1., 0., 1., 0.])
states= {'agent_0': tensor([0., 1., 0., 1.]), 'agent_1': tensor([1., 0., 1., 0.])}
agent.state_act= tensor([0., 1., 0., 1.])
 action, distrib= tensor(1) tensor([0.5133, 0.4867], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 0., 1.]), 'agent_1': tensor([1., 0., 1., 0.])}
agent.state_act= tensor([1., 0., 1., 0.])
 action, distrib= tensor(0) tensor([0.6470, 0.3530], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.6470, 0.3530], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 0., 0., 1.])
agent.state_act= tensor([0., 0., 1., 0.])
states= {'agent_0': tensor([0., 0., 0., 1.]), 'agent_1': tensor([0., 0., 1., 0.])}
agent.state_act= tensor([0., 0., 0., 1.])
 action, distrib= tensor(0) tensor([0.5239, 0.4761], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 0., 1.]), 'agent_1': tensor([0., 0., 1., 0.])}
agent.state_act= tensor([0., 0., 1., 0.])
 action, distrib= tensor(0) tensor([0.5732, 0.4268], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(0)}
d= tensor([0.5732, 0.4268], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 1., 0., 1.])
agent.state_act= tensor([1., 1., 1., 0.])
states= {'agent_0': tensor([1., 1., 0., 1.]), 'agent_1': tensor([1., 1., 1., 0.])}
agent.state_act= tensor([1., 1., 0., 1.])
 action, distrib= tensor(1) tensor([0.5180, 0.4820], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 0., 1.]), 'agent_1': tensor([1., 1., 1., 0.])}
agent.state_act= tensor([1., 1., 1., 0.])
 action, distrib= tensor(0) tensor([0.6534, 0.3466], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.6534, 0.3466], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
UPDATE
inside update
inside update
done
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.4729, 0.5271], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.6982, 0.3018], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(6.), 'agent_1': tensor(0.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.4729, 0.5271], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.6982, 0.3018], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.4729, 0.5271], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.6982, 0.3018], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(6.), 'agent_1': tensor(0.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.4729, 0.5271], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.6982, 0.3018], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.4729, 0.5271], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6982, 0.3018], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.4729, 0.5271], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.6982, 0.3018], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.4729, 0.5271], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6982, 0.3018], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.4729, 0.5271], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6982, 0.3018], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.4729, 0.5271], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6982, 0.3018], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.4729, 0.5271], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6982, 0.3018], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
R eval= {'agent_0': tensor(28.2920), 'agent_1': tensor(33.3039)}
==========>Epoch= 2
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6982, 0.3018], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.4729, 0.5271], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.4729, 0.5271], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.6328, 0.3672], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.4718, 0.5282], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
actions= {'agent_1': tensor(1), 'agent_0': tensor(1)}
d= tensor([0.4718, 0.5282], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 0., 1., 0.])
agent.state_act= tensor([0., 0., 0., 1.])
states= {'agent_1': tensor([0., 0., 1., 0.]), 'agent_0': tensor([0., 0., 0., 1.])}
agent.state_act= tensor([0., 0., 1., 0.])
 action, distrib= tensor(1) tensor([0.5909, 0.4091], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 0.]), 'agent_0': tensor([0., 0., 0., 1.])}
agent.state_act= tensor([0., 0., 0., 1.])
 action, distrib= tensor(1) tensor([0.5551, 0.4449], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
actions= {'agent_1': tensor(1), 'agent_0': tensor(1)}
d= tensor([0.5551, 0.4449], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.6328, 0.3672], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.4718, 0.5282], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(0.), 'agent_0': tensor(6.)}
actions= {'agent_1': tensor(1), 'agent_0': tensor(0)}
d= tensor([0.4718, 0.5282], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6982, 0.3018], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.4729, 0.5271], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.4729, 0.5271], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 0., 0., 1.])
agent.state_act= tensor([0., 0., 1., 0.])
states= {'agent_1': tensor([0., 0., 0., 1.]), 'agent_0': tensor([0., 0., 1., 0.])}
agent.state_act= tensor([0., 0., 0., 1.])
 action, distrib= tensor(0) tensor([0.6513, 0.3487], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 0., 1.]), 'agent_0': tensor([0., 0., 1., 0.])}
agent.state_act= tensor([0., 0., 1., 0.])
 action, distrib= tensor(1) tensor([0.4622, 0.5378], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.4622, 0.5378], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 1., 1., 0.])
agent.state_act= tensor([1., 0., 0., 1.])
states= {'agent_1': tensor([0., 1., 1., 0.]), 'agent_0': tensor([1., 0., 0., 1.])}
agent.state_act= tensor([0., 1., 1., 0.])
 action, distrib= tensor(0) tensor([0.6242, 0.3758], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 0.]), 'agent_0': tensor([1., 0., 0., 1.])}
agent.state_act= tensor([1., 0., 0., 1.])
 action, distrib= tensor(0) tensor([0.5610, 0.4390], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.5610, 0.4390], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 1., 1., 0.])
agent.state_act= tensor([1., 0., 0., 1.])
states= {'agent_1': tensor([0., 1., 1., 0.]), 'agent_0': tensor([1., 0., 0., 1.])}
agent.state_act= tensor([0., 1., 1., 0.])
 action, distrib= tensor(0) tensor([0.6242, 0.3758], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 0.]), 'agent_0': tensor([1., 0., 0., 1.])}
agent.state_act= tensor([1., 0., 0., 1.])
 action, distrib= tensor(1) tensor([0.5610, 0.4390], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.5610, 0.4390], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 1., 0., 0.])
agent.state_act= tensor([1., 1., 0., 0.])
states= {'agent_1': tensor([1., 1., 0., 0.]), 'agent_0': tensor([1., 1., 0., 0.])}
agent.state_act= tensor([1., 1., 0., 0.])
 action, distrib= tensor(1) tensor([0.6927, 0.3073], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 0., 0.]), 'agent_0': tensor([1., 1., 0., 0.])}
agent.state_act= tensor([1., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.5540, 0.4460], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(0.), 'agent_0': tensor(6.)}
actions= {'agent_1': tensor(1), 'agent_0': tensor(0)}
d= tensor([0.5540, 0.4460], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 1., 1., 0.])
agent.state_act= tensor([1., 0., 0., 1.])
states= {'agent_1': tensor([0., 1., 1., 0.]), 'agent_0': tensor([1., 0., 0., 1.])}
agent.state_act= tensor([0., 1., 1., 0.])
 action, distrib= tensor(0) tensor([0.6242, 0.3758], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 0.]), 'agent_0': tensor([1., 0., 0., 1.])}
agent.state_act= tensor([1., 0., 0., 1.])
 action, distrib= tensor(0) tensor([0.5610, 0.4390], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.5610, 0.4390], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
UPDATE
inside update
inside update
done
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.6662, 0.3338], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.4503, 0.5497], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(0.), 'agent_0': tensor(6.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6662, 0.3338], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.4503, 0.5497], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6662, 0.3338], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.4503, 0.5497], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6662, 0.3338], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.4503, 0.5497], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.6662, 0.3338], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.4503, 0.5497], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6662, 0.3338], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.4503, 0.5497], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6662, 0.3338], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.4503, 0.5497], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6662, 0.3338], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.4503, 0.5497], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6662, 0.3338], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.4503, 0.5497], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6662, 0.3338], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.4503, 0.5497], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
R eval= {'agent_1': tensor(17.3542), 'agent_0': tensor(17.4142)}
==========>Epoch= 3
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.4503, 0.5497], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.6662, 0.3338], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(6.), 'agent_1': tensor(0.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(1)}
d= tensor([0.6662, 0.3338], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.4379, 0.5621], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6363, 0.3637], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.6363, 0.3637], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 1., 0.])
agent.state_act= tensor([0., 1., 0., 1.])
states= {'agent_0': tensor([1., 0., 1., 0.]), 'agent_1': tensor([0., 1., 0., 1.])}
agent.state_act= tensor([1., 0., 1., 0.])
 action, distrib= tensor(1) tensor([0.4543, 0.5457], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 0.]), 'agent_1': tensor([0., 1., 0., 1.])}
agent.state_act= tensor([0., 1., 0., 1.])
 action, distrib= tensor(0) tensor([0.6855, 0.3145], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.6855, 0.3145], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 1., 0., 1.])
agent.state_act= tensor([1., 1., 1., 0.])
states= {'agent_0': tensor([1., 1., 0., 1.]), 'agent_1': tensor([1., 1., 1., 0.])}
agent.state_act= tensor([1., 1., 0., 1.])
 action, distrib= tensor(0) tensor([0.5385, 0.4615], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 0., 1.]), 'agent_1': tensor([1., 1., 1., 0.])}
agent.state_act= tensor([1., 1., 1., 0.])
 action, distrib= tensor(0) tensor([0.6931, 0.3069], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(0)}
d= tensor([0.6931, 0.3069], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 0., 0., 1.])
agent.state_act= tensor([0., 0., 1., 0.])
states= {'agent_0': tensor([0., 0., 0., 1.]), 'agent_1': tensor([0., 0., 1., 0.])}
agent.state_act= tensor([0., 0., 0., 1.])
 action, distrib= tensor(0) tensor([0.5371, 0.4629], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 0., 1.]), 'agent_1': tensor([0., 0., 1., 0.])}
agent.state_act= tensor([0., 0., 1., 0.])
 action, distrib= tensor(0) tensor([0.5893, 0.4107], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(0)}
d= tensor([0.5893, 0.4107], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 1., 0., 0.])
agent.state_act= tensor([1., 1., 0., 0.])
states= {'agent_0': tensor([1., 1., 0., 0.]), 'agent_1': tensor([1., 1., 0., 0.])}
agent.state_act= tensor([1., 1., 0., 0.])
 action, distrib= tensor(1) tensor([0.5434, 0.4566], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 0., 0.]), 'agent_1': tensor([1., 1., 0., 0.])}
agent.state_act= tensor([1., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.7093, 0.2907], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.7093, 0.2907], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 1., 0., 0.])
agent.state_act= tensor([1., 0., 0., 0.])
states= {'agent_0': tensor([0., 1., 0., 0.]), 'agent_1': tensor([1., 0., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(1) tensor([0.5306, 0.4694], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 0., 0.]), 'agent_1': tensor([1., 0., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.6965, 0.3035], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.6965, 0.3035], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 0., 0., 1.])
agent.state_act= tensor([0., 0., 1., 0.])
states= {'agent_0': tensor([0., 0., 0., 1.]), 'agent_1': tensor([0., 0., 1., 0.])}
agent.state_act= tensor([0., 0., 0., 1.])
 action, distrib= tensor(1) tensor([0.5371, 0.4629], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 0., 1.]), 'agent_1': tensor([0., 0., 1., 0.])}
agent.state_act= tensor([0., 0., 1., 0.])
 action, distrib= tensor(1) tensor([0.5893, 0.4107], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(1)}
d= tensor([0.5893, 0.4107], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 0., 0., 1.])
agent.state_act= tensor([0., 0., 1., 0.])
states= {'agent_0': tensor([0., 0., 0., 1.]), 'agent_1': tensor([0., 0., 1., 0.])}
agent.state_act= tensor([0., 0., 0., 1.])
 action, distrib= tensor(0) tensor([0.5371, 0.4629], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 0., 1.]), 'agent_1': tensor([0., 0., 1., 0.])}
agent.state_act= tensor([0., 0., 1., 0.])
 action, distrib= tensor(0) tensor([0.5893, 0.4107], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(0)}
d= tensor([0.5893, 0.4107], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.4499, 0.5501], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7196, 0.2804], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(0)}
d= tensor([0.7196, 0.2804], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
UPDATE
inside update
inside update
done
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.4049, 0.5951], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6355, 0.3645], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.4049, 0.5951], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6355, 0.3645], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.4049, 0.5951], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6355, 0.3645], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.4049, 0.5951], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.6355, 0.3645], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(6.), 'agent_1': tensor(0.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.4049, 0.5951], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6355, 0.3645], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.4049, 0.5951], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6355, 0.3645], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.4049, 0.5951], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.6355, 0.3645], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(6.), 'agent_1': tensor(0.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.4049, 0.5951], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6355, 0.3645], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.4049, 0.5951], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6355, 0.3645], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.4049, 0.5951], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6355, 0.3645], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
R eval= {'agent_0': tensor(13.3828), 'agent_1': tensor(36.3393)}
==========>Epoch= 4
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.6355, 0.3645], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.4049, 0.5951], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(0.), 'agent_0': tensor(6.)}
actions= {'agent_1': tensor(1), 'agent_0': tensor(0)}
d= tensor([0.4049, 0.5951], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.7139, 0.2861], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3983, 0.6017], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
actions= {'agent_1': tensor(1), 'agent_0': tensor(1)}
d= tensor([0.3983, 0.6017], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 1., 0., 1.])
agent.state_act= tensor([1., 0., 1., 0.])
states= {'agent_1': tensor([0., 1., 0., 1.]), 'agent_0': tensor([1., 0., 1., 0.])}
agent.state_act= tensor([0., 1., 0., 1.])
 action, distrib= tensor(1) tensor([0.6904, 0.3096], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 0., 1.]), 'agent_0': tensor([1., 0., 1., 0.])}
agent.state_act= tensor([1., 0., 1., 0.])
 action, distrib= tensor(0) tensor([0.4360, 0.5640], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(0.), 'agent_0': tensor(6.)}
actions= {'agent_1': tensor(1), 'agent_0': tensor(0)}
d= tensor([0.4360, 0.5640], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6355, 0.3645], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.4049, 0.5951], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.4049, 0.5951], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 1., 0., 1.])
agent.state_act= tensor([1., 0., 1., 0.])
states= {'agent_1': tensor([0., 1., 0., 1.]), 'agent_0': tensor([1., 0., 1., 0.])}
agent.state_act= tensor([0., 1., 0., 1.])
 action, distrib= tensor(0) tensor([0.6904, 0.3096], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 0., 1.]), 'agent_0': tensor([1., 0., 1., 0.])}
agent.state_act= tensor([1., 0., 1., 0.])
 action, distrib= tensor(1) tensor([0.4360, 0.5640], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.4360, 0.5640], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 1., 1., 0.])
agent.state_act= tensor([1., 1., 0., 1.])
states= {'agent_1': tensor([1., 1., 1., 0.]), 'agent_0': tensor([1., 1., 0., 1.])}
agent.state_act= tensor([1., 1., 1., 0.])
 action, distrib= tensor(0) tensor([0.6974, 0.3026], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 0.]), 'agent_0': tensor([1., 1., 0., 1.])}
agent.state_act= tensor([1., 1., 0., 1.])
 action, distrib= tensor(1) tensor([0.5124, 0.4876], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.5124, 0.4876], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 0., 1., 0.])
agent.state_act= tensor([0., 1., 0., 1.])
states= {'agent_1': tensor([1., 0., 1., 0.]), 'agent_0': tensor([0., 1., 0., 1.])}
agent.state_act= tensor([1., 0., 1., 0.])
 action, distrib= tensor(0) tensor([0.6772, 0.3228], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 0.]), 'agent_0': tensor([0., 1., 0., 1.])}
agent.state_act= tensor([0., 1., 0., 1.])
 action, distrib= tensor(0) tensor([0.5026, 0.4974], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.5026, 0.4974], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 1., 0.])
agent.state_act= tensor([0., 1., 0., 1.])
states= {'agent_1': tensor([1., 0., 1., 0.]), 'agent_0': tensor([0., 1., 0., 1.])}
agent.state_act= tensor([1., 0., 1., 0.])
 action, distrib= tensor(1) tensor([0.6772, 0.3228], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 0.]), 'agent_0': tensor([0., 1., 0., 1.])}
agent.state_act= tensor([0., 1., 0., 1.])
 action, distrib= tensor(0) tensor([0.5026, 0.4974], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(0.), 'agent_0': tensor(6.)}
actions= {'agent_1': tensor(1), 'agent_0': tensor(0)}
d= tensor([0.5026, 0.4974], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 1., 0., 0.])
agent.state_act= tensor([1., 1., 0., 0.])
states= {'agent_1': tensor([1., 1., 0., 0.]), 'agent_0': tensor([1., 1., 0., 0.])}
agent.state_act= tensor([1., 1., 0., 0.])
 action, distrib= tensor(1) tensor([0.7161, 0.2839], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 0., 0.]), 'agent_0': tensor([1., 1., 0., 0.])}
agent.state_act= tensor([1., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.5276, 0.4724], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(0.), 'agent_0': tensor(6.)}
actions= {'agent_1': tensor(1), 'agent_0': tensor(0)}
d= tensor([0.5276, 0.4724], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 1., 0., 1.])
agent.state_act= tensor([1., 0., 1., 0.])
states= {'agent_1': tensor([0., 1., 0., 1.]), 'agent_0': tensor([1., 0., 1., 0.])}
agent.state_act= tensor([0., 1., 0., 1.])
 action, distrib= tensor(0) tensor([0.6904, 0.3096], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 0., 1.]), 'agent_0': tensor([1., 0., 1., 0.])}
agent.state_act= tensor([1., 0., 1., 0.])
 action, distrib= tensor(1) tensor([0.4360, 0.5640], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.4360, 0.5640], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
UPDATE
inside update
inside update
done
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.7208, 0.2792], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.4036, 0.5964], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7208, 0.2792], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.4036, 0.5964], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7208, 0.2792], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.4036, 0.5964], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.7208, 0.2792], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.4036, 0.5964], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(0.), 'agent_0': tensor(6.)}
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7208, 0.2792], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.4036, 0.5964], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7208, 0.2792], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.4036, 0.5964], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7208, 0.2792], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.4036, 0.5964], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.7208, 0.2792], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.4036, 0.5964], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(0.), 'agent_0': tensor(6.)}
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7208, 0.2792], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.4036, 0.5964], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7208, 0.2792], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.4036, 0.5964], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
R eval= {'agent_1': tensor(30.8786), 'agent_0': tensor(19.2298)}
==========>Epoch= 5
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7208, 0.2792], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.4036, 0.5964], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.4036, 0.5964], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7071, 0.2929], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3838, 0.6162], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.3838, 0.6162], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 0., 1., 0.])
agent.state_act= tensor([0., 0., 0., 1.])
states= {'agent_1': tensor([0., 0., 1., 0.]), 'agent_0': tensor([0., 0., 0., 1.])}
agent.state_act= tensor([0., 0., 1., 0.])
 action, distrib= tensor(0) tensor([0.5760, 0.4240], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 0.]), 'agent_0': tensor([0., 0., 0., 1.])}
agent.state_act= tensor([0., 0., 0., 1.])
 action, distrib= tensor(1) tensor([0.5049, 0.4951], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.5049, 0.4951], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 1., 1., 0.])
agent.state_act= tensor([1., 0., 0., 1.])
states= {'agent_1': tensor([0., 1., 1., 0.]), 'agent_0': tensor([1., 0., 0., 1.])}
agent.state_act= tensor([0., 1., 1., 0.])
 action, distrib= tensor(1) tensor([0.6233, 0.3767], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 0.]), 'agent_0': tensor([1., 0., 0., 1.])}
agent.state_act= tensor([1., 0., 0., 1.])
 action, distrib= tensor(1) tensor([0.5212, 0.4788], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
actions= {'agent_1': tensor(1), 'agent_0': tensor(1)}
d= tensor([0.5212, 0.4788], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 0., 1., 0.])
agent.state_act= tensor([0., 1., 0., 1.])
states= {'agent_1': tensor([1., 0., 1., 0.]), 'agent_0': tensor([0., 1., 0., 1.])}
agent.state_act= tensor([1., 0., 1., 0.])
 action, distrib= tensor(0) tensor([0.6729, 0.3271], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 0.]), 'agent_0': tensor([0., 1., 0., 1.])}
agent.state_act= tensor([0., 1., 0., 1.])
 action, distrib= tensor(1) tensor([0.4823, 0.5177], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.4823, 0.5177], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.6230, 0.3770], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3998, 0.6002], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
actions= {'agent_1': tensor(1), 'agent_0': tensor(1)}
d= tensor([0.3998, 0.6002], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 0., 1., 0.])
agent.state_act= tensor([0., 0., 0., 1.])
states= {'agent_1': tensor([0., 0., 1., 0.]), 'agent_0': tensor([0., 0., 0., 1.])}
agent.state_act= tensor([0., 0., 1., 0.])
 action, distrib= tensor(1) tensor([0.5760, 0.4240], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 0.]), 'agent_0': tensor([0., 0., 0., 1.])}
agent.state_act= tensor([0., 0., 0., 1.])
 action, distrib= tensor(0) tensor([0.5049, 0.4951], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(0.), 'agent_0': tensor(6.)}
actions= {'agent_1': tensor(1), 'agent_0': tensor(0)}
d= tensor([0.5049, 0.4951], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.7071, 0.2929], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3838, 0.6162], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
actions= {'agent_1': tensor(1), 'agent_0': tensor(1)}
d= tensor([0.3838, 0.6162], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 1., 0., 1.])
agent.state_act= tensor([1., 0., 1., 0.])
states= {'agent_1': tensor([0., 1., 0., 1.]), 'agent_0': tensor([1., 0., 1., 0.])}
agent.state_act= tensor([0., 1., 0., 1.])
 action, distrib= tensor(0) tensor([0.6839, 0.3161], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 0., 1.]), 'agent_0': tensor([1., 0., 1., 0.])}
agent.state_act= tensor([1., 0., 1., 0.])
 action, distrib= tensor(0) tensor([0.4449, 0.5551], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.4449, 0.5551], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6586, 0.3414], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.4216, 0.5784], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.4216, 0.5784], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
UPDATE
inside update
inside update
done
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.7261, 0.2739], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3814, 0.6186], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(0.), 'agent_0': tensor(6.)}
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7261, 0.2739], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3814, 0.6186], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.7261, 0.2739], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3814, 0.6186], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.7261, 0.2739], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3814, 0.6186], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(0.), 'agent_0': tensor(6.)}
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7261, 0.2739], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3814, 0.6186], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7261, 0.2739], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3814, 0.6186], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7261, 0.2739], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3814, 0.6186], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7261, 0.2739], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3814, 0.6186], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.7261, 0.2739], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3814, 0.6186], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7261, 0.2739], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3814, 0.6186], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
R eval= {'agent_1': tensor(29.6152), 'agent_0': tensor(24.1422)}
==========>Epoch= 6
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.7261, 0.2739], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3814, 0.6186], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
actions= {'agent_1': tensor(1), 'agent_0': tensor(1)}
d= tensor([0.3814, 0.6186], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7261, 0.2739], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3814, 0.6186], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.3814, 0.6186], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7135, 0.2865], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3603, 0.6397], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.3603, 0.6397], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 0., 1., 0.])
agent.state_act= tensor([0., 0., 0., 1.])
states= {'agent_1': tensor([0., 0., 1., 0.]), 'agent_0': tensor([0., 0., 0., 1.])}
agent.state_act= tensor([0., 0., 1., 0.])
 action, distrib= tensor(0) tensor([0.5786, 0.4214], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 0.]), 'agent_0': tensor([0., 0., 0., 1.])}
agent.state_act= tensor([0., 0., 0., 1.])
 action, distrib= tensor(1) tensor([0.4844, 0.5156], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.4844, 0.5156], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 1., 1., 0.])
agent.state_act= tensor([1., 0., 0., 1.])
states= {'agent_1': tensor([0., 1., 1., 0.]), 'agent_0': tensor([1., 0., 0., 1.])}
agent.state_act= tensor([0., 1., 1., 0.])
 action, distrib= tensor(0) tensor([0.6246, 0.3754], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 0.]), 'agent_0': tensor([1., 0., 0., 1.])}
agent.state_act= tensor([1., 0., 0., 1.])
 action, distrib= tensor(0) tensor([0.5043, 0.4957], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.5043, 0.4957], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 1., 1., 0.])
agent.state_act= tensor([1., 0., 0., 1.])
states= {'agent_1': tensor([0., 1., 1., 0.]), 'agent_0': tensor([1., 0., 0., 1.])}
agent.state_act= tensor([0., 1., 1., 0.])
 action, distrib= tensor(0) tensor([0.6246, 0.3754], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 0.]), 'agent_0': tensor([1., 0., 0., 1.])}
agent.state_act= tensor([1., 0., 0., 1.])
 action, distrib= tensor(0) tensor([0.5043, 0.4957], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.5043, 0.4957], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 1., 0., 0.])
agent.state_act= tensor([1., 0., 0., 0.])
states= {'agent_1': tensor([0., 1., 0., 0.]), 'agent_0': tensor([1., 0., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.6527, 0.3473], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 0., 0.]), 'agent_0': tensor([1., 0., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.5437, 0.4563], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.5437, 0.4563], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 1., 0., 0.])
agent.state_act= tensor([1., 0., 0., 0.])
states= {'agent_1': tensor([0., 1., 0., 0.]), 'agent_0': tensor([1., 0., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.6527, 0.3473], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 0., 0.]), 'agent_0': tensor([1., 0., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(1) tensor([0.5437, 0.4563], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.5437, 0.4563], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 1., 0., 0.])
agent.state_act= tensor([1., 1., 0., 0.])
states= {'agent_1': tensor([1., 1., 0., 0.]), 'agent_0': tensor([1., 1., 0., 0.])}
agent.state_act= tensor([1., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.7235, 0.2765], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 0., 0.]), 'agent_0': tensor([1., 1., 0., 0.])}
agent.state_act= tensor([1., 1., 0., 0.])
 action, distrib= tensor(1) tensor([0.4999, 0.5001], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.4999, 0.5001], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 0., 1., 0.])
agent.state_act= tensor([0., 1., 0., 1.])
states= {'agent_1': tensor([1., 0., 1., 0.]), 'agent_0': tensor([0., 1., 0., 1.])}
agent.state_act= tensor([1., 0., 1., 0.])
 action, distrib= tensor(1) tensor([0.6819, 0.3181], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 0.]), 'agent_0': tensor([0., 1., 0., 1.])}
agent.state_act= tensor([0., 1., 0., 1.])
 action, distrib= tensor(1) tensor([0.4522, 0.5478], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
actions= {'agent_1': tensor(1), 'agent_0': tensor(1)}
d= tensor([0.4522, 0.5478], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
UPDATE
inside update
inside update
done
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6517, 0.3483], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3924, 0.6076], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.6517, 0.3483], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3924, 0.6076], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6517, 0.3483], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3924, 0.6076], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6517, 0.3483], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3924, 0.6076], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.6517, 0.3483], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3924, 0.6076], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6517, 0.3483], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3924, 0.6076], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6517, 0.3483], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3924, 0.6076], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6517, 0.3483], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3924, 0.6076], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6517, 0.3483], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3924, 0.6076], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6517, 0.3483], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3924, 0.6076], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
R eval= {'agent_1': tensor(31.8711), 'agent_0': tensor(14.4628)}
==========>Epoch= 7
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3924, 0.6076], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6517, 0.3483], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.6517, 0.3483], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3567, 0.6433], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7260, 0.2740], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.7260, 0.2740], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 1., 0., 1.])
agent.state_act= tensor([1., 0., 1., 0.])
states= {'agent_0': tensor([0., 1., 0., 1.]), 'agent_1': tensor([1., 0., 1., 0.])}
agent.state_act= tensor([0., 1., 0., 1.])
 action, distrib= tensor(0) tensor([0.4259, 0.5741], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 0., 1.]), 'agent_1': tensor([1., 0., 1., 0.])}
agent.state_act= tensor([1., 0., 1., 0.])
 action, distrib= tensor(0) tensor([0.6897, 0.3103], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(0)}
d= tensor([0.6897, 0.3103], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 1., 0., 1.])
agent.state_act= tensor([1., 0., 1., 0.])
states= {'agent_0': tensor([0., 1., 0., 1.]), 'agent_1': tensor([1., 0., 1., 0.])}
agent.state_act= tensor([0., 1., 0., 1.])
 action, distrib= tensor(1) tensor([0.4259, 0.5741], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 0., 1.]), 'agent_1': tensor([1., 0., 1., 0.])}
agent.state_act= tensor([1., 0., 1., 0.])
 action, distrib= tensor(0) tensor([0.6897, 0.3103], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.6897, 0.3103], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 0., 0., 0.])
agent.state_act= tensor([0., 0., 0., 0.])
states= {'agent_0': tensor([0., 0., 0., 0.]), 'agent_1': tensor([0., 0., 0., 0.])}
agent.state_act= tensor([0., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.5134, 0.4866], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 0., 0.]), 'agent_1': tensor([0., 0., 0., 0.])}
agent.state_act= tensor([0., 0., 0., 0.])
 action, distrib= tensor(1) tensor([0.6124, 0.3876], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(6.), 'agent_1': tensor(0.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(1)}
d= tensor([0.6124, 0.3876], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 1., 0., 1.])
agent.state_act= tensor([1., 0., 1., 0.])
states= {'agent_0': tensor([0., 1., 0., 1.]), 'agent_1': tensor([1., 0., 1., 0.])}
agent.state_act= tensor([0., 1., 0., 1.])
 action, distrib= tensor(1) tensor([0.4259, 0.5741], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 0., 1.]), 'agent_1': tensor([1., 0., 1., 0.])}
agent.state_act= tensor([1., 0., 1., 0.])
 action, distrib= tensor(0) tensor([0.6897, 0.3103], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.6897, 0.3103], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 0., 1., 0.])
agent.state_act= tensor([0., 0., 0., 1.])
states= {'agent_0': tensor([0., 0., 1., 0.]), 'agent_1': tensor([0., 0., 0., 1.])}
agent.state_act= tensor([0., 0., 1., 0.])
 action, distrib= tensor(1) tensor([0.4063, 0.5937], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 0.]), 'agent_1': tensor([0., 0., 0., 1.])}
agent.state_act= tensor([0., 0., 0., 1.])
 action, distrib= tensor(0) tensor([0.6503, 0.3497], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.6503, 0.3497], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 0., 1.])
agent.state_act= tensor([0., 1., 1., 0.])
states= {'agent_0': tensor([1., 0., 0., 1.]), 'agent_1': tensor([0., 1., 1., 0.])}
agent.state_act= tensor([1., 0., 0., 1.])
 action, distrib= tensor(1) tensor([0.4900, 0.5100], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 0., 1.]), 'agent_1': tensor([0., 1., 1., 0.])}
agent.state_act= tensor([0., 1., 1., 0.])
 action, distrib= tensor(0) tensor([0.6219, 0.3781], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.6219, 0.3781], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 1., 0., 1.])
agent.state_act= tensor([1., 1., 1., 0.])
states= {'agent_0': tensor([1., 1., 0., 1.]), 'agent_1': tensor([1., 1., 1., 0.])}
agent.state_act= tensor([1., 1., 0., 1.])
 action, distrib= tensor(1) tensor([0.4465, 0.5535], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 0., 1.]), 'agent_1': tensor([1., 1., 1., 0.])}
agent.state_act= tensor([1., 1., 1., 0.])
 action, distrib= tensor(1) tensor([0.7064, 0.2936], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(1)}
d= tensor([0.7064, 0.2936], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 1., 0., 1.])
agent.state_act= tensor([1., 1., 1., 0.])
states= {'agent_0': tensor([1., 1., 0., 1.]), 'agent_1': tensor([1., 1., 1., 0.])}
agent.state_act= tensor([1., 1., 0., 1.])
 action, distrib= tensor(1) tensor([0.4465, 0.5535], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 0., 1.]), 'agent_1': tensor([1., 1., 1., 0.])}
agent.state_act= tensor([1., 1., 1., 0.])
 action, distrib= tensor(0) tensor([0.7064, 0.2936], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.7064, 0.2936], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
UPDATE
inside update
inside update
done
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3278, 0.6722], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7294, 0.2706], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3278, 0.6722], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7294, 0.2706], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3278, 0.6722], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7294, 0.2706], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3278, 0.6722], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7294, 0.2706], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3278, 0.6722], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.7294, 0.2706], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(6.), 'agent_1': tensor(0.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3278, 0.6722], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7294, 0.2706], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3278, 0.6722], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7294, 0.2706], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3278, 0.6722], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7294, 0.2706], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3278, 0.6722], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7294, 0.2706], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3278, 0.6722], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7294, 0.2706], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
R eval= {'agent_0': tensor(9.4734), 'agent_1': tensor(33.0581)}
==========>Epoch= 8
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3278, 0.6722], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7294, 0.2706], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.7294, 0.2706], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3610, 0.6390], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.6280, 0.3720], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(6.), 'agent_1': tensor(0.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(1)}
d= tensor([0.6280, 0.3720], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 1., 0., 1.])
agent.state_act= tensor([1., 0., 1., 0.])
states= {'agent_0': tensor([0., 1., 0., 1.]), 'agent_1': tensor([1., 0., 1., 0.])}
agent.state_act= tensor([0., 1., 0., 1.])
 action, distrib= tensor(1) tensor([0.4239, 0.5761], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 0., 1.]), 'agent_1': tensor([1., 0., 1., 0.])}
agent.state_act= tensor([1., 0., 1., 0.])
 action, distrib= tensor(0) tensor([0.7047, 0.2953], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.7047, 0.2953], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 0., 1., 0.])
agent.state_act= tensor([0., 0., 0., 1.])
states= {'agent_0': tensor([0., 0., 1., 0.]), 'agent_1': tensor([0., 0., 0., 1.])}
agent.state_act= tensor([0., 0., 1., 0.])
 action, distrib= tensor(1) tensor([0.4045, 0.5955], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 0.]), 'agent_1': tensor([0., 0., 0., 1.])}
agent.state_act= tensor([0., 0., 0., 1.])
 action, distrib= tensor(1) tensor([0.6531, 0.3469], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(1)}
d= tensor([0.6531, 0.3469], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 0., 0., 1.])
agent.state_act= tensor([0., 0., 1., 0.])
states= {'agent_0': tensor([0., 0., 0., 1.]), 'agent_1': tensor([0., 0., 1., 0.])}
agent.state_act= tensor([0., 0., 0., 1.])
 action, distrib= tensor(1) tensor([0.4684, 0.5316], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 0., 1.]), 'agent_1': tensor([0., 0., 1., 0.])}
agent.state_act= tensor([0., 0., 1., 0.])
 action, distrib= tensor(0) tensor([0.5903, 0.4097], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.5903, 0.4097], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3886, 0.6114], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6549, 0.3451], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.6549, 0.3451], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 1., 0., 1.])
agent.state_act= tensor([1., 1., 1., 0.])
states= {'agent_0': tensor([1., 1., 0., 1.]), 'agent_1': tensor([1., 1., 1., 0.])}
agent.state_act= tensor([1., 1., 0., 1.])
 action, distrib= tensor(1) tensor([0.4462, 0.5538], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 0., 1.]), 'agent_1': tensor([1., 1., 1., 0.])}
agent.state_act= tensor([1., 1., 1., 0.])
 action, distrib= tensor(0) tensor([0.7185, 0.2815], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.7185, 0.2815], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 1., 0., 1.])
agent.state_act= tensor([1., 0., 1., 0.])
states= {'agent_0': tensor([0., 1., 0., 1.]), 'agent_1': tensor([1., 0., 1., 0.])}
agent.state_act= tensor([0., 1., 0., 1.])
 action, distrib= tensor(1) tensor([0.4239, 0.5761], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 0., 1.]), 'agent_1': tensor([1., 0., 1., 0.])}
agent.state_act= tensor([1., 0., 1., 0.])
 action, distrib= tensor(1) tensor([0.7047, 0.2953], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(1)}
d= tensor([0.7047, 0.2953], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 0., 0., 1.])
agent.state_act= tensor([0., 1., 1., 0.])
states= {'agent_0': tensor([1., 0., 0., 1.]), 'agent_1': tensor([0., 1., 1., 0.])}
agent.state_act= tensor([1., 0., 0., 1.])
 action, distrib= tensor(1) tensor([0.4929, 0.5071], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 0., 1.]), 'agent_1': tensor([0., 1., 1., 0.])}
agent.state_act= tensor([0., 1., 1., 0.])
 action, distrib= tensor(1) tensor([0.6275, 0.3725], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(1)}
d= tensor([0.6275, 0.3725], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3278, 0.6722], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7294, 0.2706], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.7294, 0.2706], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
UPDATE
inside update
inside update
done
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3682, 0.6318], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6340, 0.3660], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3682, 0.6318], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.6340, 0.3660], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3682, 0.6318], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6340, 0.3660], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3682, 0.6318], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6340, 0.3660], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3682, 0.6318], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.6340, 0.3660], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(6.), 'agent_1': tensor(0.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3682, 0.6318], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6340, 0.3660], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3682, 0.6318], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6340, 0.3660], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3682, 0.6318], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6340, 0.3660], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3682, 0.6318], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.6340, 0.3660], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(6.), 'agent_1': tensor(0.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3682, 0.6318], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6340, 0.3660], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
R eval= {'agent_0': tensor(18.1716), 'agent_1': tensor(35.4728)}
==========>Epoch= 9
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3682, 0.6318], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6340, 0.3660], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.6340, 0.3660], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3987, 0.6013], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.6604, 0.3396], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(1)}
d= tensor([0.6604, 0.3396], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 1., 0., 1.])
agent.state_act= tensor([1., 0., 1., 0.])
states= {'agent_0': tensor([0., 1., 0., 1.]), 'agent_1': tensor([1., 0., 1., 0.])}
agent.state_act= tensor([0., 1., 0., 1.])
 action, distrib= tensor(0) tensor([0.4329, 0.5671], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 0., 1.]), 'agent_1': tensor([1., 0., 1., 0.])}
agent.state_act= tensor([1., 0., 1., 0.])
 action, distrib= tensor(0) tensor([0.7231, 0.2769], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(0)}
d= tensor([0.7231, 0.2769], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3332, 0.6668], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7418, 0.2582], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.7418, 0.2582], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 0., 0., 0.])
agent.state_act= tensor([0., 0., 0., 0.])
states= {'agent_0': tensor([0., 0., 0., 0.]), 'agent_1': tensor([0., 0., 0., 0.])}
agent.state_act= tensor([0., 0., 0., 0.])
 action, distrib= tensor(1) tensor([0.5273, 0.4727], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 0., 0.]), 'agent_1': tensor([0., 0., 0., 0.])}
agent.state_act= tensor([0., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.6249, 0.3751], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.6249, 0.3751], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 0., 1.])
agent.state_act= tensor([0., 1., 1., 0.])
states= {'agent_0': tensor([1., 0., 0., 1.]), 'agent_1': tensor([0., 1., 1., 0.])}
agent.state_act= tensor([1., 0., 0., 1.])
 action, distrib= tensor(1) tensor([0.5061, 0.4939], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 0., 1.]), 'agent_1': tensor([0., 1., 1., 0.])}
agent.state_act= tensor([0., 1., 1., 0.])
 action, distrib= tensor(0) tensor([0.6388, 0.3612], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.6388, 0.3612], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 1., 0., 1.])
agent.state_act= tensor([1., 1., 1., 0.])
states= {'agent_0': tensor([1., 1., 0., 1.]), 'agent_1': tensor([1., 1., 1., 0.])}
agent.state_act= tensor([1., 1., 0., 1.])
 action, distrib= tensor(1) tensor([0.4581, 0.5419], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 0., 1.]), 'agent_1': tensor([1., 1., 1., 0.])}
agent.state_act= tensor([1., 1., 1., 0.])
 action, distrib= tensor(0) tensor([0.7355, 0.2645], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.7355, 0.2645], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 1., 0., 1.])
agent.state_act= tensor([1., 0., 1., 0.])
states= {'agent_0': tensor([0., 1., 0., 1.]), 'agent_1': tensor([1., 0., 1., 0.])}
agent.state_act= tensor([0., 1., 0., 1.])
 action, distrib= tensor(1) tensor([0.4329, 0.5671], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 0., 1.]), 'agent_1': tensor([1., 0., 1., 0.])}
agent.state_act= tensor([1., 0., 1., 0.])
 action, distrib= tensor(1) tensor([0.7231, 0.2769], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(1)}
d= tensor([0.7231, 0.2769], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 0., 0., 1.])
agent.state_act= tensor([0., 1., 1., 0.])
states= {'agent_0': tensor([1., 0., 0., 1.]), 'agent_1': tensor([0., 1., 1., 0.])}
agent.state_act= tensor([1., 0., 0., 1.])
 action, distrib= tensor(0) tensor([0.5061, 0.4939], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 0., 1.]), 'agent_1': tensor([0., 1., 1., 0.])}
agent.state_act= tensor([0., 1., 1., 0.])
 action, distrib= tensor(0) tensor([0.6388, 0.3612], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(0)}
d= tensor([0.6388, 0.3612], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3987, 0.6013], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.6604, 0.3396], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(6.), 'agent_1': tensor(0.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(1)}
d= tensor([0.6604, 0.3396], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
UPDATE
inside update
inside update
done
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3577, 0.6423], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.6481, 0.3519], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(6.), 'agent_1': tensor(0.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3577, 0.6423], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.6481, 0.3519], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3577, 0.6423], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6481, 0.3519], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3577, 0.6423], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6481, 0.3519], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3577, 0.6423], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.6481, 0.3519], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3577, 0.6423], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6481, 0.3519], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3577, 0.6423], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.6481, 0.3519], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3577, 0.6423], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6481, 0.3519], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3577, 0.6423], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.6481, 0.3519], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3577, 0.6423], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6481, 0.3519], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
R eval= {'agent_0': tensor(27.8900), 'agent_1': tensor(33.4765)}
==========>Epoch= 10
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6481, 0.3519], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3577, 0.6423], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.3577, 0.6423], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7642, 0.2358], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3463, 0.6537], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.3463, 0.6537], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 0., 0., 0.])
agent.state_act= tensor([0., 1., 0., 0.])
states= {'agent_1': tensor([1., 0., 0., 0.]), 'agent_0': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.7591, 0.2409], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 0., 0.]), 'agent_0': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.4640, 0.5360], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.4640, 0.5360], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 1., 0.])
agent.state_act= tensor([0., 1., 0., 1.])
states= {'agent_1': tensor([1., 0., 1., 0.]), 'agent_0': tensor([0., 1., 0., 1.])}
agent.state_act= tensor([1., 0., 1., 0.])
 action, distrib= tensor(0) tensor([0.7466, 0.2534], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 0.]), 'agent_0': tensor([0., 1., 0., 1.])}
agent.state_act= tensor([0., 1., 0., 1.])
 action, distrib= tensor(0) tensor([0.4287, 0.5713], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.4287, 0.5713], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 0., 0.])
agent.state_act= tensor([0., 1., 0., 0.])
states= {'agent_1': tensor([1., 0., 0., 0.]), 'agent_0': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.7591, 0.2409], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 0., 0.]), 'agent_0': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.4640, 0.5360], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.4640, 0.5360], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 0., 0.])
agent.state_act= tensor([0., 1., 0., 0.])
states= {'agent_1': tensor([1., 0., 0., 0.]), 'agent_0': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.7591, 0.2409], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 0., 0.]), 'agent_0': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.4640, 0.5360], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.4640, 0.5360], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 0., 0.])
agent.state_act= tensor([0., 1., 0., 0.])
states= {'agent_1': tensor([1., 0., 0., 0.]), 'agent_0': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.7591, 0.2409], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 0., 0.]), 'agent_0': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.4640, 0.5360], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.4640, 0.5360], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 0., 0.])
agent.state_act= tensor([0., 1., 0., 0.])
states= {'agent_1': tensor([1., 0., 0., 0.]), 'agent_0': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(1) tensor([0.7591, 0.2409], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 0., 0.]), 'agent_0': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(1) tensor([0.4640, 0.5360], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
actions= {'agent_1': tensor(1), 'agent_0': tensor(1)}
d= tensor([0.4640, 0.5360], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 1., 0., 0.])
agent.state_act= tensor([1., 0., 0., 0.])
states= {'agent_1': tensor([0., 1., 0., 0.]), 'agent_0': tensor([1., 0., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.6670, 0.3330], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 0., 0.]), 'agent_0': tensor([1., 0., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.5571, 0.4429], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.5571, 0.4429], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6718, 0.3282], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3859, 0.6141], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.3859, 0.6141], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
UPDATE
inside update
inside update
done
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6832, 0.3168], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3696, 0.6304], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.6832, 0.3168], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3696, 0.6304], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.6832, 0.3168], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3696, 0.6304], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(0.), 'agent_0': tensor(6.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.6832, 0.3168], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3696, 0.6304], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6832, 0.3168], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3696, 0.6304], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6832, 0.3168], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3696, 0.6304], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6832, 0.3168], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3696, 0.6304], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6832, 0.3168], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3696, 0.6304], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.6832, 0.3168], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3696, 0.6304], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(0.), 'agent_0': tensor(6.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6832, 0.3168], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3696, 0.6304], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
R eval= {'agent_1': tensor(29.5781), 'agent_0': tensor(24.1016)}
Epoch : 10 	 Measure: nan
==========>Epoch= 11
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.6832, 0.3168], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3696, 0.6304], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
actions= {'agent_1': tensor(1), 'agent_0': tensor(1)}
d= tensor([0.3696, 0.6304], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7820, 0.2180], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3166, 0.6834], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.3166, 0.6834], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7820, 0.2180], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3166, 0.6834], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.3166, 0.6834], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 0., 0., 0.])
agent.state_act= tensor([0., 0., 0., 0.])
states= {'agent_1': tensor([0., 0., 0., 0.]), 'agent_0': tensor([0., 0., 0., 0.])}
agent.state_act= tensor([0., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.6497, 0.3503], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 0., 0.]), 'agent_0': tensor([0., 0., 0., 0.])}
agent.state_act= tensor([0., 0., 0., 0.])
 action, distrib= tensor(1) tensor([0.5306, 0.4694], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.5306, 0.4694], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 1., 1., 0.])
agent.state_act= tensor([1., 0., 0., 1.])
states= {'agent_1': tensor([0., 1., 1., 0.]), 'agent_0': tensor([1., 0., 0., 1.])}
agent.state_act= tensor([0., 1., 1., 0.])
 action, distrib= tensor(0) tensor([0.6674, 0.3326], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 0.]), 'agent_0': tensor([1., 0., 0., 1.])}
agent.state_act= tensor([1., 0., 0., 1.])
 action, distrib= tensor(0) tensor([0.4919, 0.5081], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.4919, 0.5081], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 1., 1., 0.])
agent.state_act= tensor([1., 0., 0., 1.])
states= {'agent_1': tensor([0., 1., 1., 0.]), 'agent_0': tensor([1., 0., 0., 1.])}
agent.state_act= tensor([0., 1., 1., 0.])
 action, distrib= tensor(1) tensor([0.6674, 0.3326], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 0.]), 'agent_0': tensor([1., 0., 0., 1.])}
agent.state_act= tensor([1., 0., 0., 1.])
 action, distrib= tensor(0) tensor([0.4919, 0.5081], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(0.), 'agent_0': tensor(6.)}
actions= {'agent_1': tensor(1), 'agent_0': tensor(0)}
d= tensor([0.4919, 0.5081], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 0., 0., 0.])
agent.state_act= tensor([0., 0., 0., 0.])
states= {'agent_1': tensor([0., 0., 0., 0.]), 'agent_0': tensor([0., 0., 0., 0.])}
agent.state_act= tensor([0., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.6497, 0.3503], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 0., 0.]), 'agent_0': tensor([0., 0., 0., 0.])}
agent.state_act= tensor([0., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.5306, 0.4694], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.5306, 0.4694], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 1., 0., 1.])
agent.state_act= tensor([1., 1., 1., 0.])
states= {'agent_1': tensor([1., 1., 0., 1.]), 'agent_0': tensor([1., 1., 1., 0.])}
agent.state_act= tensor([1., 1., 0., 1.])
 action, distrib= tensor(0) tensor([0.7804, 0.2196], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 0., 1.]), 'agent_0': tensor([1., 1., 1., 0.])}
agent.state_act= tensor([1., 1., 1., 0.])
 action, distrib= tensor(1) tensor([0.3698, 0.6302], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.3698, 0.6302], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 0., 0., 0.])
agent.state_act= tensor([0., 1., 0., 0.])
states= {'agent_1': tensor([1., 0., 0., 0.]), 'agent_0': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.7785, 0.2215], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 0., 0.]), 'agent_0': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(1) tensor([0.4680, 0.5320], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.4680, 0.5320], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 0., 1., 0.])
agent.state_act= tensor([0., 0., 0., 1.])
states= {'agent_1': tensor([0., 0., 1., 0.]), 'agent_0': tensor([0., 0., 0., 1.])}
agent.state_act= tensor([0., 0., 1., 0.])
 action, distrib= tensor(0) tensor([0.6400, 0.3600], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 0.]), 'agent_0': tensor([0., 0., 0., 1.])}
agent.state_act= tensor([0., 0., 0., 1.])
 action, distrib= tensor(0) tensor([0.4734, 0.5266], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.4734, 0.5266], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
UPDATE
inside update
inside update
done
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7896, 0.2104], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3260, 0.6740], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7896, 0.2104], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3260, 0.6740], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7896, 0.2104], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3260, 0.6740], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7896, 0.2104], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3260, 0.6740], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7896, 0.2104], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3260, 0.6740], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.7896, 0.2104], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3260, 0.6740], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(0.), 'agent_0': tensor(6.)}
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7896, 0.2104], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3260, 0.6740], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7896, 0.2104], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3260, 0.6740], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7896, 0.2104], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3260, 0.6740], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7896, 0.2104], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3260, 0.6740], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
R eval= {'agent_1': tensor(32.6892), 'agent_0': tensor(9.5011)}
==========>Epoch= 12
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7896, 0.2104], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3260, 0.6740], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.3260, 0.6740], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.7939, 0.2061], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3133, 0.6867], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
actions= {'agent_1': tensor(1), 'agent_0': tensor(1)}
d= tensor([0.3133, 0.6867], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 1., 1., 0.])
agent.state_act= tensor([1., 0., 0., 1.])
states= {'agent_1': tensor([0., 1., 1., 0.]), 'agent_0': tensor([1., 0., 0., 1.])}
agent.state_act= tensor([0., 1., 1., 0.])
 action, distrib= tensor(1) tensor([0.6692, 0.3308], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 0.]), 'agent_0': tensor([1., 0., 0., 1.])}
agent.state_act= tensor([1., 0., 0., 1.])
 action, distrib= tensor(1) tensor([0.4824, 0.5176], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
actions= {'agent_1': tensor(1), 'agent_0': tensor(1)}
d= tensor([0.4824, 0.5176], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.7939, 0.2061], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3133, 0.6867], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(0.), 'agent_0': tensor(6.)}
actions= {'agent_1': tensor(1), 'agent_0': tensor(0)}
d= tensor([0.3133, 0.6867], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7896, 0.2104], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3260, 0.6740], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.3260, 0.6740], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 0., 0., 1.])
agent.state_act= tensor([0., 1., 1., 0.])
states= {'agent_1': tensor([1., 0., 0., 1.]), 'agent_0': tensor([0., 1., 1., 0.])}
agent.state_act= tensor([1., 0., 0., 1.])
 action, distrib= tensor(0) tensor([0.7924, 0.2076], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 0., 1.]), 'agent_0': tensor([0., 1., 1., 0.])}
agent.state_act= tensor([0., 1., 1., 0.])
 action, distrib= tensor(1) tensor([0.3476, 0.6524], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.3476, 0.6524], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 0., 1., 0.])
agent.state_act= tensor([0., 0., 0., 1.])
states= {'agent_1': tensor([0., 0., 1., 0.]), 'agent_0': tensor([0., 0., 0., 1.])}
agent.state_act= tensor([0., 0., 1., 0.])
 action, distrib= tensor(0) tensor([0.6486, 0.3514], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 0.]), 'agent_0': tensor([0., 0., 0., 1.])}
agent.state_act= tensor([0., 0., 0., 1.])
 action, distrib= tensor(1) tensor([0.4699, 0.5301], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.4699, 0.5301], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 1., 1., 0.])
agent.state_act= tensor([1., 0., 0., 1.])
states= {'agent_1': tensor([0., 1., 1., 0.]), 'agent_0': tensor([1., 0., 0., 1.])}
agent.state_act= tensor([0., 1., 1., 0.])
 action, distrib= tensor(0) tensor([0.6692, 0.3308], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 0.]), 'agent_0': tensor([1., 0., 0., 1.])}
agent.state_act= tensor([1., 0., 0., 1.])
 action, distrib= tensor(1) tensor([0.4824, 0.5176], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.4824, 0.5176], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 1., 1., 0.])
agent.state_act= tensor([1., 1., 0., 1.])
states= {'agent_1': tensor([1., 1., 1., 0.]), 'agent_0': tensor([1., 1., 0., 1.])}
agent.state_act= tensor([1., 1., 1., 0.])
 action, distrib= tensor(0) tensor([0.7862, 0.2138], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 0.]), 'agent_0': tensor([1., 1., 0., 1.])}
agent.state_act= tensor([1., 1., 0., 1.])
 action, distrib= tensor(0) tensor([0.4424, 0.5576], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.4424, 0.5576], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 0., 1., 0.])
agent.state_act= tensor([0., 0., 0., 1.])
states= {'agent_1': tensor([0., 0., 1., 0.]), 'agent_0': tensor([0., 0., 0., 1.])}
agent.state_act= tensor([0., 0., 1., 0.])
 action, distrib= tensor(1) tensor([0.6486, 0.3514], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 0.]), 'agent_0': tensor([0., 0., 0., 1.])}
agent.state_act= tensor([0., 0., 0., 1.])
 action, distrib= tensor(0) tensor([0.4699, 0.5301], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(0.), 'agent_0': tensor(6.)}
actions= {'agent_1': tensor(1), 'agent_0': tensor(0)}
d= tensor([0.4699, 0.5301], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
UPDATE
inside update
inside update
done
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7961, 0.2039], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3036, 0.6964], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7961, 0.2039], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3036, 0.6964], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7961, 0.2039], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3036, 0.6964], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7961, 0.2039], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3036, 0.6964], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7961, 0.2039], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3036, 0.6964], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7961, 0.2039], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3036, 0.6964], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7961, 0.2039], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3036, 0.6964], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7961, 0.2039], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3036, 0.6964], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7961, 0.2039], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3036, 0.6964], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7961, 0.2039], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3036, 0.6964], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
R eval= {'agent_1': tensor(47.6634), 'agent_0': tensor(1.9415)}
==========>Epoch= 13
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.7961, 0.2039], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3036, 0.6964], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(0.), 'agent_0': tensor(6.)}
actions= {'agent_1': tensor(1), 'agent_0': tensor(0)}
d= tensor([0.3036, 0.6964], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7870, 0.2130], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3114, 0.6886], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.3114, 0.6886], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 0., 0., 1.])
agent.state_act= tensor([0., 1., 1., 0.])
states= {'agent_1': tensor([1., 0., 0., 1.]), 'agent_0': tensor([0., 1., 1., 0.])}
agent.state_act= tensor([1., 0., 0., 1.])
 action, distrib= tensor(0) tensor([0.7932, 0.2068], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 0., 1.]), 'agent_0': tensor([0., 1., 1., 0.])}
agent.state_act= tensor([0., 1., 1., 0.])
 action, distrib= tensor(1) tensor([0.3413, 0.6587], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.3413, 0.6587], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 0., 1., 0.])
agent.state_act= tensor([0., 0., 0., 1.])
states= {'agent_1': tensor([0., 0., 1., 0.]), 'agent_0': tensor([0., 0., 0., 1.])}
agent.state_act= tensor([0., 0., 1., 0.])
 action, distrib= tensor(0) tensor([0.6507, 0.3493], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 0.]), 'agent_0': tensor([0., 0., 0., 1.])}
agent.state_act= tensor([0., 0., 0., 1.])
 action, distrib= tensor(0) tensor([0.4613, 0.5387], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.4613, 0.5387], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 1., 1., 0.])
agent.state_act= tensor([1., 1., 0., 1.])
states= {'agent_1': tensor([1., 1., 1., 0.]), 'agent_0': tensor([1., 1., 0., 1.])}
agent.state_act= tensor([1., 1., 1., 0.])
 action, distrib= tensor(1) tensor([0.7864, 0.2136], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 0.]), 'agent_0': tensor([1., 1., 0., 1.])}
agent.state_act= tensor([1., 1., 0., 1.])
 action, distrib= tensor(0) tensor([0.4309, 0.5691], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(0.), 'agent_0': tensor(6.)}
actions= {'agent_1': tensor(1), 'agent_0': tensor(0)}
d= tensor([0.4309, 0.5691], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 1., 0., 0.])
agent.state_act= tensor([1., 0., 0., 0.])
states= {'agent_1': tensor([0., 1., 0., 0.]), 'agent_0': tensor([1., 0., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.6651, 0.3349], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 0., 0.]), 'agent_0': tensor([1., 0., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.5389, 0.4611], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.5389, 0.4611], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 1., 0., 1.])
agent.state_act= tensor([1., 0., 1., 0.])
states= {'agent_1': tensor([0., 1., 0., 1.]), 'agent_0': tensor([1., 0., 1., 0.])}
agent.state_act= tensor([0., 1., 0., 1.])
 action, distrib= tensor(0) tensor([0.6735, 0.3265], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 0., 1.]), 'agent_0': tensor([1., 0., 1., 0.])}
agent.state_act= tensor([1., 0., 1., 0.])
 action, distrib= tensor(0) tensor([0.3920, 0.6080], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.3920, 0.6080], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 1., 0., 0.])
agent.state_act= tensor([1., 0., 0., 0.])
states= {'agent_1': tensor([0., 1., 0., 0.]), 'agent_0': tensor([1., 0., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(1) tensor([0.6651, 0.3349], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 0., 0.]), 'agent_0': tensor([1., 0., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(1) tensor([0.5389, 0.4611], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
actions= {'agent_1': tensor(1), 'agent_0': tensor(1)}
d= tensor([0.5389, 0.4611], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 0., 0., 0.])
agent.state_act= tensor([0., 1., 0., 0.])
states= {'agent_1': tensor([1., 0., 0., 0.]), 'agent_0': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.7943, 0.2057], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 0., 0.]), 'agent_0': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(1) tensor([0.4736, 0.5264], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.4736, 0.5264], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.6673, 0.3327], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3259, 0.6741], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
actions= {'agent_1': tensor(1), 'agent_0': tensor(1)}
d= tensor([0.3259, 0.6741], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
UPDATE
inside update
inside update
done
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6696, 0.3304], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3260, 0.6740], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6696, 0.3304], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3260, 0.6740], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6696, 0.3304], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3260, 0.6740], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.6696, 0.3304], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3260, 0.6740], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6696, 0.3304], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3260, 0.6740], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6696, 0.3304], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3260, 0.6740], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6696, 0.3304], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3260, 0.6740], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6696, 0.3304], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3260, 0.6740], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.6696, 0.3304], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3260, 0.6740], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(0.), 'agent_0': tensor(6.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6696, 0.3304], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3260, 0.6740], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
R eval= {'agent_1': tensor(40.9140), 'agent_0': tensor(12.3780)}
==========>Epoch= 14
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6696, 0.3304], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3260, 0.6740], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.3260, 0.6740], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6684, 0.3316], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3309, 0.6691], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.3309, 0.6691], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 1., 1., 0.])
agent.state_act= tensor([1., 1., 0., 1.])
states= {'agent_1': tensor([1., 1., 1., 0.]), 'agent_0': tensor([1., 1., 0., 1.])}
agent.state_act= tensor([1., 1., 1., 0.])
 action, distrib= tensor(1) tensor([0.7872, 0.2128], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 0.]), 'agent_0': tensor([1., 1., 0., 1.])}
agent.state_act= tensor([1., 1., 0., 1.])
 action, distrib= tensor(0) tensor([0.4348, 0.5652], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(0.), 'agent_0': tensor(6.)}
actions= {'agent_1': tensor(1), 'agent_0': tensor(0)}
d= tensor([0.4348, 0.5652], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 1., 1., 0.])
agent.state_act= tensor([1., 0., 0., 1.])
states= {'agent_1': tensor([0., 1., 1., 0.]), 'agent_0': tensor([1., 0., 0., 1.])}
agent.state_act= tensor([0., 1., 1., 0.])
 action, distrib= tensor(0) tensor([0.6617, 0.3383], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 0.]), 'agent_0': tensor([1., 0., 0., 1.])}
agent.state_act= tensor([1., 0., 0., 1.])
 action, distrib= tensor(1) tensor([0.4646, 0.5354], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.4646, 0.5354], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 1., 0., 1.])
agent.state_act= tensor([1., 1., 1., 0.])
states= {'agent_1': tensor([1., 1., 0., 1.]), 'agent_0': tensor([1., 1., 1., 0.])}
agent.state_act= tensor([1., 1., 0., 1.])
 action, distrib= tensor(1) tensor([0.7795, 0.2205], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 0., 1.]), 'agent_0': tensor([1., 1., 1., 0.])}
agent.state_act= tensor([1., 1., 1., 0.])
 action, distrib= tensor(1) tensor([0.3542, 0.6458], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
actions= {'agent_1': tensor(1), 'agent_0': tensor(1)}
d= tensor([0.3542, 0.6458], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 1., 1., 0.])
agent.state_act= tensor([1., 1., 0., 1.])
states= {'agent_1': tensor([1., 1., 1., 0.]), 'agent_0': tensor([1., 1., 0., 1.])}
agent.state_act= tensor([1., 1., 1., 0.])
 action, distrib= tensor(0) tensor([0.7872, 0.2128], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 0.]), 'agent_0': tensor([1., 1., 0., 1.])}
agent.state_act= tensor([1., 1., 0., 1.])
 action, distrib= tensor(1) tensor([0.4348, 0.5652], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.4348, 0.5652], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.7984, 0.2016], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3092, 0.6908], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(0.), 'agent_0': tensor(6.)}
actions= {'agent_1': tensor(1), 'agent_0': tensor(0)}
d= tensor([0.3092, 0.6908], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 1., 1., 0.])
agent.state_act= tensor([1., 1., 0., 1.])
states= {'agent_1': tensor([1., 1., 1., 0.]), 'agent_0': tensor([1., 1., 0., 1.])}
agent.state_act= tensor([1., 1., 1., 0.])
 action, distrib= tensor(1) tensor([0.7872, 0.2128], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 0.]), 'agent_0': tensor([1., 1., 0., 1.])}
agent.state_act= tensor([1., 1., 0., 1.])
 action, distrib= tensor(1) tensor([0.4348, 0.5652], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
actions= {'agent_1': tensor(1), 'agent_0': tensor(1)}
d= tensor([0.4348, 0.5652], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 1., 0., 1.])
agent.state_act= tensor([1., 1., 1., 0.])
states= {'agent_1': tensor([1., 1., 0., 1.]), 'agent_0': tensor([1., 1., 1., 0.])}
agent.state_act= tensor([1., 1., 0., 1.])
 action, distrib= tensor(0) tensor([0.7795, 0.2205], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 0., 1.]), 'agent_0': tensor([1., 1., 1., 0.])}
agent.state_act= tensor([1., 1., 1., 0.])
 action, distrib= tensor(1) tensor([0.3542, 0.6458], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.3542, 0.6458], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7984, 0.2016], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3092, 0.6908], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.3092, 0.6908], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
UPDATE
inside update
inside update
done
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.6788, 0.3212], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3293, 0.6707], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.6788, 0.3212], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3293, 0.6707], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.6788, 0.3212], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3293, 0.6707], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6788, 0.3212], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3293, 0.6707], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.6788, 0.3212], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3293, 0.6707], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(0.), 'agent_0': tensor(6.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6788, 0.3212], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3293, 0.6707], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6788, 0.3212], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3293, 0.6707], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.6788, 0.3212], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3293, 0.6707], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6788, 0.3212], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3293, 0.6707], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6788, 0.3212], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3293, 0.6707], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
R eval= {'agent_1': tensor(42.9501), 'agent_0': tensor(26.2254)}
==========>Epoch= 15
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6788, 0.3212], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3293, 0.6707], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.3293, 0.6707], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6723, 0.3277], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3294, 0.6706], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.3294, 0.6706], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 1., 1., 0.])
agent.state_act= tensor([1., 1., 0., 1.])
states= {'agent_1': tensor([1., 1., 1., 0.]), 'agent_0': tensor([1., 1., 0., 1.])}
agent.state_act= tensor([1., 1., 1., 0.])
 action, distrib= tensor(0) tensor([0.7918, 0.2082], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 0.]), 'agent_0': tensor([1., 1., 0., 1.])}
agent.state_act= tensor([1., 1., 0., 1.])
 action, distrib= tensor(1) tensor([0.4462, 0.5538], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.4462, 0.5538], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 0., 1., 0.])
agent.state_act= tensor([0., 1., 0., 1.])
states= {'agent_1': tensor([1., 0., 1., 0.]), 'agent_0': tensor([0., 1., 0., 1.])}
agent.state_act= tensor([1., 0., 1., 0.])
 action, distrib= tensor(0) tensor([0.8018, 0.1982], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 0.]), 'agent_0': tensor([0., 1., 0., 1.])}
agent.state_act= tensor([0., 1., 0., 1.])
 action, distrib= tensor(0) tensor([0.4528, 0.5472], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.4528, 0.5472], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 1., 0.])
agent.state_act= tensor([0., 1., 0., 1.])
states= {'agent_1': tensor([1., 0., 1., 0.]), 'agent_0': tensor([0., 1., 0., 1.])}
agent.state_act= tensor([1., 0., 1., 0.])
 action, distrib= tensor(0) tensor([0.8018, 0.1982], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 0.]), 'agent_0': tensor([0., 1., 0., 1.])}
agent.state_act= tensor([0., 1., 0., 1.])
 action, distrib= tensor(0) tensor([0.4528, 0.5472], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.4528, 0.5472], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 0., 0.])
agent.state_act= tensor([0., 1., 0., 0.])
states= {'agent_1': tensor([1., 0., 0., 0.]), 'agent_0': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.8019, 0.1981], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 0., 0.]), 'agent_0': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.5031, 0.4969], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.5031, 0.4969], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 0., 0.])
agent.state_act= tensor([0., 1., 0., 0.])
states= {'agent_1': tensor([1., 0., 0., 0.]), 'agent_0': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.8019, 0.1981], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 0., 0.]), 'agent_0': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.5031, 0.4969], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.5031, 0.4969], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 0., 0.])
agent.state_act= tensor([0., 1., 0., 0.])
states= {'agent_1': tensor([1., 0., 0., 0.]), 'agent_0': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.8019, 0.1981], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 0., 0.]), 'agent_0': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(1) tensor([0.5031, 0.4969], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.5031, 0.4969], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 0., 0., 0.])
agent.state_act= tensor([0., 0., 0., 0.])
states= {'agent_1': tensor([0., 0., 0., 0.]), 'agent_0': tensor([0., 0., 0., 0.])}
agent.state_act= tensor([0., 0., 0., 0.])
 action, distrib= tensor(1) tensor([0.6632, 0.3368], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 0., 0.]), 'agent_0': tensor([0., 0., 0., 0.])}
agent.state_act= tensor([0., 0., 0., 0.])
 action, distrib= tensor(1) tensor([0.5416, 0.4584], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
actions= {'agent_1': tensor(1), 'agent_0': tensor(1)}
d= tensor([0.5416, 0.4584], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 0., 1., 0.])
agent.state_act= tensor([0., 0., 0., 1.])
states= {'agent_1': tensor([0., 0., 1., 0.]), 'agent_0': tensor([0., 0., 0., 1.])}
agent.state_act= tensor([0., 0., 1., 0.])
 action, distrib= tensor(0) tensor([0.6667, 0.3333], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 0.]), 'agent_0': tensor([0., 0., 0., 1.])}
agent.state_act= tensor([0., 0., 0., 1.])
 action, distrib= tensor(1) tensor([0.4728, 0.5272], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.4728, 0.5272], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
UPDATE
inside update
inside update
done
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.6835, 0.3165], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3296, 0.6704], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.6835, 0.3165], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3296, 0.6704], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6835, 0.3165], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3296, 0.6704], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6835, 0.3165], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3296, 0.6704], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6835, 0.3165], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3296, 0.6704], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6835, 0.3165], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3296, 0.6704], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.6835, 0.3165], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3296, 0.6704], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.6835, 0.3165], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3296, 0.6704], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6835, 0.3165], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3296, 0.6704], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6835, 0.3165], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3296, 0.6704], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
R eval= {'agent_1': tensor(53.5072), 'agent_0': tensor(19.3177)}
==========>Epoch= 16
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3296, 0.6704], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.6835, 0.3165], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(6.), 'agent_1': tensor(0.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(1)}
d= tensor([0.6835, 0.3165], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3351, 0.6649], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6938, 0.3062], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(0)}
d= tensor([0.6938, 0.3062], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 1., 1., 0.])
agent.state_act= tensor([1., 1., 0., 1.])
states= {'agent_0': tensor([1., 1., 1., 0.]), 'agent_1': tensor([1., 1., 0., 1.])}
agent.state_act= tensor([1., 1., 1., 0.])
 action, distrib= tensor(0) tensor([0.3703, 0.6297], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 1., 0.]), 'agent_1': tensor([1., 1., 0., 1.])}
agent.state_act= tensor([1., 1., 0., 1.])
 action, distrib= tensor(0) tensor([0.7870, 0.2130], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(0)}
d= tensor([0.7870, 0.2130], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 0., 0., 0.])
agent.state_act= tensor([0., 0., 0., 0.])
states= {'agent_0': tensor([0., 0., 0., 0.]), 'agent_1': tensor([0., 0., 0., 0.])}
agent.state_act= tensor([0., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.5540, 0.4460], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 0., 0.]), 'agent_1': tensor([0., 0., 0., 0.])}
agent.state_act= tensor([0., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.6734, 0.3266], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(0)}
d= tensor([0.6734, 0.3266], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 1., 0., 0.])
agent.state_act= tensor([1., 1., 0., 0.])
states= {'agent_0': tensor([1., 1., 0., 0.]), 'agent_1': tensor([1., 1., 0., 0.])}
agent.state_act= tensor([1., 1., 0., 0.])
 action, distrib= tensor(1) tensor([0.5219, 0.4781], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 0., 0.]), 'agent_1': tensor([1., 1., 0., 0.])}
agent.state_act= tensor([1., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.7936, 0.2064], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.7936, 0.2064], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 1., 0., 0.])
agent.state_act= tensor([1., 0., 0., 0.])
states= {'agent_0': tensor([0., 1., 0., 0.]), 'agent_1': tensor([1., 0., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.5262, 0.4738], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 0., 0.]), 'agent_1': tensor([1., 0., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.8094, 0.1906], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(0)}
d= tensor([0.8094, 0.1906], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 1., 0., 1.])
agent.state_act= tensor([1., 0., 1., 0.])
states= {'agent_0': tensor([0., 1., 0., 1.]), 'agent_1': tensor([1., 0., 1., 0.])}
agent.state_act= tensor([0., 1., 0., 1.])
 action, distrib= tensor(1) tensor([0.4769, 0.5231], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 0., 1.]), 'agent_1': tensor([1., 0., 1., 0.])}
agent.state_act= tensor([1., 0., 1., 0.])
 action, distrib= tensor(1) tensor([0.8114, 0.1886], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(1)}
d= tensor([0.8114, 0.1886], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 0., 0., 0.])
agent.state_act= tensor([0., 1., 0., 0.])
states= {'agent_0': tensor([1., 0., 0., 0.]), 'agent_1': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.5438, 0.4562], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 0., 0.]), 'agent_1': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.6681, 0.3319], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(0)}
d= tensor([0.6681, 0.3319], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3296, 0.6704], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6835, 0.3165], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(0)}
d= tensor([0.6835, 0.3165], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 0., 0.])
agent.state_act= tensor([0., 1., 0., 0.])
states= {'agent_0': tensor([1., 0., 0., 0.]), 'agent_1': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.5438, 0.4562], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 0., 0.]), 'agent_1': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.6681, 0.3319], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(0)}
d= tensor([0.6681, 0.3319], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
UPDATE
inside update
inside update
done
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3354, 0.6646], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.6853, 0.3147], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3354, 0.6646], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.6853, 0.3147], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3354, 0.6646], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6853, 0.3147], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3354, 0.6646], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.6853, 0.3147], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3354, 0.6646], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.6853, 0.3147], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(6.), 'agent_1': tensor(0.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3354, 0.6646], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6853, 0.3147], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3354, 0.6646], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6853, 0.3147], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3354, 0.6646], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6853, 0.3147], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3354, 0.6646], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6853, 0.3147], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3354, 0.6646], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6853, 0.3147], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
R eval= {'agent_0': tensor(23.4002), 'agent_1': tensor(34.4714)}
==========>Epoch= 17
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6853, 0.3147], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3354, 0.6646], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.3354, 0.6646], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.8014, 0.1986], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3406, 0.6594], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.3406, 0.6594], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 0., 1., 0.])
agent.state_act= tensor([0., 1., 0., 1.])
states= {'agent_1': tensor([1., 0., 1., 0.]), 'agent_0': tensor([0., 1., 0., 1.])}
agent.state_act= tensor([1., 0., 1., 0.])
 action, distrib= tensor(0) tensor([0.8188, 0.1812], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 0.]), 'agent_0': tensor([0., 1., 0., 1.])}
agent.state_act= tensor([0., 1., 0., 1.])
 action, distrib= tensor(0) tensor([0.5010, 0.4990], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.5010, 0.4990], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 1., 0.])
agent.state_act= tensor([0., 1., 0., 1.])
states= {'agent_1': tensor([1., 0., 1., 0.]), 'agent_0': tensor([0., 1., 0., 1.])}
agent.state_act= tensor([1., 0., 1., 0.])
 action, distrib= tensor(0) tensor([0.8188, 0.1812], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 0.]), 'agent_0': tensor([0., 1., 0., 1.])}
agent.state_act= tensor([0., 1., 0., 1.])
 action, distrib= tensor(1) tensor([0.5010, 0.4990], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.5010, 0.4990], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 0., 0., 0.])
agent.state_act= tensor([0., 0., 0., 0.])
states= {'agent_1': tensor([0., 0., 0., 0.]), 'agent_0': tensor([0., 0., 0., 0.])}
agent.state_act= tensor([0., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.6800, 0.3200], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 0., 0.]), 'agent_0': tensor([0., 0., 0., 0.])}
agent.state_act= tensor([0., 0., 0., 0.])
 action, distrib= tensor(1) tensor([0.5636, 0.4364], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.5636, 0.4364], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 1., 1., 0.])
agent.state_act= tensor([1., 0., 0., 1.])
states= {'agent_1': tensor([0., 1., 1., 0.]), 'agent_0': tensor([1., 0., 0., 1.])}
agent.state_act= tensor([0., 1., 1., 0.])
 action, distrib= tensor(1) tensor([0.6784, 0.3216], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 0.]), 'agent_0': tensor([1., 0., 0., 1.])}
agent.state_act= tensor([1., 0., 0., 1.])
 action, distrib= tensor(1) tensor([0.4805, 0.5195], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
actions= {'agent_1': tensor(1), 'agent_0': tensor(1)}
d= tensor([0.4805, 0.5195], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 0., 1., 0.])
agent.state_act= tensor([0., 1., 0., 1.])
states= {'agent_1': tensor([1., 0., 1., 0.]), 'agent_0': tensor([0., 1., 0., 1.])}
agent.state_act= tensor([1., 0., 1., 0.])
 action, distrib= tensor(0) tensor([0.8188, 0.1812], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 0.]), 'agent_0': tensor([0., 1., 0., 1.])}
agent.state_act= tensor([0., 1., 0., 1.])
 action, distrib= tensor(0) tensor([0.5010, 0.4990], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.5010, 0.4990], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.8214, 0.1786], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3536, 0.6464], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.3536, 0.6464], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 0., 0., 0.])
agent.state_act= tensor([0., 0., 0., 0.])
states= {'agent_1': tensor([0., 0., 0., 0.]), 'agent_0': tensor([0., 0., 0., 0.])}
agent.state_act= tensor([0., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.6800, 0.3200], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 0., 0.]), 'agent_0': tensor([0., 0., 0., 0.])}
agent.state_act= tensor([0., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.5636, 0.4364], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.5636, 0.4364], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 1., 1., 0.])
agent.state_act= tensor([1., 1., 0., 1.])
states= {'agent_1': tensor([1., 1., 1., 0.]), 'agent_0': tensor([1., 1., 0., 1.])}
agent.state_act= tensor([1., 1., 1., 0.])
 action, distrib= tensor(0) tensor([0.8030, 0.1970], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 0.]), 'agent_0': tensor([1., 1., 0., 1.])}
agent.state_act= tensor([1., 1., 0., 1.])
 action, distrib= tensor(0) tensor([0.4809, 0.5191], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.4809, 0.5191], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
UPDATE
inside update
inside update
done
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7144, 0.2856], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3512, 0.6488], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7144, 0.2856], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3512, 0.6488], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7144, 0.2856], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3512, 0.6488], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.7144, 0.2856], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3512, 0.6488], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7144, 0.2856], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3512, 0.6488], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.7144, 0.2856], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3512, 0.6488], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7144, 0.2856], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3512, 0.6488], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7144, 0.2856], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3512, 0.6488], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7144, 0.2856], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3512, 0.6488], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.7144, 0.2856], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3512, 0.6488], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
R eval= {'agent_1': tensor(44.9281), 'agent_0': tensor(16.0956)}
==========>Epoch= 18
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7144, 0.2856], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3512, 0.6488], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.3512, 0.6488], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.8073, 0.1927], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3441, 0.6559], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.3441, 0.6559], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 0., 0., 0.])
agent.state_act= tensor([0., 0., 0., 0.])
states= {'agent_1': tensor([0., 0., 0., 0.]), 'agent_0': tensor([0., 0., 0., 0.])}
agent.state_act= tensor([0., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.6866, 0.3134], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 0., 0.]), 'agent_0': tensor([0., 0., 0., 0.])}
agent.state_act= tensor([0., 0., 0., 0.])
 action, distrib= tensor(1) tensor([0.5649, 0.4351], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.5649, 0.4351], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 1., 0., 0.])
agent.state_act= tensor([1., 0., 0., 0.])
states= {'agent_1': tensor([0., 1., 0., 0.]), 'agent_0': tensor([1., 0., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.6674, 0.3326], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 0., 0.]), 'agent_0': tensor([1., 0., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(1) tensor([0.5399, 0.4601], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.5399, 0.4601], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 1., 1., 0.])
agent.state_act= tensor([1., 1., 0., 1.])
states= {'agent_1': tensor([1., 1., 1., 0.]), 'agent_0': tensor([1., 1., 0., 1.])}
agent.state_act= tensor([1., 1., 1., 0.])
 action, distrib= tensor(0) tensor([0.8083, 0.1917], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 0.]), 'agent_0': tensor([1., 1., 0., 1.])}
agent.state_act= tensor([1., 1., 0., 1.])
 action, distrib= tensor(0) tensor([0.4829, 0.5171], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.4829, 0.5171], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 0., 1., 0.])
agent.state_act= tensor([0., 0., 0., 1.])
states= {'agent_1': tensor([0., 0., 1., 0.]), 'agent_0': tensor([0., 0., 0., 1.])}
agent.state_act= tensor([0., 0., 1., 0.])
 action, distrib= tensor(0) tensor([0.7006, 0.2994], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 0.]), 'agent_0': tensor([0., 0., 0., 1.])}
agent.state_act= tensor([0., 0., 0., 1.])
 action, distrib= tensor(0) tensor([0.5038, 0.4962], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.5038, 0.4962], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 1., 0., 0.])
agent.state_act= tensor([1., 1., 0., 0.])
states= {'agent_1': tensor([1., 1., 0., 0.]), 'agent_0': tensor([1., 1., 0., 0.])}
agent.state_act= tensor([1., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.7986, 0.2014], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 0., 0.]), 'agent_0': tensor([1., 1., 0., 0.])}
agent.state_act= tensor([1., 1., 0., 0.])
 action, distrib= tensor(1) tensor([0.5373, 0.4627], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.5373, 0.4627], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 0., 0., 0.])
agent.state_act= tensor([0., 1., 0., 0.])
states= {'agent_1': tensor([1., 0., 0., 0.]), 'agent_0': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.8213, 0.1787], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 0., 0.]), 'agent_0': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.5556, 0.4444], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.5556, 0.4444], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 1., 0.])
agent.state_act= tensor([0., 1., 0., 1.])
states= {'agent_1': tensor([1., 0., 1., 0.]), 'agent_0': tensor([0., 1., 0., 1.])}
agent.state_act= tensor([1., 0., 1., 0.])
 action, distrib= tensor(0) tensor([0.8264, 0.1736], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 0.]), 'agent_0': tensor([0., 1., 0., 1.])}
agent.state_act= tensor([0., 1., 0., 1.])
 action, distrib= tensor(0) tensor([0.5105, 0.4895], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.5105, 0.4895], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 0., 0.])
agent.state_act= tensor([0., 1., 0., 0.])
states= {'agent_1': tensor([1., 0., 0., 0.]), 'agent_0': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.8213, 0.1787], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 0., 0.]), 'agent_0': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(1) tensor([0.5556, 0.4444], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.5556, 0.4444], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
UPDATE
inside update
inside update
done
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7269, 0.2731], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3644, 0.6356], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7269, 0.2731], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3644, 0.6356], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7269, 0.2731], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3644, 0.6356], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.7269, 0.2731], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3644, 0.6356], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(0.), 'agent_0': tensor(6.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7269, 0.2731], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3644, 0.6356], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7269, 0.2731], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3644, 0.6356], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7269, 0.2731], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3644, 0.6356], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.7269, 0.2731], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3644, 0.6356], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(0.), 'agent_0': tensor(6.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.7269, 0.2731], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3644, 0.6356], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7269, 0.2731], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3644, 0.6356], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
R eval= {'agent_1': tensor(30.7133), 'agent_0': tensor(18.8920)}
==========>Epoch= 19
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3644, 0.6356], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7269, 0.2731], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.7269, 0.2731], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3407, 0.6593], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7005, 0.2995], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(0)}
d= tensor([0.7005, 0.2995], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 0., 1.])
agent.state_act= tensor([0., 1., 1., 0.])
states= {'agent_0': tensor([1., 0., 0., 1.]), 'agent_1': tensor([0., 1., 1., 0.])}
agent.state_act= tensor([1., 0., 0., 1.])
 action, distrib= tensor(0) tensor([0.4771, 0.5229], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 0., 1.]), 'agent_1': tensor([0., 1., 1., 0.])}
agent.state_act= tensor([0., 1., 1., 0.])
 action, distrib= tensor(1) tensor([0.6902, 0.3098], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(6.), 'agent_1': tensor(0.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(1)}
d= tensor([0.6902, 0.3098], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 0., 0., 0.])
agent.state_act= tensor([0., 0., 0., 0.])
states= {'agent_0': tensor([0., 0., 0., 0.]), 'agent_1': tensor([0., 0., 0., 0.])}
agent.state_act= tensor([0., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.5686, 0.4314], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 0., 0.]), 'agent_1': tensor([0., 0., 0., 0.])}
agent.state_act= tensor([0., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.6939, 0.3061], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(0)}
d= tensor([0.6939, 0.3061], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 1., 1., 0.])
agent.state_act= tensor([1., 1., 0., 1.])
states= {'agent_0': tensor([1., 1., 1., 0.]), 'agent_1': tensor([1., 1., 0., 1.])}
agent.state_act= tensor([1., 1., 1., 0.])
 action, distrib= tensor(1) tensor([0.4007, 0.5993], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 1., 0.]), 'agent_1': tensor([1., 1., 0., 1.])}
agent.state_act= tensor([1., 1., 0., 1.])
 action, distrib= tensor(0) tensor([0.7977, 0.2023], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.7977, 0.2023], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 1., 0., 0.])
agent.state_act= tensor([1., 0., 0., 0.])
states= {'agent_0': tensor([0., 1., 0., 0.]), 'agent_1': tensor([1., 0., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(1) tensor([0.5681, 0.4319], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 0., 0.]), 'agent_1': tensor([1., 0., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.8263, 0.1737], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.8263, 0.1737], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 0., 0., 1.])
agent.state_act= tensor([0., 0., 1., 0.])
states= {'agent_0': tensor([0., 0., 0., 1.]), 'agent_1': tensor([0., 0., 1., 0.])}
agent.state_act= tensor([0., 0., 0., 1.])
 action, distrib= tensor(0) tensor([0.5112, 0.4888], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 0., 1.]), 'agent_1': tensor([0., 0., 1., 0.])}
agent.state_act= tensor([0., 0., 1., 0.])
 action, distrib= tensor(0) tensor([0.7111, 0.2889], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(0)}
d= tensor([0.7111, 0.2889], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 1., 0., 1.])
agent.state_act= tensor([1., 1., 1., 0.])
states= {'agent_0': tensor([1., 1., 0., 1.]), 'agent_1': tensor([1., 1., 1., 0.])}
agent.state_act= tensor([1., 1., 0., 1.])
 action, distrib= tensor(0) tensor([0.4930, 0.5070], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 0., 1.]), 'agent_1': tensor([1., 1., 1., 0.])}
agent.state_act= tensor([1., 1., 1., 0.])
 action, distrib= tensor(0) tensor([0.8133, 0.1867], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(0)}
d= tensor([0.8133, 0.1867], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 0., 0., 0.])
agent.state_act= tensor([0., 0., 0., 0.])
states= {'agent_0': tensor([0., 0., 0., 0.]), 'agent_1': tensor([0., 0., 0., 0.])}
agent.state_act= tensor([0., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.5686, 0.4314], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 0., 0.]), 'agent_1': tensor([0., 0., 0., 0.])}
agent.state_act= tensor([0., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.6939, 0.3061], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(0)}
d= tensor([0.6939, 0.3061], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 1., 0., 0.])
agent.state_act= tensor([1., 1., 0., 0.])
states= {'agent_0': tensor([1., 1., 0., 0.]), 'agent_1': tensor([1., 1., 0., 0.])}
agent.state_act= tensor([1., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.5445, 0.4555], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 0., 0.]), 'agent_1': tensor([1., 1., 0., 0.])}
agent.state_act= tensor([1., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.8018, 0.1982], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(0)}
d= tensor([0.8018, 0.1982], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
UPDATE
inside update
inside update
done
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3787, 0.6213], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.7368, 0.2632], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(6.), 'agent_1': tensor(0.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3787, 0.6213], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7368, 0.2632], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3787, 0.6213], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7368, 0.2632], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3787, 0.6213], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7368, 0.2632], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3787, 0.6213], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7368, 0.2632], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3787, 0.6213], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7368, 0.2632], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3787, 0.6213], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7368, 0.2632], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3787, 0.6213], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.7368, 0.2632], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(6.), 'agent_1': tensor(0.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3787, 0.6213], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7368, 0.2632], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3787, 0.6213], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7368, 0.2632], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
R eval= {'agent_0': tensor(15.3976), 'agent_1': tensor(26.7523)}
==========>Epoch= 20
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7368, 0.2632], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3787, 0.6213], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.3787, 0.6213], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7061, 0.2939], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3519, 0.6481], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.3519, 0.6481], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 1., 1., 0.])
agent.state_act= tensor([1., 1., 0., 1.])
states= {'agent_1': tensor([1., 1., 1., 0.]), 'agent_0': tensor([1., 1., 0., 1.])}
agent.state_act= tensor([1., 1., 1., 0.])
 action, distrib= tensor(0) tensor([0.8139, 0.1861], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 0.]), 'agent_0': tensor([1., 1., 0., 1.])}
agent.state_act= tensor([1., 1., 0., 1.])
 action, distrib= tensor(1) tensor([0.5018, 0.4982], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.5018, 0.4982], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 0., 1., 0.])
agent.state_act= tensor([0., 1., 0., 1.])
states= {'agent_1': tensor([1., 0., 1., 0.]), 'agent_0': tensor([0., 1., 0., 1.])}
agent.state_act= tensor([1., 0., 1., 0.])
 action, distrib= tensor(1) tensor([0.8359, 0.1641], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 0.]), 'agent_0': tensor([0., 1., 0., 1.])}
agent.state_act= tensor([0., 1., 0., 1.])
 action, distrib= tensor(1) tensor([0.5370, 0.4630], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
actions= {'agent_1': tensor(1), 'agent_0': tensor(1)}
d= tensor([0.5370, 0.4630], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 1., 1., 0.])
agent.state_act= tensor([1., 0., 0., 1.])
states= {'agent_1': tensor([0., 1., 1., 0.]), 'agent_0': tensor([1., 0., 0., 1.])}
agent.state_act= tensor([0., 1., 1., 0.])
 action, distrib= tensor(0) tensor([0.6916, 0.3084], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 0.]), 'agent_0': tensor([1., 0., 0., 1.])}
agent.state_act= tensor([1., 0., 0., 1.])
 action, distrib= tensor(0) tensor([0.4804, 0.5196], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.4804, 0.5196], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7061, 0.2939], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3519, 0.6481], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.3519, 0.6481], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 1., 0., 0.])
agent.state_act= tensor([1., 1., 0., 0.])
states= {'agent_1': tensor([1., 1., 0., 0.]), 'agent_0': tensor([1., 1., 0., 0.])}
agent.state_act= tensor([1., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.8014, 0.1986], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 0., 0.]), 'agent_0': tensor([1., 1., 0., 0.])}
agent.state_act= tensor([1., 1., 0., 0.])
 action, distrib= tensor(1) tensor([0.5494, 0.4506], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.5494, 0.4506], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 0., 1., 0.])
agent.state_act= tensor([0., 1., 0., 1.])
states= {'agent_1': tensor([1., 0., 1., 0.]), 'agent_0': tensor([0., 1., 0., 1.])}
agent.state_act= tensor([1., 0., 1., 0.])
 action, distrib= tensor(0) tensor([0.8359, 0.1641], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 0.]), 'agent_0': tensor([0., 1., 0., 1.])}
agent.state_act= tensor([0., 1., 0., 1.])
 action, distrib= tensor(0) tensor([0.5370, 0.4630], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.5370, 0.4630], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 1., 0.])
agent.state_act= tensor([0., 1., 0., 1.])
states= {'agent_1': tensor([1., 0., 1., 0.]), 'agent_0': tensor([0., 1., 0., 1.])}
agent.state_act= tensor([1., 0., 1., 0.])
 action, distrib= tensor(0) tensor([0.8359, 0.1641], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 0.]), 'agent_0': tensor([0., 1., 0., 1.])}
agent.state_act= tensor([0., 1., 0., 1.])
 action, distrib= tensor(0) tensor([0.5370, 0.4630], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.5370, 0.4630], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 0., 0.])
agent.state_act= tensor([0., 1., 0., 0.])
states= {'agent_1': tensor([1., 0., 0., 0.]), 'agent_0': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.8286, 0.1714], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 0., 0.]), 'agent_0': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.5754, 0.4246], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.5754, 0.4246], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
UPDATE
inside update
inside update
done
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.8446, 0.1554], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.4068, 0.5932], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.8446, 0.1554], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.4068, 0.5932], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.8446, 0.1554], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.4068, 0.5932], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.8446, 0.1554], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.4068, 0.5932], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.8446, 0.1554], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.4068, 0.5932], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.8446, 0.1554], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.4068, 0.5932], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.8446, 0.1554], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.4068, 0.5932], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.8446, 0.1554], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.4068, 0.5932], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.8446, 0.1554], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.4068, 0.5932], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.8446, 0.1554], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.4068, 0.5932], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
R eval= {'agent_1': tensor(37.8619), 'agent_0': tensor(3.9018)}
Epoch : 20 	 Measure: nan
==========>Epoch= 21
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.4068, 0.5932], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.8446, 0.1554], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.8446, 0.1554], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3823, 0.6177], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.7461, 0.2539], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(1)}
d= tensor([0.7461, 0.2539], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 0., 0., 1.])
agent.state_act= tensor([0., 0., 1., 0.])
states= {'agent_0': tensor([0., 0., 0., 1.]), 'agent_1': tensor([0., 0., 1., 0.])}
agent.state_act= tensor([0., 0., 0., 1.])
 action, distrib= tensor(1) tensor([0.5111, 0.4889], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 0., 1.]), 'agent_1': tensor([0., 0., 1., 0.])}
agent.state_act= tensor([0., 0., 1., 0.])
 action, distrib= tensor(0) tensor([0.7219, 0.2781], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.7219, 0.2781], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3543, 0.6457], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7131, 0.2869], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.7131, 0.2869], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 1., 0., 1.])
agent.state_act= tensor([1., 1., 1., 0.])
states= {'agent_0': tensor([1., 1., 0., 1.]), 'agent_1': tensor([1., 1., 1., 0.])}
agent.state_act= tensor([1., 1., 0., 1.])
 action, distrib= tensor(0) tensor([0.4952, 0.5048], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 0., 1.]), 'agent_1': tensor([1., 1., 1., 0.])}
agent.state_act= tensor([1., 1., 1., 0.])
 action, distrib= tensor(0) tensor([0.8131, 0.1869], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(0)}
d= tensor([0.8131, 0.1869], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 0., 0., 1.])
agent.state_act= tensor([0., 0., 1., 0.])
states= {'agent_0': tensor([0., 0., 0., 1.]), 'agent_1': tensor([0., 0., 1., 0.])}
agent.state_act= tensor([0., 0., 0., 1.])
 action, distrib= tensor(1) tensor([0.5111, 0.4889], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 0., 1.]), 'agent_1': tensor([0., 0., 1., 0.])}
agent.state_act= tensor([0., 0., 1., 0.])
 action, distrib= tensor(1) tensor([0.7219, 0.2781], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(1)}
d= tensor([0.7219, 0.2781], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 0., 0., 0.])
agent.state_act= tensor([0., 0., 0., 0.])
states= {'agent_0': tensor([0., 0., 0., 0.]), 'agent_1': tensor([0., 0., 0., 0.])}
agent.state_act= tensor([0., 0., 0., 0.])
 action, distrib= tensor(1) tensor([0.5636, 0.4364], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 0., 0.]), 'agent_1': tensor([0., 0., 0., 0.])}
agent.state_act= tensor([0., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.7013, 0.2987], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.7013, 0.2987], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3543, 0.6457], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7131, 0.2869], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.7131, 0.2869], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 1., 0., 1.])
agent.state_act= tensor([1., 1., 1., 0.])
states= {'agent_0': tensor([1., 1., 0., 1.]), 'agent_1': tensor([1., 1., 1., 0.])}
agent.state_act= tensor([1., 1., 0., 1.])
 action, distrib= tensor(0) tensor([0.4952, 0.5048], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 0., 1.]), 'agent_1': tensor([1., 1., 1., 0.])}
agent.state_act= tensor([1., 1., 1., 0.])
 action, distrib= tensor(1) tensor([0.8131, 0.1869], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(6.), 'agent_1': tensor(0.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(1)}
d= tensor([0.8131, 0.1869], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 0., 0., 1.])
agent.state_act= tensor([0., 1., 1., 0.])
states= {'agent_0': tensor([1., 0., 0., 1.]), 'agent_1': tensor([0., 1., 1., 0.])}
agent.state_act= tensor([1., 0., 0., 1.])
 action, distrib= tensor(1) tensor([0.4738, 0.5262], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 0., 1.]), 'agent_1': tensor([0., 1., 1., 0.])}
agent.state_act= tensor([0., 1., 1., 0.])
 action, distrib= tensor(0) tensor([0.6934, 0.3066], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.6934, 0.3066], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
UPDATE
inside update
inside update
done
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3764, 0.6236], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.8238, 0.1762], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3764, 0.6236], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.8238, 0.1762], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3764, 0.6236], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.8238, 0.1762], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3764, 0.6236], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.8238, 0.1762], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3764, 0.6236], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.8238, 0.1762], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3764, 0.6236], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.8238, 0.1762], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3764, 0.6236], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.8238, 0.1762], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3764, 0.6236], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.8238, 0.1762], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3764, 0.6236], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.8238, 0.1762], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3764, 0.6236], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.8238, 0.1762], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
R eval= {'agent_0': tensor(4.8515), 'agent_1': tensor(56.4005)}
==========>Epoch= 22
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3764, 0.6236], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.8238, 0.1762], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.8238, 0.1762], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.4056, 0.5944], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.8492, 0.1508], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(1)}
d= tensor([0.8492, 0.1508], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 0., 0., 1.])
agent.state_act= tensor([0., 1., 1., 0.])
states= {'agent_0': tensor([1., 0., 0., 1.]), 'agent_1': tensor([0., 1., 1., 0.])}
agent.state_act= tensor([1., 0., 0., 1.])
 action, distrib= tensor(0) tensor([0.4692, 0.5308], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 0., 1.]), 'agent_1': tensor([0., 1., 1., 0.])}
agent.state_act= tensor([0., 1., 1., 0.])
 action, distrib= tensor(0) tensor([0.6968, 0.3032], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(0)}
d= tensor([0.6968, 0.3032], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3548, 0.6452], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.7192, 0.2808], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(1)}
d= tensor([0.7192, 0.2808], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 1., 0., 0.])
agent.state_act= tensor([1., 0., 0., 0.])
states= {'agent_0': tensor([0., 1., 0., 0.]), 'agent_1': tensor([1., 0., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(1) tensor([0.5646, 0.4354], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 0., 0.]), 'agent_1': tensor([1., 0., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.8313, 0.1687], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.8313, 0.1687], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3823, 0.6177], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7533, 0.2467], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(0)}
d= tensor([0.7533, 0.2467], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 1., 0., 1.])
agent.state_act= tensor([1., 1., 1., 0.])
states= {'agent_0': tensor([1., 1., 0., 1.]), 'agent_1': tensor([1., 1., 1., 0.])}
agent.state_act= tensor([1., 1., 0., 1.])
 action, distrib= tensor(0) tensor([0.4893, 0.5107], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 0., 1.]), 'agent_1': tensor([1., 1., 1., 0.])}
agent.state_act= tensor([1., 1., 1., 0.])
 action, distrib= tensor(0) tensor([0.8163, 0.1837], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(0)}
d= tensor([0.8163, 0.1837], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 0., 0., 0.])
agent.state_act= tensor([0., 0., 0., 0.])
states= {'agent_0': tensor([0., 0., 0., 0.]), 'agent_1': tensor([0., 0., 0., 0.])}
agent.state_act= tensor([0., 0., 0., 0.])
 action, distrib= tensor(1) tensor([0.5589, 0.4411], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 0., 0.]), 'agent_1': tensor([0., 0., 0., 0.])}
agent.state_act= tensor([0., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.7042, 0.2958], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.7042, 0.2958], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 0., 0.])
agent.state_act= tensor([0., 1., 0., 0.])
states= {'agent_0': tensor([1., 0., 0., 0.]), 'agent_1': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.5247, 0.4753], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 0., 0.]), 'agent_1': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(1) tensor([0.6701, 0.3299], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(6.), 'agent_1': tensor(0.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(1)}
d= tensor([0.6701, 0.3299], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 0., 0., 1.])
agent.state_act= tensor([0., 0., 1., 0.])
states= {'agent_0': tensor([0., 0., 0., 1.]), 'agent_1': tensor([0., 0., 1., 0.])}
agent.state_act= tensor([0., 0., 0., 1.])
 action, distrib= tensor(1) tensor([0.5059, 0.4941], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 0., 1.]), 'agent_1': tensor([0., 0., 1., 0.])}
agent.state_act= tensor([0., 0., 1., 0.])
 action, distrib= tensor(0) tensor([0.7270, 0.2730], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.7270, 0.2730], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
UPDATE
inside update
inside update
done
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3499, 0.6501], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7199, 0.2801], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3499, 0.6501], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7199, 0.2801], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3499, 0.6501], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7199, 0.2801], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3499, 0.6501], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7199, 0.2801], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3499, 0.6501], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7199, 0.2801], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3499, 0.6501], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.7199, 0.2801], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3499, 0.6501], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7199, 0.2801], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3499, 0.6501], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7199, 0.2801], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3499, 0.6501], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.7199, 0.2801], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3499, 0.6501], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7199, 0.2801], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
R eval= {'agent_0': tensor(10.3488), 'agent_1': tensor(50.5965)}
==========>Epoch= 23
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3499, 0.6501], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7199, 0.2801], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.7199, 0.2801], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3693, 0.6307], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.8226, 0.1774], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(0)}
d= tensor([0.8226, 0.1774], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 0., 0., 1.])
agent.state_act= tensor([0., 0., 1., 0.])
states= {'agent_0': tensor([0., 0., 0., 1.]), 'agent_1': tensor([0., 0., 1., 0.])}
agent.state_act= tensor([0., 0., 0., 1.])
 action, distrib= tensor(1) tensor([0.5015, 0.4985], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 0., 1.]), 'agent_1': tensor([0., 0., 1., 0.])}
agent.state_act= tensor([0., 0., 1., 0.])
 action, distrib= tensor(0) tensor([0.7256, 0.2744], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.7256, 0.2744], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 0., 0.])
agent.state_act= tensor([0., 1., 0., 0.])
states= {'agent_0': tensor([1., 0., 0., 0.]), 'agent_1': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.5211, 0.4789], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 0., 0.]), 'agent_1': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(1) tensor([0.6705, 0.3295], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(6.), 'agent_1': tensor(0.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(1)}
d= tensor([0.6705, 0.3295], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 0., 0., 1.])
agent.state_act= tensor([0., 0., 1., 0.])
states= {'agent_0': tensor([0., 0., 0., 1.]), 'agent_1': tensor([0., 0., 1., 0.])}
agent.state_act= tensor([0., 0., 0., 1.])
 action, distrib= tensor(0) tensor([0.5015, 0.4985], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 0., 1.]), 'agent_1': tensor([0., 0., 1., 0.])}
agent.state_act= tensor([0., 0., 1., 0.])
 action, distrib= tensor(0) tensor([0.7256, 0.2744], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(0)}
d= tensor([0.7256, 0.2744], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 1., 1., 0.])
agent.state_act= tensor([1., 1., 0., 1.])
states= {'agent_0': tensor([1., 1., 1., 0.]), 'agent_1': tensor([1., 1., 0., 1.])}
agent.state_act= tensor([1., 1., 1., 0.])
 action, distrib= tensor(1) tensor([0.4101, 0.5899], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 1., 0.]), 'agent_1': tensor([1., 1., 0., 1.])}
agent.state_act= tensor([1., 1., 0., 1.])
 action, distrib= tensor(0) tensor([0.8048, 0.1952], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.8048, 0.1952], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 1., 0., 0.])
agent.state_act= tensor([1., 0., 0., 0.])
states= {'agent_0': tensor([0., 1., 0., 0.]), 'agent_1': tensor([1., 0., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.5597, 0.4403], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 0., 0.]), 'agent_1': tensor([1., 0., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.8289, 0.1711], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(0)}
d= tensor([0.8289, 0.1711], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 1., 0., 1.])
agent.state_act= tensor([1., 0., 1., 0.])
states= {'agent_0': tensor([0., 1., 0., 1.]), 'agent_1': tensor([1., 0., 1., 0.])}
agent.state_act= tensor([0., 1., 0., 1.])
 action, distrib= tensor(0) tensor([0.5188, 0.4812], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 0., 1.]), 'agent_1': tensor([1., 0., 1., 0.])}
agent.state_act= tensor([1., 0., 1., 0.])
 action, distrib= tensor(0) tensor([0.8381, 0.1619], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(0)}
d= tensor([0.8381, 0.1619], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 1., 0., 0.])
agent.state_act= tensor([1., 0., 0., 0.])
states= {'agent_0': tensor([0., 1., 0., 0.]), 'agent_1': tensor([1., 0., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(1) tensor([0.5597, 0.4403], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 0., 0.]), 'agent_1': tensor([1., 0., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.8289, 0.1711], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.8289, 0.1711], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 0., 0., 0.])
agent.state_act= tensor([0., 0., 0., 0.])
states= {'agent_0': tensor([0., 0., 0., 0.]), 'agent_1': tensor([0., 0., 0., 0.])}
agent.state_act= tensor([0., 0., 0., 0.])
 action, distrib= tensor(1) tensor([0.5562, 0.4438], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 0., 0.]), 'agent_1': tensor([0., 0., 0., 0.])}
agent.state_act= tensor([0., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.7028, 0.2972], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.7028, 0.2972], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
UPDATE
inside update
inside update
done
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3477, 0.6523], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.7230, 0.2770], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(6.), 'agent_1': tensor(0.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3477, 0.6523], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7230, 0.2770], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3477, 0.6523], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7230, 0.2770], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3477, 0.6523], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7230, 0.2770], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3477, 0.6523], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7230, 0.2770], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3477, 0.6523], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7230, 0.2770], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3477, 0.6523], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7230, 0.2770], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3477, 0.6523], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7230, 0.2770], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3477, 0.6523], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.7230, 0.2770], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(6.), 'agent_1': tensor(0.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3477, 0.6523], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7230, 0.2770], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
R eval= {'agent_0': tensor(15.3326), 'agent_1': tensor(26.8534)}
==========>Epoch= 24
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.7230, 0.2770], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3477, 0.6523], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
actions= {'agent_1': tensor(1), 'agent_0': tensor(1)}
d= tensor([0.3477, 0.6523], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.8457, 0.1543], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3954, 0.6046], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.3954, 0.6046], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.8457, 0.1543], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3954, 0.6046], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
actions= {'agent_1': tensor(1), 'agent_0': tensor(1)}
d= tensor([0.3954, 0.6046], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 1., 0., 0.])
agent.state_act= tensor([1., 0., 0., 0.])
states= {'agent_1': tensor([0., 1., 0., 0.]), 'agent_0': tensor([1., 0., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.6715, 0.3285], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 0., 0.]), 'agent_0': tensor([1., 0., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.5206, 0.4794], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.5206, 0.4794], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7230, 0.2770], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3477, 0.6523], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.3477, 0.6523], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 1., 0., 0.])
agent.state_act= tensor([1., 1., 0., 0.])
states= {'agent_1': tensor([1., 1., 0., 0.]), 'agent_0': tensor([1., 1., 0., 0.])}
agent.state_act= tensor([1., 1., 0., 0.])
 action, distrib= tensor(1) tensor([0.7988, 0.2012], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 0., 0.]), 'agent_0': tensor([1., 1., 0., 0.])}
agent.state_act= tensor([1., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.5296, 0.4704], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(0.), 'agent_0': tensor(6.)}
actions= {'agent_1': tensor(1), 'agent_0': tensor(0)}
d= tensor([0.5296, 0.4704], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 1., 1., 0.])
agent.state_act= tensor([1., 0., 0., 1.])
states= {'agent_1': tensor([0., 1., 1., 0.]), 'agent_0': tensor([1., 0., 0., 1.])}
agent.state_act= tensor([0., 1., 1., 0.])
 action, distrib= tensor(0) tensor([0.6990, 0.3010], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 0.]), 'agent_0': tensor([1., 0., 0., 1.])}
agent.state_act= tensor([1., 0., 0., 1.])
 action, distrib= tensor(0) tensor([0.4612, 0.5388], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.4612, 0.5388], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 1., 0., 1.])
agent.state_act= tensor([1., 0., 1., 0.])
states= {'agent_1': tensor([0., 1., 0., 1.]), 'agent_0': tensor([1., 0., 1., 0.])}
agent.state_act= tensor([0., 1., 0., 1.])
 action, distrib= tensor(0) tensor([0.6980, 0.3020], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 0., 1.]), 'agent_0': tensor([1., 0., 1., 0.])}
agent.state_act= tensor([1., 0., 1., 0.])
 action, distrib= tensor(0) tensor([0.3951, 0.6049], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.3951, 0.6049], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 1., 0., 0.])
agent.state_act= tensor([1., 0., 0., 0.])
states= {'agent_1': tensor([0., 1., 0., 0.]), 'agent_0': tensor([1., 0., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.6715, 0.3285], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 0., 0.]), 'agent_0': tensor([1., 0., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(1) tensor([0.5206, 0.4794], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.5206, 0.4794], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 1., 0., 0.])
agent.state_act= tensor([1., 1., 0., 0.])
states= {'agent_1': tensor([1., 1., 0., 0.]), 'agent_0': tensor([1., 1., 0., 0.])}
agent.state_act= tensor([1., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.7988, 0.2012], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 0., 0.]), 'agent_0': tensor([1., 1., 0., 0.])}
agent.state_act= tensor([1., 1., 0., 0.])
 action, distrib= tensor(1) tensor([0.5296, 0.4704], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.5296, 0.4704], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
UPDATE
inside update
inside update
done
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.8359, 0.1641], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3905, 0.6095], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.8359, 0.1641], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3905, 0.6095], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.8359, 0.1641], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3905, 0.6095], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.8359, 0.1641], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3905, 0.6095], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.8359, 0.1641], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3905, 0.6095], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.8359, 0.1641], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3905, 0.6095], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.8359, 0.1641], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3905, 0.6095], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.8359, 0.1641], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3905, 0.6095], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.8359, 0.1641], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3905, 0.6095], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.8359, 0.1641], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3905, 0.6095], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
R eval= {'agent_1': tensor(38.0502), 'agent_0': tensor(3.8641)}
==========>Epoch= 25
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3905, 0.6095], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.8359, 0.1641], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(6.), 'agent_1': tensor(0.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(1)}
d= tensor([0.8359, 0.1641], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3604, 0.6396], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.8117, 0.1883], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.8117, 0.1883], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 1., 1., 0.])
agent.state_act= tensor([1., 0., 0., 1.])
states= {'agent_0': tensor([0., 1., 1., 0.]), 'agent_1': tensor([1., 0., 0., 1.])}
agent.state_act= tensor([0., 1., 1., 0.])
 action, distrib= tensor(1) tensor([0.4341, 0.5659], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 0.]), 'agent_1': tensor([1., 0., 0., 1.])}
agent.state_act= tensor([1., 0., 0., 1.])
 action, distrib= tensor(1) tensor([0.8209, 0.1791], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(1)}
d= tensor([0.8209, 0.1791], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 0., 0., 1.])
agent.state_act= tensor([0., 1., 1., 0.])
states= {'agent_0': tensor([1., 0., 0., 1.]), 'agent_1': tensor([0., 1., 1., 0.])}
agent.state_act= tensor([1., 0., 0., 1.])
 action, distrib= tensor(1) tensor([0.4572, 0.5428], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 0., 1.]), 'agent_1': tensor([0., 1., 1., 0.])}
agent.state_act= tensor([0., 1., 1., 0.])
 action, distrib= tensor(1) tensor([0.6885, 0.3115], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(1)}
d= tensor([0.6885, 0.3115], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3905, 0.6095], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.8359, 0.1641], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.8359, 0.1641], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3696, 0.6304], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.7420, 0.2580], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(1)}
d= tensor([0.7420, 0.2580], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 0., 0., 1.])
agent.state_act= tensor([0., 0., 1., 0.])
states= {'agent_0': tensor([0., 0., 0., 1.]), 'agent_1': tensor([0., 0., 1., 0.])}
agent.state_act= tensor([0., 0., 0., 1.])
 action, distrib= tensor(0) tensor([0.4954, 0.5046], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 0., 1.]), 'agent_1': tensor([0., 0., 1., 0.])}
agent.state_act= tensor([0., 0., 1., 0.])
 action, distrib= tensor(0) tensor([0.7150, 0.2850], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(0)}
d= tensor([0.7150, 0.2850], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3604, 0.6396], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.8117, 0.1883], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(0)}
d= tensor([0.8117, 0.1883], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 0., 0., 0.])
agent.state_act= tensor([0., 0., 0., 0.])
states= {'agent_0': tensor([0., 0., 0., 0.]), 'agent_1': tensor([0., 0., 0., 0.])}
agent.state_act= tensor([0., 0., 0., 0.])
 action, distrib= tensor(1) tensor([0.5568, 0.4432], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 0., 0.]), 'agent_1': tensor([0., 0., 0., 0.])}
agent.state_act= tensor([0., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.6910, 0.3090], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.6910, 0.3090], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 0., 0.])
agent.state_act= tensor([0., 1., 0., 0.])
states= {'agent_0': tensor([1., 0., 0., 0.]), 'agent_1': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.5205, 0.4795], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 0., 0.]), 'agent_1': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.6613, 0.3387], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(0)}
d= tensor([0.6613, 0.3387], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
UPDATE
inside update
inside update
done
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3327, 0.6673], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.6897, 0.3103], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3327, 0.6673], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6897, 0.3103], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3327, 0.6673], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6897, 0.3103], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3327, 0.6673], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6897, 0.3103], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3327, 0.6673], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6897, 0.3103], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3327, 0.6673], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6897, 0.3103], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3327, 0.6673], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6897, 0.3103], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3327, 0.6673], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.6897, 0.3103], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3327, 0.6673], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.6897, 0.3103], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(6.), 'agent_1': tensor(0.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3327, 0.6673], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.6897, 0.3103], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
R eval= {'agent_0': tensor(22.6175), 'agent_1': tensor(34.7234)}
==========>Epoch= 26
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3327, 0.6673], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.6897, 0.3103], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(1)}
d= tensor([0.6897, 0.3103], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3841, 0.6159], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.8169, 0.1831], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(6.), 'agent_1': tensor(0.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(1)}
d= tensor([0.8169, 0.1831], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3519, 0.6481], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7904, 0.2096], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.7904, 0.2096], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 1., 1., 0.])
agent.state_act= tensor([1., 0., 0., 1.])
states= {'agent_0': tensor([0., 1., 1., 0.]), 'agent_1': tensor([1., 0., 0., 1.])}
agent.state_act= tensor([0., 1., 1., 0.])
 action, distrib= tensor(1) tensor([0.4311, 0.5689], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 0.]), 'agent_1': tensor([1., 0., 0., 1.])}
agent.state_act= tensor([1., 0., 0., 1.])
 action, distrib= tensor(1) tensor([0.8012, 0.1988], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(1)}
d= tensor([0.8012, 0.1988], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 0., 0., 1.])
agent.state_act= tensor([0., 1., 1., 0.])
states= {'agent_0': tensor([1., 0., 0., 1.]), 'agent_1': tensor([0., 1., 1., 0.])}
agent.state_act= tensor([1., 0., 0., 1.])
 action, distrib= tensor(1) tensor([0.4498, 0.5502], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 0., 1.]), 'agent_1': tensor([0., 1., 1., 0.])}
agent.state_act= tensor([0., 1., 1., 0.])
 action, distrib= tensor(0) tensor([0.6691, 0.3309], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.6691, 0.3309], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3519, 0.6481], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.7904, 0.2096], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(1)}
d= tensor([0.7904, 0.2096], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 1., 0., 1.])
agent.state_act= tensor([1., 1., 1., 0.])
states= {'agent_0': tensor([1., 1., 0., 1.]), 'agent_1': tensor([1., 1., 1., 0.])}
agent.state_act= tensor([1., 1., 0., 1.])
 action, distrib= tensor(1) tensor([0.4677, 0.5323], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 0., 1.]), 'agent_1': tensor([1., 1., 1., 0.])}
agent.state_act= tensor([1., 1., 1., 0.])
 action, distrib= tensor(0) tensor([0.7828, 0.2172], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.7828, 0.2172], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3841, 0.6159], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.8169, 0.1831], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(6.), 'agent_1': tensor(0.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(1)}
d= tensor([0.8169, 0.1831], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 1., 0., 1.])
agent.state_act= tensor([1., 1., 1., 0.])
states= {'agent_0': tensor([1., 1., 0., 1.]), 'agent_1': tensor([1., 1., 1., 0.])}
agent.state_act= tensor([1., 1., 0., 1.])
 action, distrib= tensor(0) tensor([0.4677, 0.5323], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 0., 1.]), 'agent_1': tensor([1., 1., 1., 0.])}
agent.state_act= tensor([1., 1., 1., 0.])
 action, distrib= tensor(1) tensor([0.7828, 0.2172], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(6.), 'agent_1': tensor(0.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(1)}
d= tensor([0.7828, 0.2172], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 0., 1., 0.])
agent.state_act= tensor([0., 1., 0., 1.])
states= {'agent_0': tensor([1., 0., 1., 0.]), 'agent_1': tensor([0., 1., 0., 1.])}
agent.state_act= tensor([1., 0., 1., 0.])
 action, distrib= tensor(1) tensor([0.3852, 0.6148], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 0.]), 'agent_1': tensor([0., 1., 0., 1.])}
agent.state_act= tensor([0., 1., 0., 1.])
 action, distrib= tensor(1) tensor([0.6662, 0.3338], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(1)}
d= tensor([0.6662, 0.3338], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
UPDATE
inside update
inside update
done
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3771, 0.6229], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.8014, 0.1986], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3771, 0.6229], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.8014, 0.1986], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3771, 0.6229], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.8014, 0.1986], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3771, 0.6229], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.8014, 0.1986], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3771, 0.6229], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.8014, 0.1986], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3771, 0.6229], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.8014, 0.1986], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3771, 0.6229], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.8014, 0.1986], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3771, 0.6229], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.8014, 0.1986], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3771, 0.6229], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.8014, 0.1986], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3771, 0.6229], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.8014, 0.1986], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
R eval= {'agent_0': tensor(2.8542), 'agent_1': tensor(43.0997)}
==========>Epoch= 27
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3771, 0.6229], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.8014, 0.1986], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.8014, 0.1986], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3552, 0.6448], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7049, 0.2951], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(0)}
d= tensor([0.7049, 0.2951], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 1., 0., 1.])
agent.state_act= tensor([1., 1., 1., 0.])
states= {'agent_0': tensor([1., 1., 0., 1.]), 'agent_1': tensor([1., 1., 1., 0.])}
agent.state_act= tensor([1., 1., 0., 1.])
 action, distrib= tensor(0) tensor([0.4592, 0.5408], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 0., 1.]), 'agent_1': tensor([1., 1., 1., 0.])}
agent.state_act= tensor([1., 1., 1., 0.])
 action, distrib= tensor(1) tensor([0.7679, 0.2321], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(6.), 'agent_1': tensor(0.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(1)}
d= tensor([0.7679, 0.2321], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 0., 0., 0.])
agent.state_act= tensor([0., 1., 0., 0.])
states= {'agent_0': tensor([1., 0., 0., 0.]), 'agent_1': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(1) tensor([0.5138, 0.4862], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 0., 0.]), 'agent_1': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.6305, 0.3695], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.6305, 0.3695], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 1., 1., 0.])
agent.state_act= tensor([1., 1., 0., 1.])
states= {'agent_0': tensor([1., 1., 1., 0.]), 'agent_1': tensor([1., 1., 0., 1.])}
agent.state_act= tensor([1., 1., 1., 0.])
 action, distrib= tensor(0) tensor([0.3943, 0.6057], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 1., 0.]), 'agent_1': tensor([1., 1., 0., 1.])}
agent.state_act= tensor([1., 1., 0., 1.])
 action, distrib= tensor(0) tensor([0.7554, 0.2446], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(0)}
d= tensor([0.7554, 0.2446], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 0., 0., 1.])
agent.state_act= tensor([0., 0., 1., 0.])
states= {'agent_0': tensor([0., 0., 0., 1.]), 'agent_1': tensor([0., 0., 1., 0.])}
agent.state_act= tensor([0., 0., 0., 1.])
 action, distrib= tensor(0) tensor([0.4854, 0.5146], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 0., 1.]), 'agent_1': tensor([0., 0., 1., 0.])}
agent.state_act= tensor([0., 0., 1., 0.])
 action, distrib= tensor(1) tensor([0.6821, 0.3179], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(6.), 'agent_1': tensor(0.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(1)}
d= tensor([0.6821, 0.3179], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 1., 0., 0.])
agent.state_act= tensor([1., 0., 0., 0.])
states= {'agent_0': tensor([0., 1., 0., 0.]), 'agent_1': tensor([1., 0., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.5597, 0.4403], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 0., 0.]), 'agent_1': tensor([1., 0., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(1) tensor([0.7818, 0.2182], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(6.), 'agent_1': tensor(0.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(1)}
d= tensor([0.7818, 0.2182], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 1., 1., 0.])
agent.state_act= tensor([1., 1., 0., 1.])
states= {'agent_0': tensor([1., 1., 1., 0.]), 'agent_1': tensor([1., 1., 0., 1.])}
agent.state_act= tensor([1., 1., 1., 0.])
 action, distrib= tensor(0) tensor([0.3943, 0.6057], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 1., 0.]), 'agent_1': tensor([1., 1., 0., 1.])}
agent.state_act= tensor([1., 1., 0., 1.])
 action, distrib= tensor(0) tensor([0.7554, 0.2446], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(0)}
d= tensor([0.7554, 0.2446], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 0., 1., 0.])
agent.state_act= tensor([0., 0., 0., 1.])
states= {'agent_0': tensor([0., 0., 1., 0.]), 'agent_1': tensor([0., 0., 0., 1.])}
agent.state_act= tensor([0., 0., 1., 0.])
 action, distrib= tensor(0) tensor([0.4190, 0.5810], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 0.]), 'agent_1': tensor([0., 0., 0., 1.])}
agent.state_act= tensor([0., 0., 0., 1.])
 action, distrib= tensor(1) tensor([0.6812, 0.3188], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(6.), 'agent_1': tensor(0.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(1)}
d= tensor([0.6812, 0.3188], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 1., 0., 0.])
agent.state_act= tensor([1., 0., 0., 0.])
states= {'agent_0': tensor([0., 1., 0., 0.]), 'agent_1': tensor([1., 0., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.5597, 0.4403], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 0., 0.]), 'agent_1': tensor([1., 0., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.7818, 0.2182], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(0)}
d= tensor([0.7818, 0.2182], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
UPDATE
inside update
inside update
done
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3673, 0.6327], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.7920, 0.2080], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3673, 0.6327], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7920, 0.2080], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3673, 0.6327], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7920, 0.2080], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3673, 0.6327], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.7920, 0.2080], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3673, 0.6327], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7920, 0.2080], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3673, 0.6327], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7920, 0.2080], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3673, 0.6327], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.7920, 0.2080], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3673, 0.6327], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7920, 0.2080], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3673, 0.6327], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7920, 0.2080], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3673, 0.6327], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7920, 0.2080], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
R eval= {'agent_0': tensor(16.4996), 'agent_1': tensor(44.7555)}
==========>Epoch= 28
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7920, 0.2080], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3673, 0.6327], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.3673, 0.6327], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.7920, 0.2080], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3673, 0.6327], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
actions= {'agent_1': tensor(1), 'agent_0': tensor(1)}
d= tensor([0.3673, 0.6327], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 1., 0., 0.])
agent.state_act= tensor([1., 0., 0., 0.])
states= {'agent_1': tensor([0., 1., 0., 0.]), 'agent_0': tensor([1., 0., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.6234, 0.3766], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 0., 0.]), 'agent_0': tensor([1., 0., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(1) tensor([0.5081, 0.4919], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.5081, 0.4919], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7637, 0.2363], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3322, 0.6678], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.3322, 0.6678], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 0., 1., 0.])
agent.state_act= tensor([0., 0., 0., 1.])
states= {'agent_1': tensor([0., 0., 1., 0.]), 'agent_0': tensor([0., 0., 0., 1.])}
agent.state_act= tensor([0., 0., 1., 0.])
 action, distrib= tensor(0) tensor([0.6749, 0.3251], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 0.]), 'agent_0': tensor([0., 0., 0., 1.])}
agent.state_act= tensor([0., 0., 0., 1.])
 action, distrib= tensor(1) tensor([0.4789, 0.5211], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.4789, 0.5211], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 1., 0., 0.])
agent.state_act= tensor([1., 0., 0., 0.])
states= {'agent_1': tensor([0., 1., 0., 0.]), 'agent_0': tensor([1., 0., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.6234, 0.3766], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 0., 0.]), 'agent_0': tensor([1., 0., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.5081, 0.4919], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.5081, 0.4919], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 1., 1., 0.])
agent.state_act= tensor([1., 0., 0., 1.])
states= {'agent_1': tensor([0., 1., 1., 0.]), 'agent_0': tensor([1., 0., 0., 1.])}
agent.state_act= tensor([0., 1., 1., 0.])
 action, distrib= tensor(0) tensor([0.6476, 0.3524], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 0.]), 'agent_0': tensor([1., 0., 0., 1.])}
agent.state_act= tensor([1., 0., 0., 1.])
 action, distrib= tensor(1) tensor([0.4328, 0.5672], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.4328, 0.5672], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 1., 0., 0.])
agent.state_act= tensor([1., 1., 0., 0.])
states= {'agent_1': tensor([1., 1., 0., 0.]), 'agent_0': tensor([1., 1., 0., 0.])}
agent.state_act= tensor([1., 1., 0., 0.])
 action, distrib= tensor(1) tensor([0.7416, 0.2584], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 0., 0.]), 'agent_0': tensor([1., 1., 0., 0.])}
agent.state_act= tensor([1., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.5166, 0.4834], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(0.), 'agent_0': tensor(6.)}
actions= {'agent_1': tensor(1), 'agent_0': tensor(0)}
d= tensor([0.5166, 0.4834], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 1., 1., 0.])
agent.state_act= tensor([1., 0., 0., 1.])
states= {'agent_1': tensor([0., 1., 1., 0.]), 'agent_0': tensor([1., 0., 0., 1.])}
agent.state_act= tensor([0., 1., 1., 0.])
 action, distrib= tensor(1) tensor([0.6476, 0.3524], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 0.]), 'agent_0': tensor([1., 0., 0., 1.])}
agent.state_act= tensor([1., 0., 0., 1.])
 action, distrib= tensor(1) tensor([0.4328, 0.5672], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
actions= {'agent_1': tensor(1), 'agent_0': tensor(1)}
d= tensor([0.4328, 0.5672], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 0., 0., 1.])
agent.state_act= tensor([0., 1., 1., 0.])
states= {'agent_1': tensor([1., 0., 0., 1.]), 'agent_0': tensor([0., 1., 1., 0.])}
agent.state_act= tensor([1., 0., 0., 1.])
 action, distrib= tensor(0) tensor([0.7758, 0.2242], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 0., 1.]), 'agent_0': tensor([0., 1., 1., 0.])}
agent.state_act= tensor([0., 1., 1., 0.])
 action, distrib= tensor(1) tensor([0.4194, 0.5806], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.4194, 0.5806], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
UPDATE
inside update
inside update
done
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6891, 0.3109], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3470, 0.6530], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6891, 0.3109], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3470, 0.6530], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6891, 0.3109], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3470, 0.6530], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.6891, 0.3109], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3470, 0.6530], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(0.), 'agent_0': tensor(6.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.6891, 0.3109], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3470, 0.6530], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6891, 0.3109], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3470, 0.6530], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6891, 0.3109], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3470, 0.6530], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.6891, 0.3109], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3470, 0.6530], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(0.), 'agent_0': tensor(6.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6891, 0.3109], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3470, 0.6530], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.6891, 0.3109], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3470, 0.6530], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
R eval= {'agent_1': tensor(34.7138), 'agent_0': tensor(22.6585)}
==========>Epoch= 29
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6891, 0.3109], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3470, 0.6530], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.3470, 0.6530], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.6592, 0.3408], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3139, 0.6861], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(0.), 'agent_0': tensor(6.)}
actions= {'agent_1': tensor(1), 'agent_0': tensor(0)}
d= tensor([0.3139, 0.6861], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 0., 1., 0.])
agent.state_act= tensor([0., 0., 0., 1.])
states= {'agent_1': tensor([0., 0., 1., 0.]), 'agent_0': tensor([0., 0., 0., 1.])}
agent.state_act= tensor([0., 0., 1., 0.])
 action, distrib= tensor(1) tensor([0.6701, 0.3299], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 0.]), 'agent_0': tensor([0., 0., 0., 1.])}
agent.state_act= tensor([0., 0., 0., 1.])
 action, distrib= tensor(0) tensor([0.4781, 0.5219], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(0.), 'agent_0': tensor(6.)}
actions= {'agent_1': tensor(1), 'agent_0': tensor(0)}
d= tensor([0.4781, 0.5219], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 0., 1.])
agent.state_act= tensor([0., 1., 1., 0.])
states= {'agent_1': tensor([1., 0., 0., 1.]), 'agent_0': tensor([0., 1., 1., 0.])}
agent.state_act= tensor([1., 0., 0., 1.])
 action, distrib= tensor(1) tensor([0.7662, 0.2338], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 0., 1.]), 'agent_0': tensor([0., 1., 1., 0.])}
agent.state_act= tensor([0., 1., 1., 0.])
 action, distrib= tensor(0) tensor([0.4205, 0.5795], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(0.), 'agent_0': tensor(6.)}
actions= {'agent_1': tensor(1), 'agent_0': tensor(0)}
d= tensor([0.4205, 0.5795], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 1., 0., 1.])
agent.state_act= tensor([1., 1., 1., 0.])
states= {'agent_1': tensor([1., 1., 0., 1.]), 'agent_0': tensor([1., 1., 1., 0.])}
agent.state_act= tensor([1., 1., 0., 1.])
 action, distrib= tensor(0) tensor([0.7365, 0.2635], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 0., 1.]), 'agent_0': tensor([1., 1., 1., 0.])}
agent.state_act= tensor([1., 1., 1., 0.])
 action, distrib= tensor(0) tensor([0.3843, 0.6157], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.3843, 0.6157], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 0., 0., 1.])
agent.state_act= tensor([0., 0., 1., 0.])
states= {'agent_1': tensor([0., 0., 0., 1.]), 'agent_0': tensor([0., 0., 1., 0.])}
agent.state_act= tensor([0., 0., 0., 1.])
 action, distrib= tensor(1) tensor([0.6665, 0.3335], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 0., 1.]), 'agent_0': tensor([0., 0., 1., 0.])}
agent.state_act= tensor([0., 0., 1., 0.])
 action, distrib= tensor(0) tensor([0.4128, 0.5872], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(0.), 'agent_0': tensor(6.)}
actions= {'agent_1': tensor(1), 'agent_0': tensor(0)}
d= tensor([0.4128, 0.5872], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 0., 0.])
agent.state_act= tensor([0., 1., 0., 0.])
states= {'agent_1': tensor([1., 0., 0., 0.]), 'agent_0': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.7645, 0.2355], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 0., 0.]), 'agent_0': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.5545, 0.4455], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.5545, 0.4455], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 0., 1.])
agent.state_act= tensor([0., 1., 1., 0.])
states= {'agent_1': tensor([1., 0., 0., 1.]), 'agent_0': tensor([0., 1., 1., 0.])}
agent.state_act= tensor([1., 0., 0., 1.])
 action, distrib= tensor(0) tensor([0.7662, 0.2338], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 0., 1.]), 'agent_0': tensor([0., 1., 1., 0.])}
agent.state_act= tensor([0., 1., 1., 0.])
 action, distrib= tensor(0) tensor([0.4205, 0.5795], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.4205, 0.5795], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 0., 0.])
agent.state_act= tensor([0., 1., 0., 0.])
states= {'agent_1': tensor([1., 0., 0., 0.]), 'agent_0': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.7645, 0.2355], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 0., 0.]), 'agent_0': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(1) tensor([0.5545, 0.4455], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.5545, 0.4455], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 0., 0., 0.])
agent.state_act= tensor([0., 0., 0., 0.])
states= {'agent_1': tensor([0., 0., 0., 0.]), 'agent_0': tensor([0., 0., 0., 0.])}
agent.state_act= tensor([0., 0., 0., 0.])
 action, distrib= tensor(1) tensor([0.6486, 0.3514], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 0., 0.]), 'agent_0': tensor([0., 0., 0., 0.])}
agent.state_act= tensor([0., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.5530, 0.4470], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(0.), 'agent_0': tensor(6.)}
actions= {'agent_1': tensor(1), 'agent_0': tensor(0)}
d= tensor([0.5530, 0.4470], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
UPDATE
inside update
inside update
done
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7770, 0.2230], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3692, 0.6308], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7770, 0.2230], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3692, 0.6308], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7770, 0.2230], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3692, 0.6308], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7770, 0.2230], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3692, 0.6308], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.7770, 0.2230], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3692, 0.6308], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7770, 0.2230], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3692, 0.6308], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7770, 0.2230], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3692, 0.6308], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7770, 0.2230], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3692, 0.6308], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7770, 0.2230], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3692, 0.6308], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7770, 0.2230], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3692, 0.6308], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
R eval= {'agent_1': tensor(46.7528), 'agent_0': tensor(6.7345)}
==========>Epoch= 30
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3692, 0.6308], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7770, 0.2230], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.7770, 0.2230], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3485, 0.6515], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6871, 0.3129], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.6871, 0.3129], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 0., 1.])
agent.state_act= tensor([0., 1., 1., 0.])
states= {'agent_0': tensor([1., 0., 0., 1.]), 'agent_1': tensor([0., 1., 1., 0.])}
agent.state_act= tensor([1., 0., 0., 1.])
 action, distrib= tensor(1) tensor([0.4318, 0.5682], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 0., 1.]), 'agent_1': tensor([0., 1., 1., 0.])}
agent.state_act= tensor([0., 1., 1., 0.])
 action, distrib= tensor(1) tensor([0.6462, 0.3538], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(1)}
d= tensor([0.6462, 0.3538], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 1., 0., 1.])
agent.state_act= tensor([1., 0., 1., 0.])
states= {'agent_0': tensor([0., 1., 0., 1.]), 'agent_1': tensor([1., 0., 1., 0.])}
agent.state_act= tensor([0., 1., 0., 1.])
 action, distrib= tensor(1) tensor([0.4959, 0.5041], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 0., 1.]), 'agent_1': tensor([1., 0., 1., 0.])}
agent.state_act= tensor([1., 0., 1., 0.])
 action, distrib= tensor(0) tensor([0.7721, 0.2279], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.7721, 0.2279], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3485, 0.6515], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6871, 0.3129], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.6871, 0.3129], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 0., 1.])
agent.state_act= tensor([0., 1., 1., 0.])
states= {'agent_0': tensor([1., 0., 0., 1.]), 'agent_1': tensor([0., 1., 1., 0.])}
agent.state_act= tensor([1., 0., 0., 1.])
 action, distrib= tensor(0) tensor([0.4318, 0.5682], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 0., 1.]), 'agent_1': tensor([0., 1., 1., 0.])}
agent.state_act= tensor([0., 1., 1., 0.])
 action, distrib= tensor(0) tensor([0.6462, 0.3538], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(0)}
d= tensor([0.6462, 0.3538], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 0., 1.])
agent.state_act= tensor([0., 1., 1., 0.])
states= {'agent_0': tensor([1., 0., 0., 1.]), 'agent_1': tensor([0., 1., 1., 0.])}
agent.state_act= tensor([1., 0., 0., 1.])
 action, distrib= tensor(1) tensor([0.4318, 0.5682], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 0., 1.]), 'agent_1': tensor([0., 1., 1., 0.])}
agent.state_act= tensor([0., 1., 1., 0.])
 action, distrib= tensor(1) tensor([0.6462, 0.3538], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(1)}
d= tensor([0.6462, 0.3538], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 1., 0., 0.])
agent.state_act= tensor([1., 0., 0., 0.])
states= {'agent_0': tensor([0., 1., 0., 0.]), 'agent_1': tensor([1., 0., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.5536, 0.4464], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 0., 0.]), 'agent_1': tensor([1., 0., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.7606, 0.2394], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(0)}
d= tensor([0.7606, 0.2394], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3692, 0.6308], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7770, 0.2230], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(0)}
d= tensor([0.7770, 0.2230], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 1., 0., 0.])
agent.state_act= tensor([1., 0., 0., 0.])
states= {'agent_0': tensor([0., 1., 0., 0.]), 'agent_1': tensor([1., 0., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(1) tensor([0.5536, 0.4464], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 0., 0.]), 'agent_1': tensor([1., 0., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.7606, 0.2394], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.7606, 0.2394], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
UPDATE
inside update
inside update
done
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3410, 0.6590], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6854, 0.3146], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3410, 0.6590], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6854, 0.3146], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3410, 0.6590], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6854, 0.3146], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3410, 0.6590], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.6854, 0.3146], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3410, 0.6590], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.6854, 0.3146], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3410, 0.6590], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.6854, 0.3146], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3410, 0.6590], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6854, 0.3146], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3410, 0.6590], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.6854, 0.3146], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3410, 0.6590], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.6854, 0.3146], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3410, 0.6590], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.6854, 0.3146], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
R eval= {'agent_0': tensor(30.2411), 'agent_1': tensor(41.7705)}
Epoch : 30 	 Measure: nan
==========>Epoch= 31
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3410, 0.6590], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.6854, 0.3146], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(1)}
d= tensor([0.6854, 0.3146], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3410, 0.6590], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6854, 0.3146], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.6854, 0.3146], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3086, 0.6914], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6571, 0.3429], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(0)}
d= tensor([0.6571, 0.3429], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 0., 1.])
agent.state_act= tensor([0., 1., 1., 0.])
states= {'agent_0': tensor([1., 0., 0., 1.]), 'agent_1': tensor([0., 1., 1., 0.])}
agent.state_act= tensor([1., 0., 0., 1.])
 action, distrib= tensor(1) tensor([0.4266, 0.5734], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 0., 1.]), 'agent_1': tensor([0., 1., 1., 0.])}
agent.state_act= tensor([0., 1., 1., 0.])
 action, distrib= tensor(0) tensor([0.6464, 0.3536], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.6464, 0.3536], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 1., 0., 0.])
agent.state_act= tensor([1., 1., 0., 0.])
states= {'agent_0': tensor([1., 1., 0., 0.]), 'agent_1': tensor([1., 1., 0., 0.])}
agent.state_act= tensor([1., 1., 0., 0.])
 action, distrib= tensor(1) tensor([0.5105, 0.4895], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 0., 0.]), 'agent_1': tensor([1., 1., 0., 0.])}
agent.state_act= tensor([1., 1., 0., 0.])
 action, distrib= tensor(1) tensor([0.7281, 0.2719], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(1)}
d= tensor([0.7281, 0.2719], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 1., 0., 1.])
agent.state_act= tensor([1., 1., 1., 0.])
states= {'agent_0': tensor([1., 1., 0., 1.]), 'agent_1': tensor([1., 1., 1., 0.])}
agent.state_act= tensor([1., 1., 0., 1.])
 action, distrib= tensor(0) tensor([0.4419, 0.5581], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 0., 1.]), 'agent_1': tensor([1., 1., 1., 0.])}
agent.state_act= tensor([1., 1., 1., 0.])
 action, distrib= tensor(0) tensor([0.7435, 0.2565], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(0)}
d= tensor([0.7435, 0.2565], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3410, 0.6590], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6854, 0.3146], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(0)}
d= tensor([0.6854, 0.3146], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 1., 0., 0.])
agent.state_act= tensor([1., 1., 0., 0.])
states= {'agent_0': tensor([1., 1., 0., 0.]), 'agent_1': tensor([1., 1., 0., 0.])}
agent.state_act= tensor([1., 1., 0., 0.])
 action, distrib= tensor(1) tensor([0.5105, 0.4895], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 0., 0.]), 'agent_1': tensor([1., 1., 0., 0.])}
agent.state_act= tensor([1., 1., 0., 0.])
 action, distrib= tensor(1) tensor([0.7281, 0.2719], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(1)}
d= tensor([0.7281, 0.2719], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 1., 0., 0.])
agent.state_act= tensor([1., 1., 0., 0.])
states= {'agent_0': tensor([1., 1., 0., 0.]), 'agent_1': tensor([1., 1., 0., 0.])}
agent.state_act= tensor([1., 1., 0., 0.])
 action, distrib= tensor(1) tensor([0.5105, 0.4895], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 0., 0.]), 'agent_1': tensor([1., 1., 0., 0.])}
agent.state_act= tensor([1., 1., 0., 0.])
 action, distrib= tensor(1) tensor([0.7281, 0.2719], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(1)}
d= tensor([0.7281, 0.2719], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3251, 0.6749], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7451, 0.2549], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.7451, 0.2549], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
UPDATE
inside update
inside update
done
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3543, 0.6457], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7670, 0.2330], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3543, 0.6457], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7670, 0.2330], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3543, 0.6457], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7670, 0.2330], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3543, 0.6457], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.7670, 0.2330], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3543, 0.6457], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7670, 0.2330], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3543, 0.6457], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7670, 0.2330], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3543, 0.6457], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.7670, 0.2330], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3543, 0.6457], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.7670, 0.2330], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(6.), 'agent_1': tensor(0.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3543, 0.6457], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7670, 0.2330], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3543, 0.6457], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7670, 0.2330], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
R eval= {'agent_0': tensor(18.0250), 'agent_1': tensor(35.4979)}
==========>Epoch= 32
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7670, 0.2330], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3543, 0.6457], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.3543, 0.6457], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.7670, 0.2330], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3543, 0.6457], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(0.), 'agent_0': tensor(6.)}
actions= {'agent_1': tensor(1), 'agent_0': tensor(0)}
d= tensor([0.3543, 0.6457], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 1., 0., 0.])
agent.state_act= tensor([1., 1., 0., 0.])
states= {'agent_1': tensor([1., 1., 0., 0.]), 'agent_0': tensor([1., 1., 0., 0.])}
agent.state_act= tensor([1., 1., 0., 0.])
 action, distrib= tensor(1) tensor([0.7273, 0.2727], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 0., 0.]), 'agent_0': tensor([1., 1., 0., 0.])}
agent.state_act= tensor([1., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.5093, 0.4907], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(0.), 'agent_0': tensor(6.)}
actions= {'agent_1': tensor(1), 'agent_0': tensor(0)}
d= tensor([0.5093, 0.4907], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 1., 0., 1.])
agent.state_act= tensor([1., 0., 1., 0.])
states= {'agent_1': tensor([0., 1., 0., 1.]), 'agent_0': tensor([1., 0., 1., 0.])}
agent.state_act= tensor([0., 1., 0., 1.])
 action, distrib= tensor(0) tensor([0.6380, 0.3620], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 0., 1.]), 'agent_0': tensor([1., 0., 1., 0.])}
agent.state_act= tensor([1., 0., 1., 0.])
 action, distrib= tensor(1) tensor([0.3602, 0.6398], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.3602, 0.6398], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 1., 0., 1.])
agent.state_act= tensor([1., 1., 1., 0.])
states= {'agent_1': tensor([1., 1., 0., 1.]), 'agent_0': tensor([1., 1., 1., 0.])}
agent.state_act= tensor([1., 1., 0., 1.])
 action, distrib= tensor(1) tensor([0.7244, 0.2756], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 0., 1.]), 'agent_0': tensor([1., 1., 1., 0.])}
agent.state_act= tensor([1., 1., 1., 0.])
 action, distrib= tensor(1) tensor([0.3729, 0.6271], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
actions= {'agent_1': tensor(1), 'agent_0': tensor(1)}
d= tensor([0.3729, 0.6271], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 1., 1., 0.])
agent.state_act= tensor([1., 1., 0., 1.])
states= {'agent_1': tensor([1., 1., 1., 0.]), 'agent_0': tensor([1., 1., 0., 1.])}
agent.state_act= tensor([1., 1., 1., 0.])
 action, distrib= tensor(0) tensor([0.7419, 0.2581], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 0.]), 'agent_0': tensor([1., 1., 0., 1.])}
agent.state_act= tensor([1., 1., 0., 1.])
 action, distrib= tensor(0) tensor([0.4389, 0.5611], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.4389, 0.5611], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6827, 0.3173], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3348, 0.6652], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.3348, 0.6652], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 1., 0., 0.])
agent.state_act= tensor([1., 1., 0., 0.])
states= {'agent_1': tensor([1., 1., 0., 0.]), 'agent_0': tensor([1., 1., 0., 0.])}
agent.state_act= tensor([1., 1., 0., 0.])
 action, distrib= tensor(1) tensor([0.7273, 0.2727], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 0., 0.]), 'agent_0': tensor([1., 1., 0., 0.])}
agent.state_act= tensor([1., 1., 0., 0.])
 action, distrib= tensor(1) tensor([0.5093, 0.4907], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
actions= {'agent_1': tensor(1), 'agent_0': tensor(1)}
d= tensor([0.5093, 0.4907], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 1., 0., 0.])
agent.state_act= tensor([1., 1., 0., 0.])
states= {'agent_1': tensor([1., 1., 0., 0.]), 'agent_0': tensor([1., 1., 0., 0.])}
agent.state_act= tensor([1., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.7273, 0.2727], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 0., 0.]), 'agent_0': tensor([1., 1., 0., 0.])}
agent.state_act= tensor([1., 1., 0., 0.])
 action, distrib= tensor(1) tensor([0.5093, 0.4907], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.5093, 0.4907], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7670, 0.2330], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3543, 0.6457], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.3543, 0.6457], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
UPDATE
inside update
inside update
done
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6744, 0.3256], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3392, 0.6608], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6744, 0.3256], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3392, 0.6608], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6744, 0.3256], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3392, 0.6608], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6744, 0.3256], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3392, 0.6608], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.6744, 0.3256], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3392, 0.6608], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(0.), 'agent_0': tensor(6.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6744, 0.3256], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3392, 0.6608], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.6744, 0.3256], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3392, 0.6608], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6744, 0.3256], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3392, 0.6608], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6744, 0.3256], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3392, 0.6608], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6744, 0.3256], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3392, 0.6608], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
R eval= {'agent_1': tensor(41.0103), 'agent_0': tensor(12.4021)}
==========>Epoch= 33
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3392, 0.6608], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6744, 0.3256], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.6744, 0.3256], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3086, 0.6914], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6504, 0.3496], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(0)}
d= tensor([0.6504, 0.3496], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 0., 1.])
agent.state_act= tensor([0., 1., 1., 0.])
states= {'agent_0': tensor([1., 0., 0., 1.]), 'agent_1': tensor([0., 1., 1., 0.])}
agent.state_act= tensor([1., 0., 0., 1.])
 action, distrib= tensor(0) tensor([0.4289, 0.5711], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 0., 1.]), 'agent_1': tensor([0., 1., 1., 0.])}
agent.state_act= tensor([0., 1., 1., 0.])
 action, distrib= tensor(0) tensor([0.6445, 0.3555], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(0)}
d= tensor([0.6445, 0.3555], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 0., 0.])
agent.state_act= tensor([0., 1., 0., 0.])
states= {'agent_0': tensor([1., 0., 0., 0.]), 'agent_1': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.5068, 0.4932], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 0., 0.]), 'agent_1': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(1) tensor([0.6268, 0.3732], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(6.), 'agent_1': tensor(0.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(1)}
d= tensor([0.6268, 0.3732], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 0., 0., 0.])
agent.state_act= tensor([0., 0., 0., 0.])
states= {'agent_0': tensor([0., 0., 0., 0.]), 'agent_1': tensor([0., 0., 0., 0.])}
agent.state_act= tensor([0., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.5510, 0.4490], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 0., 0.]), 'agent_1': tensor([0., 0., 0., 0.])}
agent.state_act= tensor([0., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.6474, 0.3526], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(0)}
d= tensor([0.6474, 0.3526], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 1., 1., 0.])
agent.state_act= tensor([1., 1., 0., 1.])
states= {'agent_0': tensor([1., 1., 1., 0.]), 'agent_1': tensor([1., 1., 0., 1.])}
agent.state_act= tensor([1., 1., 1., 0.])
 action, distrib= tensor(1) tensor([0.3819, 0.6181], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 1., 0.]), 'agent_1': tensor([1., 1., 0., 1.])}
agent.state_act= tensor([1., 1., 0., 1.])
 action, distrib= tensor(1) tensor([0.7145, 0.2855], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(1)}
d= tensor([0.7145, 0.2855], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 1., 0., 0.])
agent.state_act= tensor([1., 1., 0., 0.])
states= {'agent_0': tensor([1., 1., 0., 0.]), 'agent_1': tensor([1., 1., 0., 0.])}
agent.state_act= tensor([1., 1., 0., 0.])
 action, distrib= tensor(1) tensor([0.5185, 0.4815], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 0., 0.]), 'agent_1': tensor([1., 1., 0., 0.])}
agent.state_act= tensor([1., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.7196, 0.2804], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.7196, 0.2804], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3624, 0.6376], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7553, 0.2447], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.7553, 0.2447], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 0., 0., 1.])
agent.state_act= tensor([0., 0., 1., 0.])
states= {'agent_0': tensor([0., 0., 0., 1.]), 'agent_1': tensor([0., 0., 1., 0.])}
agent.state_act= tensor([0., 0., 0., 1.])
 action, distrib= tensor(1) tensor([0.4738, 0.5262], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 0., 1.]), 'agent_1': tensor([0., 0., 1., 0.])}
agent.state_act= tensor([0., 0., 1., 0.])
 action, distrib= tensor(0) tensor([0.6646, 0.3354], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.6646, 0.3354], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 0., 1.])
agent.state_act= tensor([0., 1., 1., 0.])
states= {'agent_0': tensor([1., 0., 0., 1.]), 'agent_1': tensor([0., 1., 1., 0.])}
agent.state_act= tensor([1., 0., 0., 1.])
 action, distrib= tensor(1) tensor([0.4289, 0.5711], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 0., 1.]), 'agent_1': tensor([0., 1., 1., 0.])}
agent.state_act= tensor([0., 1., 1., 0.])
 action, distrib= tensor(0) tensor([0.6445, 0.3555], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.6445, 0.3555], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
UPDATE
inside update
inside update
done
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3461, 0.6539], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7177, 0.2823], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3461, 0.6539], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7177, 0.2823], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3461, 0.6539], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7177, 0.2823], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3461, 0.6539], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.7177, 0.2823], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(6.), 'agent_1': tensor(0.)}
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3461, 0.6539], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.7177, 0.2823], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3461, 0.6539], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7177, 0.2823], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3461, 0.6539], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.7177, 0.2823], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3461, 0.6539], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7177, 0.2823], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3461, 0.6539], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7177, 0.2823], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3461, 0.6539], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7177, 0.2823], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
R eval= {'agent_0': tensor(19.1768), 'agent_1': tensor(30.4238)}
==========>Epoch= 34
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7177, 0.2823], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3461, 0.6539], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.3461, 0.6539], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7421, 0.2579], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3784, 0.6216], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.3784, 0.6216], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 1., 0.])
agent.state_act= tensor([0., 1., 0., 1.])
states= {'agent_1': tensor([1., 0., 1., 0.]), 'agent_0': tensor([0., 1., 0., 1.])}
agent.state_act= tensor([1., 0., 1., 0.])
 action, distrib= tensor(0) tensor([0.7430, 0.2570], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 0.]), 'agent_0': tensor([0., 1., 0., 1.])}
agent.state_act= tensor([0., 1., 0., 1.])
 action, distrib= tensor(1) tensor([0.5103, 0.4897], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.5103, 0.4897], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 0., 0., 0.])
agent.state_act= tensor([0., 0., 0., 0.])
states= {'agent_1': tensor([0., 0., 0., 0.]), 'agent_0': tensor([0., 0., 0., 0.])}
agent.state_act= tensor([0., 0., 0., 0.])
 action, distrib= tensor(1) tensor([0.6420, 0.3580], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 0., 0.]), 'agent_0': tensor([0., 0., 0., 0.])}
agent.state_act= tensor([0., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.5587, 0.4413], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(0.), 'agent_0': tensor(6.)}
actions= {'agent_1': tensor(1), 'agent_0': tensor(0)}
d= tensor([0.5587, 0.4413], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 1., 0.])
agent.state_act= tensor([0., 1., 0., 1.])
states= {'agent_1': tensor([1., 0., 1., 0.]), 'agent_0': tensor([0., 1., 0., 1.])}
agent.state_act= tensor([1., 0., 1., 0.])
 action, distrib= tensor(0) tensor([0.7430, 0.2570], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 0.]), 'agent_0': tensor([0., 1., 0., 1.])}
agent.state_act= tensor([0., 1., 0., 1.])
 action, distrib= tensor(0) tensor([0.5103, 0.4897], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.5103, 0.4897], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 0., 1.])
agent.state_act= tensor([0., 1., 1., 0.])
states= {'agent_1': tensor([1., 0., 0., 1.]), 'agent_0': tensor([0., 1., 1., 0.])}
agent.state_act= tensor([1., 0., 0., 1.])
 action, distrib= tensor(0) tensor([0.7286, 0.2714], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 0., 1.]), 'agent_0': tensor([0., 1., 1., 0.])}
agent.state_act= tensor([0., 1., 1., 0.])
 action, distrib= tensor(1) tensor([0.4307, 0.5693], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.4307, 0.5693], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 0., 0., 0.])
agent.state_act= tensor([0., 0., 0., 0.])
states= {'agent_1': tensor([0., 0., 0., 0.]), 'agent_0': tensor([0., 0., 0., 0.])}
agent.state_act= tensor([0., 0., 0., 0.])
 action, distrib= tensor(1) tensor([0.6420, 0.3580], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 0., 0.]), 'agent_0': tensor([0., 0., 0., 0.])}
agent.state_act= tensor([0., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.5587, 0.4413], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(0.), 'agent_0': tensor(6.)}
actions= {'agent_1': tensor(1), 'agent_0': tensor(0)}
d= tensor([0.5587, 0.4413], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 1., 0.])
agent.state_act= tensor([0., 1., 0., 1.])
states= {'agent_1': tensor([1., 0., 1., 0.]), 'agent_0': tensor([0., 1., 0., 1.])}
agent.state_act= tensor([1., 0., 1., 0.])
 action, distrib= tensor(0) tensor([0.7430, 0.2570], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 0.]), 'agent_0': tensor([0., 1., 0., 1.])}
agent.state_act= tensor([0., 1., 0., 1.])
 action, distrib= tensor(0) tensor([0.5103, 0.4897], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.5103, 0.4897], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 0., 1.])
agent.state_act= tensor([0., 1., 1., 0.])
states= {'agent_1': tensor([1., 0., 0., 1.]), 'agent_0': tensor([0., 1., 1., 0.])}
agent.state_act= tensor([1., 0., 0., 1.])
 action, distrib= tensor(0) tensor([0.7286, 0.2714], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 0., 1.]), 'agent_0': tensor([0., 1., 1., 0.])}
agent.state_act= tensor([0., 1., 1., 0.])
 action, distrib= tensor(1) tensor([0.4307, 0.5693], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.4307, 0.5693], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 0., 0., 0.])
agent.state_act= tensor([0., 0., 0., 0.])
states= {'agent_1': tensor([0., 0., 0., 0.]), 'agent_0': tensor([0., 0., 0., 0.])}
agent.state_act= tensor([0., 0., 0., 0.])
 action, distrib= tensor(1) tensor([0.6420, 0.3580], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 0., 0.]), 'agent_0': tensor([0., 0., 0., 0.])}
agent.state_act= tensor([0., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.5587, 0.4413], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(0.), 'agent_0': tensor(6.)}
actions= {'agent_1': tensor(1), 'agent_0': tensor(0)}
d= tensor([0.5587, 0.4413], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
UPDATE
inside update
inside update
done
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.7361, 0.2639], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3939, 0.6061], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7361, 0.2639], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3939, 0.6061], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7361, 0.2639], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3939, 0.6061], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7361, 0.2639], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3939, 0.6061], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7361, 0.2639], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3939, 0.6061], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7361, 0.2639], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3939, 0.6061], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7361, 0.2639], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3939, 0.6061], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.7361, 0.2639], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3939, 0.6061], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7361, 0.2639], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3939, 0.6061], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7361, 0.2639], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3939, 0.6061], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
R eval= {'agent_1': tensor(36.3161), 'agent_0': tensor(13.4848)}
==========>Epoch= 35
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7361, 0.2639], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3939, 0.6061], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.3939, 0.6061], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7361, 0.2639], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.3939, 0.6061], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.3939, 0.6061], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 0., 0.])
agent.state_act= tensor([0., 1., 0., 0.])
states= {'agent_1': tensor([1., 0., 0., 0.]), 'agent_0': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(1) tensor([0.7299, 0.2701], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 0., 0.]), 'agent_0': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(1) tensor([0.5761, 0.4239], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
actions= {'agent_1': tensor(1), 'agent_0': tensor(1)}
d= tensor([0.5761, 0.4239], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 1., 0., 0.])
agent.state_act= tensor([1., 0., 0., 0.])
states= {'agent_1': tensor([0., 1., 0., 0.]), 'agent_0': tensor([1., 0., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.6235, 0.3765], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 0., 0.]), 'agent_0': tensor([1., 0., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(1) tensor([0.5288, 0.4712], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.5288, 0.4712], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.7119, 0.2881], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.3632, 0.6368], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.3632, 0.6368], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 0., 1., 0.])
agent.state_act= tensor([0., 1., 0., 1.])
states= {'agent_1': tensor([1., 0., 1., 0.]), 'agent_0': tensor([0., 1., 0., 1.])}
agent.state_act= tensor([1., 0., 1., 0.])
 action, distrib= tensor(0) tensor([0.7382, 0.2618], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 0.]), 'agent_0': tensor([0., 1., 0., 1.])}
agent.state_act= tensor([0., 1., 0., 1.])
 action, distrib= tensor(0) tensor([0.5217, 0.4783], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.5217, 0.4783], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 1., 0.])
agent.state_act= tensor([0., 1., 0., 1.])
states= {'agent_1': tensor([1., 0., 1., 0.]), 'agent_0': tensor([0., 1., 0., 1.])}
agent.state_act= tensor([1., 0., 1., 0.])
 action, distrib= tensor(0) tensor([0.7382, 0.2618], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 0.]), 'agent_0': tensor([0., 1., 0., 1.])}
agent.state_act= tensor([0., 1., 0., 1.])
 action, distrib= tensor(1) tensor([0.5217, 0.4783], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.5217, 0.4783], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 0., 0., 0.])
agent.state_act= tensor([0., 0., 0., 0.])
states= {'agent_1': tensor([0., 0., 0., 0.]), 'agent_0': tensor([0., 0., 0., 0.])}
agent.state_act= tensor([0., 0., 0., 0.])
 action, distrib= tensor(1) tensor([0.6430, 0.3570], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 0., 0.]), 'agent_0': tensor([0., 0., 0., 0.])}
agent.state_act= tensor([0., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.5641, 0.4359], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(0.), 'agent_0': tensor(6.)}
actions= {'agent_1': tensor(1), 'agent_0': tensor(0)}
d= tensor([0.5641, 0.4359], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 1., 0.])
agent.state_act= tensor([0., 1., 0., 1.])
states= {'agent_1': tensor([1., 0., 1., 0.]), 'agent_0': tensor([0., 1., 0., 1.])}
agent.state_act= tensor([1., 0., 1., 0.])
 action, distrib= tensor(0) tensor([0.7382, 0.2618], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 0.]), 'agent_0': tensor([0., 1., 0., 1.])}
agent.state_act= tensor([0., 1., 0., 1.])
 action, distrib= tensor(1) tensor([0.5217, 0.4783], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.5217, 0.4783], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 0., 0., 1.])
agent.state_act= tensor([0., 0., 1., 0.])
states= {'agent_1': tensor([0., 0., 0., 1.]), 'agent_0': tensor([0., 0., 1., 0.])}
agent.state_act= tensor([0., 0., 0., 1.])
 action, distrib= tensor(0) tensor([0.6476, 0.3524], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 0., 1.]), 'agent_0': tensor([0., 0., 1., 0.])}
agent.state_act= tensor([0., 0., 1., 0.])
 action, distrib= tensor(1) tensor([0.4268, 0.5732], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.4268, 0.5732], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
UPDATE
inside update
inside update
done
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.6374, 0.3626], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3573, 0.6427], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.6374, 0.3626], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3573, 0.6427], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.6374, 0.3626], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3573, 0.6427], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6374, 0.3626], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3573, 0.6427], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6374, 0.3626], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3573, 0.6427], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6374, 0.3626], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3573, 0.6427], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.6374, 0.3626], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3573, 0.6427], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6374, 0.3626], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3573, 0.6427], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.6374, 0.3626], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3573, 0.6427], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(0.), 'agent_0': tensor(6.)}
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6374, 0.3626], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3573, 0.6427], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
R eval= {'agent_1': tensor(38.5521), 'agent_0': tensor(26.9685)}
==========>Epoch= 36
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3573, 0.6427], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.6374, 0.3626], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(1)}
d= tensor([0.6374, 0.3626], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.4185, 0.5815], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7290, 0.2710], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(0)}
d= tensor([0.7290, 0.2710], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.4185, 0.5815], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.7290, 0.2710], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(6.), 'agent_1': tensor(0.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(1)}
d= tensor([0.7290, 0.2710], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 1., 0., 0.])
agent.state_act= tensor([1., 1., 0., 0.])
states= {'agent_0': tensor([1., 1., 0., 0.]), 'agent_1': tensor([1., 1., 0., 0.])}
agent.state_act= tensor([1., 1., 0., 0.])
 action, distrib= tensor(1) tensor([0.5628, 0.4372], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 0., 0.]), 'agent_1': tensor([1., 1., 0., 0.])}
agent.state_act= tensor([1., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.7011, 0.2989], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.7011, 0.2989], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 1., 1., 0.])
agent.state_act= tensor([1., 0., 0., 1.])
states= {'agent_0': tensor([0., 1., 1., 0.]), 'agent_1': tensor([1., 0., 0., 1.])}
agent.state_act= tensor([0., 1., 1., 0.])
 action, distrib= tensor(0) tensor([0.4666, 0.5334], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 0.]), 'agent_1': tensor([1., 0., 0., 1.])}
agent.state_act= tensor([1., 0., 0., 1.])
 action, distrib= tensor(0) tensor([0.7174, 0.2826], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(0)}
d= tensor([0.7174, 0.2826], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 1., 0., 1.])
agent.state_act= tensor([1., 0., 1., 0.])
states= {'agent_0': tensor([0., 1., 0., 1.]), 'agent_1': tensor([1., 0., 1., 0.])}
agent.state_act= tensor([0., 1., 0., 1.])
 action, distrib= tensor(0) tensor([0.5384, 0.4616], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 0., 1.]), 'agent_1': tensor([1., 0., 1., 0.])}
agent.state_act= tensor([1., 0., 1., 0.])
 action, distrib= tensor(0) tensor([0.7323, 0.2677], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(0)}
d= tensor([0.7323, 0.2677], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 1., 0., 0.])
agent.state_act= tensor([1., 0., 0., 0.])
states= {'agent_0': tensor([0., 1., 0., 0.]), 'agent_1': tensor([1., 0., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(1) tensor([0.5876, 0.4124], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 0., 0.]), 'agent_1': tensor([1., 0., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(1) tensor([0.7247, 0.2753], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(1)}
d= tensor([0.7247, 0.2753], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 0., 0., 0.])
agent.state_act= tensor([0., 1., 0., 0.])
states= {'agent_0': tensor([1., 0., 0., 0.]), 'agent_1': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.5390, 0.4610], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 0., 0.]), 'agent_1': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.6245, 0.3755], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(0)}
d= tensor([0.6245, 0.3755], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.3573, 0.6427], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6374, 0.3626], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.6374, 0.3626], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 1., 0., 0.])
agent.state_act= tensor([1., 1., 0., 0.])
states= {'agent_0': tensor([1., 1., 0., 0.]), 'agent_1': tensor([1., 1., 0., 0.])}
agent.state_act= tensor([1., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.5628, 0.4372], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 0., 0.]), 'agent_1': tensor([1., 1., 0., 0.])}
agent.state_act= tensor([1., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.7011, 0.2989], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
actions= {'agent_0': tensor(0), 'agent_1': tensor(0)}
d= tensor([0.7011, 0.2989], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
UPDATE
inside update
inside update
done
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.4042, 0.5958], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.6499, 0.3501], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(6.), 'agent_1': tensor(0.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.4042, 0.5958], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6499, 0.3501], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.4042, 0.5958], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6499, 0.3501], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.4042, 0.5958], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.6499, 0.3501], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(6.), 'agent_1': tensor(0.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.4042, 0.5958], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6499, 0.3501], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.4042, 0.5958], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6499, 0.3501], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.4042, 0.5958], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6499, 0.3501], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(1.), 'agent_1': tensor(1.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.4042, 0.5958], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.6499, 0.3501], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.4042, 0.5958], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6499, 0.3501], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(0.), 'agent_1': tensor(6.)}
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.4042, 0.5958], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 1., 1.]), 'agent_1': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.6499, 0.3501], grad_fn=<SoftmaxBackward0>)
rew= {'agent_0': tensor(5.), 'agent_1': tensor(5.)}
R eval= {'agent_0': tensor(22.9812), 'agent_1': tensor(34.0460)}
==========>Epoch= 37
agent.state_act= tensor([0., 0., 1., 1.])
agent.state_act= tensor([0., 0., 1., 1.])
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.6499, 0.3501], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
agent.state_act= tensor([0., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.4042, 0.5958], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.4042, 0.5958], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 1., 1., 1.])
agent.state_act= tensor([1., 0., 1., 1.])
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6268, 0.3732], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.3773, 0.6227], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.3773, 0.6227], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([0., 1., 1., 0.])
agent.state_act= tensor([1., 0., 0., 1.])
states= {'agent_1': tensor([0., 1., 1., 0.]), 'agent_0': tensor([1., 0., 0., 1.])}
agent.state_act= tensor([0., 1., 1., 0.])
 action, distrib= tensor(1) tensor([0.6280, 0.3720], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 0.]), 'agent_0': tensor([1., 0., 0., 1.])}
agent.state_act= tensor([1., 0., 0., 1.])
 action, distrib= tensor(1) tensor([0.4850, 0.5150], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
actions= {'agent_1': tensor(1), 'agent_0': tensor(1)}
d= tensor([0.4850, 0.5150], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 0., 0., 0.])
agent.state_act= tensor([0., 1., 0., 0.])
states= {'agent_1': tensor([1., 0., 0., 0.]), 'agent_0': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.7149, 0.2851], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 0., 0.]), 'agent_0': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.5972, 0.4028], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.5972, 0.4028], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7162, 0.2838], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.4436, 0.5564], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.4436, 0.5564], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 0., 0., 0.])
agent.state_act= tensor([0., 0., 0., 0.])
states= {'agent_1': tensor([0., 0., 0., 0.]), 'agent_0': tensor([0., 0., 0., 0.])}
agent.state_act= tensor([0., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.6383, 0.3617], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 0., 0.]), 'agent_0': tensor([0., 0., 0., 0.])}
agent.state_act= tensor([0., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.5760, 0.4240], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.5760, 0.4240], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 1., 1., 0.])
agent.state_act= tensor([1., 1., 0., 1.])
states= {'agent_1': tensor([1., 1., 1., 0.]), 'agent_0': tensor([1., 1., 0., 1.])}
agent.state_act= tensor([1., 1., 1., 0.])
 action, distrib= tensor(0) tensor([0.6991, 0.3009], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 0.]), 'agent_0': tensor([1., 1., 0., 1.])}
agent.state_act= tensor([1., 1., 0., 1.])
 action, distrib= tensor(1) tensor([0.5198, 0.4802], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.5198, 0.4802], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([1., 0., 0., 0.])
agent.state_act= tensor([0., 1., 0., 0.])
states= {'agent_1': tensor([1., 0., 0., 0.]), 'agent_0': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.7149, 0.2851], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 0., 0.]), 'agent_0': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.5972, 0.4028], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.5972, 0.4028], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 1., 0.])
agent.state_act= tensor([0., 1., 0., 1.])
states= {'agent_1': tensor([1., 0., 1., 0.]), 'agent_0': tensor([0., 1., 0., 1.])}
agent.state_act= tensor([1., 0., 1., 0.])
 action, distrib= tensor(0) tensor([0.7215, 0.2785], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 0.]), 'agent_0': tensor([0., 1., 0., 1.])}
agent.state_act= tensor([0., 1., 0., 1.])
 action, distrib= tensor(0) tensor([0.5535, 0.4465], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.5535, 0.4465], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 0., 0.])
agent.state_act= tensor([0., 1., 0., 0.])
states= {'agent_1': tensor([1., 0., 0., 0.]), 'agent_0': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.7149, 0.2851], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 0., 0.]), 'agent_0': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.5972, 0.4028], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.5972, 0.4028], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
UPDATE
inside update
inside update
done
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.7026, 0.2974], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.4637, 0.5363], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(5.), 'agent_0': tensor(5.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7026, 0.2974], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.4637, 0.5363], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7026, 0.2974], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.4637, 0.5363], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7026, 0.2974], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.4637, 0.5363], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7026, 0.2974], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.4637, 0.5363], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7026, 0.2974], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.4637, 0.5363], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7026, 0.2974], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.4637, 0.5363], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7026, 0.2974], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.4637, 0.5363], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7026, 0.2974], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.4637, 0.5363], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.7026, 0.2974], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.4637, 0.5363], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(0.), 'agent_0': tensor(6.)}
R eval= {'agent_1': tensor(22.4003), 'agent_0': tensor(16.1790)}
==========>Epoch= 38
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7026, 0.2974], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.4637, 0.5363], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.4637, 0.5363], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 1., 1.])
agent.state_act= tensor([0., 1., 1., 1.])
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.7026, 0.2974], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 1., 1.]), 'agent_0': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.4637, 0.5363], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.4637, 0.5363], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 0., 0.])
agent.state_act= tensor([0., 1., 0., 0.])
states= {'agent_1': tensor([1., 0., 0., 0.]), 'agent_0': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.7032, 0.2968], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 0., 0.]), 'agent_0': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.6004, 0.3996], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.6004, 0.3996], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 0., 0.])
agent.state_act= tensor([0., 1., 0., 0.])
states= {'agent_1': tensor([1., 0., 0., 0.]), 'agent_0': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.7032, 0.2968], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 0., 0.]), 'agent_0': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.6004, 0.3996], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.6004, 0.3996], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 0., 0.])
agent.state_act= tensor([0., 1., 0., 0.])
states= {'agent_1': tensor([1., 0., 0., 0.]), 'agent_0': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.7032, 0.2968], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 0., 0.]), 'agent_0': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.6004, 0.3996], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.6004, 0.3996], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 0., 0.])
agent.state_act= tensor([0., 1., 0., 0.])
states= {'agent_1': tensor([1., 0., 0., 0.]), 'agent_0': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.7032, 0.2968], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 0., 0.]), 'agent_0': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.6004, 0.3996], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.6004, 0.3996], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 0., 0.])
agent.state_act= tensor([0., 1., 0., 0.])
states= {'agent_1': tensor([1., 0., 0., 0.]), 'agent_0': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.7032, 0.2968], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 0., 0.]), 'agent_0': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(0) tensor([0.6004, 0.3996], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
d= tensor([0.6004, 0.3996], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
agent.state_act= tensor([1., 0., 0., 0.])
agent.state_act= tensor([0., 1., 0., 0.])
states= {'agent_1': tensor([1., 0., 0., 0.]), 'agent_0': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([1., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.7032, 0.2968], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 0., 0., 0.]), 'agent_0': tensor([0., 1., 0., 0.])}
agent.state_act= tensor([0., 1., 0., 0.])
 action, distrib= tensor(1) tensor([0.6004, 0.3996], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.6004, 0.3996], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 0., 0., 0.])
agent.state_act= tensor([0., 0., 0., 0.])
states= {'agent_1': tensor([0., 0., 0., 0.]), 'agent_0': tensor([0., 0., 0., 0.])}
agent.state_act= tensor([0., 0., 0., 0.])
 action, distrib= tensor(0) tensor([0.6323, 0.3677], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 0., 0., 0.]), 'agent_0': tensor([0., 0., 0., 0.])}
agent.state_act= tensor([0., 0., 0., 0.])
 action, distrib= tensor(1) tensor([0.5767, 0.4233], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.5767, 0.4233], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
agent.state_act= tensor([0., 1., 1., 0.])
agent.state_act= tensor([1., 0., 0., 1.])
states= {'agent_1': tensor([0., 1., 1., 0.]), 'agent_0': tensor([1., 0., 0., 1.])}
agent.state_act= tensor([0., 1., 1., 0.])
 action, distrib= tensor(0) tensor([0.6185, 0.3815], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([0., 1., 1., 0.]), 'agent_0': tensor([1., 0., 0., 1.])}
agent.state_act= tensor([1., 0., 0., 1.])
 action, distrib= tensor(1) tensor([0.4940, 0.5060], grad_fn=<SoftmaxBackward0>)
rewards= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
d= tensor([0.4940, 0.5060], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
UPDATE
inside update
inside update
done
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6638, 0.3362], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.4618, 0.5382], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6638, 0.3362], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.4618, 0.5382], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(1.), 'agent_0': tensor(1.)}
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.6638, 0.3362], grad_fn=<SoftmaxBackward0>)
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.4618, 0.5382], grad_fn=<SoftmaxBackward0>)
rew= {'agent_1': tensor(6.), 'agent_0': tensor(0.)}
agent.state_act= tensor([1., 1., 1., 1.])
agent.state_act= tensor([1., 1., 1., 1.])
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0)
/home/nicole/marl-emecom/env1/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
/home/nicole/marl-emecom/env1/lib/python3.8/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars
  ret = ret.dtype.type(ret / rcount)
Traceback (most recent call last):
  File "caller.py", line 63, in <module>
    train_reinforce(args)
  File "/home/nicole/marl-emecom/src/experiments_anastassacos/train_reinforce.py", line 212, in train_reinforce
    objective(args, repo_name)
  File "/home/nicole/marl-emecom/src/experiments_anastassacos/train_reinforce.py", line 138, in objective
    returns_eval = eval_anast(parallel_env, active_agents, active_agents_idxs, config.num_game_iterations, social_norm, 0.99)
  File "/home/nicole/marl-emecom/src/utils/utils.py", line 92, in eval_anast
    a, d = active_agents[agent].select_action()#states[idx_agent])
  File "/home/nicole/marl-emecom/src/algos/anast/agent_anast.py", line 55, in select_action
    print(" action, distrib=", action, distrib)
  File "/home/nicole/marl-emecom/env1/lib/python3.8/site-packages/torch/_tensor.py", line 305, in __repr__
    return torch._tensor_str._str(self)
  File "/home/nicole/marl-emecom/env1/lib/python3.8/site-packages/torch/_tensor_str.py", line 434, in _str
    return _str_intern(self)
  File "/home/nicole/marl-emecom/env1/lib/python3.8/site-packages/torch/_tensor_str.py", line 409, in _str_intern
    tensor_str = _tensor_str(self, indent)
  File "/home/nicole/marl-emecom/env1/lib/python3.8/site-packages/torch/_tensor_str.py", line 264, in _tensor_str
    formatter = _Formatter(get_summarized_data(self) if summarize else self)
  File "/home/nicole/marl-emecom/env1/lib/python3.8/site-packages/torch/_tensor_str.py", line 100, in __init__
    nonzero_finite_vals = torch.masked_select(tensor_view, torch.isfinite(tensor_view) & tensor_view.ne(0))
KeyboardInterrupt