config= {'b_value': 5.0, 'c_value': 1.0, 'd_value': 0.0, 'n_agents': 2, 'algorithm': 'reinforce', 'wandb_mode': 'offline', 'num_game_iterations': 1, 'n_epochs': 2000, 'obs_size': 4, 'action_size': 2, 'random_baseline': False, 'lr_opponent': 0, 'embedding_dim': 1, 'binary_reputation': 1, 'opponent_selection': 0, 'other_reputation_threshold': 0.4, 'cooperation_threshold': 0.4, 'optuna_': 0, 'device': 'cpu', 'reputation_in_reward': 0, 'lr_actor': 0.01, 'n_hidden_act': 2, 'hidden_size_act': 16, 'batch_size': 1, 'decayRate': 0.999}
DD= tensor(1.)
Dc= tensor(6.)
Cd= tensor(0.)
CC= tensor(5.)
mv= tensor(6.)
mat= tensor([[1., 6.],
        [0., 5.]])
norm mat= tensor([[0.1667, 1.0000],
        [0.0000, 0.8333]])
is_dummy= [0, 1]
Agent 0
Normative agent 1
==========>Epoch= 0
Traceback (most recent call last):
  File "caller_optuna_anastassacos.py", line 64, in <module>
    training_function(args)
  File "/home/nicole/marl-emecom/src/experiments_pgg_v0/anastassacos/train_optuna_anastassacos.py", line 209, in training_function
    objective(args, repo_name)
  File "/home/nicole/marl-emecom/src/experiments_pgg_v0/anastassacos/train_optuna_anastassacos.py", line 88, in objective
    actions[idx_agent] = agents[idx_agent].select_action()
  File "/home/nicole/marl-emecom/src/algos/anastassacos/normativeagent.py", line 67, in select_action
    if (self.obs_m_fact > 1):# and self.obs_m_fact < 2): # if we are playing in a mixed-motive environment
AttributeError: 'NormativeAgent' object has no attribute 'obs_m_fact'