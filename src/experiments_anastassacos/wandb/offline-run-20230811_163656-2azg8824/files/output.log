config= {'b_value': 5.0, 'c_value': 1.0, 'd_value': 0.0, 'n_agents': 2, 'algorithm': 'reinforce', 'wandb_mode': 'offline', 'num_game_iterations': 10, 'n_epochs': 2000, 'obs_size': 4, 'action_size': 2, 'random_baseline': False, 'embedding_dim': 1, 'binary_reputation': 1, 'other_reputation_threshold': 0.4, 'cooperation_threshold': 0.4, 'optuna_': 0, 'device': 'cpu', 'reputation_in_reward': 0, 'lr_actor': 0.01, 'n_hidden_act': 2, 'hidden_size_act': 16, 'batch_size': 1, 'decayRate': 0.999}
DD= tensor(1.)
Dc= tensor(6.)
Cd= tensor(0.)
CC= tensor(5.)
mv= tensor(6.)
mat= tensor([[1., 6.],
        [0., 5.]])
norm mat= tensor([[0.1667, 1.0000],
        [0.0000, 0.8333]])
is_dummy= [0, 0]
Agent 0
Agent 1
==========>Epoch= 0
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.4383, 0.5617], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 1., 1.]), 'agent_1': tensor([1., 1., 1., 1.])}
agent.state_act= tensor([1., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.5333, 0.4667], grad_fn=<SoftmaxBackward0>)
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.5333, 0.4667], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(0) tensor([0.4744, 0.5256], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 1.]), 'agent_1': tensor([1., 0., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(1) tensor([0.5298, 0.4702], grad_fn=<SoftmaxBackward0>)
actions= {'agent_0': tensor(0), 'agent_1': tensor(1)}
d= tensor([0.5298, 0.4702], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
states= {'agent_0': tensor([1., 1., 0., 1.]), 'agent_1': tensor([1., 1., 1., 0.])}
agent.state_act= tensor([1., 1., 0., 1.])
 action, distrib= tensor(1) tensor([0.4251, 0.5749], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 1., 0., 1.]), 'agent_1': tensor([1., 1., 1., 0.])}
agent.state_act= tensor([1., 1., 1., 0.])
 action, distrib= tensor(0) tensor([0.5244, 0.4756], grad_fn=<SoftmaxBackward0>)
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.5244, 0.4756], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
states= {'agent_0': tensor([0., 1., 1., 0.]), 'agent_1': tensor([1., 0., 0., 1.])}
agent.state_act= tensor([0., 1., 1., 0.])
 action, distrib= tensor(1) tensor([0.4557, 0.5443], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 1., 1., 0.]), 'agent_1': tensor([1., 0., 0., 1.])}
agent.state_act= tensor([1., 0., 0., 1.])
 action, distrib= tensor(0) tensor([0.5239, 0.4761], grad_fn=<SoftmaxBackward0>)
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.5239, 0.4761], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
states= {'agent_0': tensor([0., 0., 0., 1.]), 'agent_1': tensor([0., 0., 1., 0.])}
agent.state_act= tensor([0., 0., 0., 1.])
 action, distrib= tensor(1) tensor([0.4648, 0.5352], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 0., 1.]), 'agent_1': tensor([0., 0., 1., 0.])}
agent.state_act= tensor([0., 0., 1., 0.])
 action, distrib= tensor(1) tensor([0.5584, 0.4416], grad_fn=<SoftmaxBackward0>)
actions= {'agent_0': tensor(1), 'agent_1': tensor(1)}
d= tensor([0.5584, 0.4416], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(1)]
states= {'agent_0': tensor([0., 0., 0., 1.]), 'agent_1': tensor([0., 0., 1., 0.])}
agent.state_act= tensor([0., 0., 0., 1.])
 action, distrib= tensor(1) tensor([0.4648, 0.5352], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 0., 1.]), 'agent_1': tensor([0., 0., 1., 0.])}
agent.state_act= tensor([0., 0., 1., 0.])
 action, distrib= tensor(0) tensor([0.5584, 0.4416], grad_fn=<SoftmaxBackward0>)
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.5584, 0.4416], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([1., 0., 1., 1.])
 action, distrib= tensor(0) tensor([0.4427, 0.5573], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 1.]), 'agent_1': tensor([0., 1., 1., 1.])}
agent.state_act= tensor([0., 1., 1., 1.])
 action, distrib= tensor(1) tensor([0.5719, 0.4281], grad_fn=<SoftmaxBackward0>)
actions= {'agent_0': tensor(0), 'agent_1': tensor(1)}
d= tensor([0.5719, 0.4281], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(1)]
states= {'agent_0': tensor([0., 0., 0., 1.]), 'agent_1': tensor([0., 0., 1., 0.])}
agent.state_act= tensor([0., 0., 0., 1.])
 action, distrib= tensor(1) tensor([0.4648, 0.5352], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([0., 0., 0., 1.]), 'agent_1': tensor([0., 0., 1., 0.])}
agent.state_act= tensor([0., 0., 1., 0.])
 action, distrib= tensor(0) tensor([0.5584, 0.4416], grad_fn=<SoftmaxBackward0>)
actions= {'agent_0': tensor(1), 'agent_1': tensor(0)}
d= tensor([0.5584, 0.4416], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(1)]
self.saved_actions[ag_idx]= [tensor(0)]
states= {'agent_0': tensor([1., 0., 1., 0.]), 'agent_1': tensor([0., 1., 0., 1.])}
agent.state_act= tensor([1., 0., 1., 0.])
 action, distrib= tensor(0) tensor([0.4262, 0.5738], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 1., 0.]), 'agent_1': tensor([0., 1., 0., 1.])}
agent.state_act= tensor([0., 1., 0., 1.])
 action, distrib= tensor(0) tensor([0.5651, 0.4349], grad_fn=<SoftmaxBackward0>)
actions= {'agent_0': tensor(0), 'agent_1': tensor(0)}
d= tensor([0.5651, 0.4349], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
states= {'agent_0': tensor([1., 0., 0., 1.]), 'agent_1': tensor([0., 1., 1., 0.])}
agent.state_act= tensor([1., 0., 0., 1.])
 action, distrib= tensor(0) tensor([0.4356, 0.5644], grad_fn=<SoftmaxBackward0>)
states= {'agent_0': tensor([1., 0., 0., 1.]), 'agent_1': tensor([0., 1., 1., 0.])}
agent.state_act= tensor([0., 1., 1., 0.])
 action, distrib= tensor(0) tensor([0.5640, 0.4360], grad_fn=<SoftmaxBackward0>)
actions= {'agent_0': tensor(0), 'agent_1': tensor(0)}
d= tensor([0.5640, 0.4360], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [tensor(0)]
self.saved_actions[ag_idx]= [tensor(0)]
UPDATE
inside update
inside update
done
 action, distrib= tensor(1) tensor([0.3681, 0.6319], grad_fn=<SoftmaxBackward0>)
 action, distrib= tensor(1) tensor([0.5617, 0.4383], grad_fn=<SoftmaxBackward0>)
self.saved_actions[ag_idx]= [(tensor(1), tensor([0.3681, 0.6319], grad_fn=<SoftmaxBackward0>))]
Traceback (most recent call last):
  File "caller.py", line 63, in <module>
    train_reinforce(args)
  File "/home/nicole/marl-emecom/src/experiments_anastassacos/train_reinforce.py", line 211, in train_reinforce
    objective(args, repo_name)
  File "/home/nicole/marl-emecom/src/experiments_anastassacos/train_reinforce.py", line 137, in objective
    returns_eval = eval_anast(parallel_env, active_agents, active_agents_idxs, config.num_game_iterations, social_norm, 0.99)
  File "/home/nicole/marl-emecom/src/utils/utils.py", line 92, in eval_anast
    social_norm.rule09_binary(active_agents_idxs)
  File "/home/nicole/marl-emecom/src/utils/social_norm.py", line 62, in rule09_binary
    avg_cooperation_level = np.mean(self.saved_actions[ag_idx])
  File "<__array_function__ internals>", line 180, in mean
  File "/home/nicole/marl-emecom/env1/lib/python3.8/site-packages/numpy/core/fromnumeric.py", line 3432, in mean
    return _methods._mean(a, axis=axis, dtype=dtype,
  File "/home/nicole/marl-emecom/env1/lib/python3.8/site-packages/numpy/core/_methods.py", line 164, in _mean
    arr = asanyarray(a)
  File "/home/nicole/marl-emecom/env1/lib/python3.8/site-packages/torch/_tensor.py", line 732, in __array__
    return self.numpy()
RuntimeError: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.