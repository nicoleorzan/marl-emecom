config= {'b_value': 5.0, 'c_value': 1.0, 'd_value': 0.0, 'n_agents': 2, 'algorithm': 'dqn', 'wandb_mode': 'offline', 'num_game_iterations': 4, 'n_epochs': 2000, 'obs_size': 4, 'action_size': 2, 'random_baseline': False, 'embedding_dim': 1, 'binary_reputation': 1, 'other_reputation_threshold': 0.4, 'cooperation_threshold': 0.4, 'optuna_': 0, 'device': 'cpu', 'reputation_in_reward': 0, 'memory_size': 500, 'n_hidden_act': 2, 'hidden_size_act': 16, 'batch_size': 5, 'lr_actor': 0.01, 'decayRate': 0.999}
DD= tensor(1.)
Dc= tensor(6.)
Cd= tensor(0.)
CC= tensor(5.)
mv= tensor(6.)
mat= tensor([[1., 6.],
        [0., 5.]])
norm mat= tensor([[0.1667, 1.0000],
        [0.0000, 0.8333]])
is_dummy= [0, 0]
==========>Epoch= 0
epsilon= 0.5
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
actions= {'agent_1': tensor(1), 'agent_0': tensor(1)}
next_states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
frame= 0
frame= 0
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
next_states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
frame= 1
frame= 1
states= {'agent_1': tensor([0., 0., 1., 1.]), 'agent_0': tensor([0., 0., 1., 1.])}
actions= {'agent_1': tensor(1), 'agent_0': tensor(0)}
next_states= {'agent_1': tensor([1., 0., 0., 0.]), 'agent_0': tensor([0., 1., 0., 0.])}
frame= 2
frame= 2
states= {'agent_1': tensor([1., 0., 0., 0.]), 'agent_0': tensor([0., 1., 0., 0.])}
actions= {'agent_1': tensor(1), 'agent_0': tensor(1)}
next_states= {'agent_1': None, 'agent_0': None}
frame= 3
frame= 3
Epoch : 0 	 Measure: 0
==========>Epoch= 1
epsilon= 0.49950034986670416
states= {'agent_1': tensor([0., 1., 1., 1.]), 'agent_0': tensor([1., 0., 1., 1.])}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
next_states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
frame= 4
frame= 4
states= {'agent_1': tensor([1., 1., 1., 1.]), 'agent_0': tensor([1., 1., 1., 1.])}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
next_states= {'agent_1': tensor([1., 0., 1., 0.]), 'agent_0': tensor([0., 1., 0., 1.])}
frame= 5
frame= 5
states= {'agent_1': tensor([1., 0., 1., 0.]), 'agent_0': tensor([0., 1., 0., 1.])}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
next_states= {'agent_1': tensor([0., 0., 1., 0.]), 'agent_0': tensor([0., 0., 0., 1.])}
frame= 6
frame= 6
states= {'agent_1': tensor([0., 0., 1., 0.]), 'agent_0': tensor([0., 0., 0., 1.])}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
next_states= {'agent_1': None, 'agent_0': None}
frame= 7
frame= 7
==========>Epoch= 2
epsilon= 0.4990011991337998
states= {'agent_1': tensor([0., 1., 1., 0.]), 'agent_0': tensor([1., 0., 0., 1.])}
actions= {'agent_1': tensor(0), 'agent_0': tensor(1)}
next_states= {'agent_1': tensor([1., 1., 1., 0.]), 'agent_0': tensor([1., 1., 0., 1.])}
frame= 8
frame= 8
states= {'agent_1': tensor([1., 1., 1., 0.]), 'agent_0': tensor([1., 1., 0., 1.])}
actions= {'agent_1': tensor(0), 'agent_0': tensor(0)}
next_states= {'agent_1': tensor([0., 0., 1., 0.]), 'agent_0': tensor([0., 0., 0., 1.])}
frame= 9
frame= 9
states= {'agent_1': tensor([0., 0., 1., 0.]), 'agent_0': tensor([0., 0., 0., 1.])}
actions= {'agent_1': tensor(1), 'agent_0': tensor(1)}
next_states= {'agent_1': tensor([0., 0., 0., 0.]), 'agent_0': tensor([0., 0., 0., 0.])}
frame= 10
===>UPDATING!
batch state= (tensor([0., 0., 1., 0.]), tensor([0., 0., 1., 1.]), tensor([0., 1., 1., 0.]), tensor([0., 1., 1., 1.]), tensor([1., 0., 0., 0.])) <class 'tuple'>
batch state= tensor([[0., 0., 1., 0.],
        [0., 0., 1., 1.],
        [0., 1., 1., 0.],
        [0., 1., 1., 1.],
        [1., 0., 0., 0.]])
batch action= tensor([[0],
        [1],
        [0],
        [0],
        [1]])
batch_reward= tensor([[7.],
        [0.],
        [7.],
        [7.],
        [6.]])
batch next state BEFORE= (None, tensor([1., 0., 0., 0.]), tensor([1., 1., 1., 0.]), tensor([1., 1., 1., 1.]), None)
Traceback (most recent call last):
  File "caller.py", line 61, in <module>
    train_dqn(args)
  File "/home/nicole/marl-emecom/src/experiments_anastassacos/train_dqn.py", line 211, in train_dqn
    objective(args, repo_name)
  File "/home/nicole/marl-emecom/src/experiments_anastassacos/train_dqn.py", line 114, in objective
    agent.update(states[ag_idx], actions[ag_idx], rewards[ag_idx], next_states[ag_idx], i + epoch*config.num_game_iterations)
  File "/home/nicole/marl-emecom/src/algos/anast/DQN_anast.py", line 158, in update
    batch_vars = self.prep_minibatch()
  File "/home/nicole/marl-emecom/src/algos/anast/DQN_anast.py", line 108, in prep_minibatch
    batch_next_state = torch.cat(batch_next_state).view(-1,self.obs_size)
TypeError: expected Tensor as element 0 in argument 0, but got NoneType