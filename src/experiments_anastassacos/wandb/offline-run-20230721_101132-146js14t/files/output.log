config= {'b_value': 5.0, 'c_value': 1.0, 'd_value': 0.0, 'n_agents': 2, 'algorithm': 'dqn', 'wandb_mode': 'offline', 'num_game_iterations': 10, 'n_epochs': 2000, 'obs_size': 4, 'action_size': 2, 'random_baseline': False, 'embedding_dim': 1, 'binary_reputation': 1, 'other_reputation_threshold': 0.4, 'cooperation_threshold': 0.4, 'optuna_': 0, 'device': 'cpu', 'reputation_in_reward': 0, 'memory_size': 500, 'n_hidden_act': 2, 'hidden_size_act': 16, 'batch_size': 128, 'lr_actor': 0.01, 'decayRate': 0.999}
DD= tensor(1.)
Dc= tensor(6.)
Cd= tensor(0.)
CC= tensor(5.)
mv= tensor(6.)
mat= tensor([[1., 6.],
        [0., 5.]])
norm mat= tensor([[0.1667, 1.0000],
        [0.0000, 0.8333]])
is_dummy= [0, 0]
==========>Epoch= 0
epsilon= 1.0
actions= {'agent_0': tensor([1]), 'agent_1': tensor([1])}
actions= {'agent_0': tensor([0]), 'agent_1': tensor([0])}
actions= {'agent_0': tensor([1]), 'agent_1': tensor([1])}
actions= {'agent_0': tensor([1]), 'agent_1': tensor([0])}
actions= {'agent_0': tensor([1]), 'agent_1': tensor([1])}
actions= {'agent_0': tensor([0]), 'agent_1': tensor([0])}
actions= {'agent_0': tensor([0]), 'agent_1': tensor([0])}
actions= {'agent_0': tensor([0]), 'agent_1': tensor([0])}
actions= {'agent_0': tensor([1]), 'agent_1': tensor([0])}
actions= {'agent_0': tensor([0]), 'agent_1': tensor([0])}
Epoch : 0 	 Measure: 0
==========>Epoch= 1
epsilon= 0.9950623544007555
actions= {'agent_0': tensor([1]), 'agent_1': tensor([0])}
actions= {'agent_0': tensor([1]), 'agent_1': tensor([1])}
actions= {'agent_0': tensor([1]), 'agent_1': tensor([1])}
actions= {'agent_0': tensor([0]), 'agent_1': tensor([1])}
actions= {'agent_0': tensor([1]), 'agent_1': tensor([1])}
actions= {'agent_0': tensor([1]), 'agent_1': tensor([0])}
actions= {'agent_0': tensor([1]), 'agent_1': tensor([1])}
actions= {'agent_0': tensor([0]), 'agent_1': tensor([1])}
actions= {'agent_0': tensor([0]), 'agent_1': tensor([0])}
actions= {'agent_0': tensor([0]), 'agent_1': tensor([0])}
==========>Epoch= 2
epsilon= 0.9901493354116764
actions= {'agent_0': tensor([0]), 'agent_1': tensor([0])}
actions= {'agent_0': tensor([1]), 'agent_1': tensor([0])}
actions= {'agent_0': tensor([1]), 'agent_1': tensor([1])}
actions= {'agent_0': tensor([0]), 'agent_1': tensor([0])}
actions= {'agent_0': tensor([0]), 'agent_1': tensor([0])}
actions= {'agent_0': tensor([0]), 'agent_1': tensor([1])}
actions= {'agent_0': tensor([0]), 'agent_1': tensor([1])}
actions= {'agent_0': tensor([1]), 'agent_1': tensor([1])}
actions= {'agent_0': tensor([1]), 'agent_1': tensor([1])}
actions= {'agent_0': tensor([0]), 'agent_1': tensor([0])}
==========>Epoch= 3
epsilon= 0.985260820207032
actions= {'agent_1': tensor([0]), 'agent_0': tensor([0])}
actions= {'agent_1': tensor([1]), 'agent_0': tensor([1])}
actions= {'agent_1': tensor([1]), 'agent_0': tensor([1])}
actions= {'agent_1': tensor([1]), 'agent_0': tensor([1])}
actions= {'agent_1': tensor([1]), 'agent_0': tensor([1])}
actions= {'agent_1': tensor([0]), 'agent_0': tensor([1])}
actions= {'agent_1': tensor([1]), 'agent_0': tensor([1])}
actions= {'agent_1': tensor([1]), 'agent_0': tensor([1])}
actions= {'agent_1': tensor([1]), 'agent_0': tensor([0])}
actions= {'agent_1': tensor([1]), 'agent_0': tensor([0])}
==========>Epoch= 4
epsilon= 0.9803966865736877
/home/nicole/marl-emecom/env1/lib/python3.8/site-packages/numpy/core/_methods.py:164: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  arr = asanyarray(a)
Traceback (most recent call last):
  File "caller.py", line 61, in <module>
    train_dqn(args)
  File "/home/nicole/marl-emecom/src/experiments_anastassacos/train_dqn.py", line 206, in train_dqn
    objective(args, repo_name)
  File "/home/nicole/marl-emecom/src/experiments_anastassacos/train_dqn.py", line 88, in objective
    actions[agent] = agents[agent].select_action(states[idx_agent], epsilon)
  File "/home/nicole/marl-emecom/src/algos/anast/DQN_anast.py", line 76, in select_action
    action, _, _ = self.policy_act.act(state)[0]
  File "/home/nicole/marl-emecom/env1/lib/python3.8/site-packages/torch/_tensor.py", line 698, in __iter__
    raise TypeError('iteration over a 0-d tensor')
TypeError: iteration over a 0-d tensor