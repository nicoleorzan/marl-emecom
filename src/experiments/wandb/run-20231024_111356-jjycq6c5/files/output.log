config= {'num_game_iterations': 1, 'n_epochs': 600, 'obs_size': 1, 'action_size': 2, 'random_baseline': False, 'wandb_mode': 'online', 'n_agents': 2, 'mult_fact': [0.5, 1.5, 2.5, 3.5], 'uncertainties': [0.0, 0.0], 'communicating_agents': [0, 0], 'listening_agents': [0, 0], 'gmm_': [0, 0], 'n_gmm_components': 0, 'batch_size': 64, 'algorithm': 'reinforce', 'lr_actor': 0.0174, 'lr_actor_comm': 0.0, 'n_hidden_act': 2, 'n_hidden_comm': 0, 'hidden_size_act': 8, 'hidden_size_comm': 0, 'mex_size': 0, 'sign_lambda': 0.0, 'list_lambda': 0.0, 'decayRate': 0.999}
Multiplier= 0.5
scenario= CC common_pot= 8 return= [2. 2.]
scenario= CD common_pot= 4 return= [1. 5.]
scenario= DC common_pot= 4 return= [5. 1.]
scenario= DD common_pot= 0.0 return= [4. 4.]
 max_values[ 0.5 ]= 5.0
 min_values[ 0.5 ]= 1.0
normalized= [[0.4 0.4]
 [0.2 1. ]
 [1.  0.2]
 [0.8 0.8]]
Multiplier= 1.5
scenario= CC common_pot= 8 return= [6. 6.]
scenario= CD common_pot= 4 return= [3. 7.]
scenario= DC common_pot= 4 return= [7. 3.]
scenario= DD common_pot= 0.0 return= [4. 4.]
 max_values[ 1.5 ]= 7.0
 min_values[ 1.5 ]= 3.0
normalized= [[0.85714286 0.85714286]
 [0.42857143 1.        ]
 [1.         0.42857143]
 [0.57142857 0.57142857]]
Multiplier= 2.5
scenario= CC common_pot= 8 return= [10. 10.]
scenario= CD common_pot= 4 return= [5. 9.]
scenario= DC common_pot= 4 return= [9. 5.]
scenario= DD common_pot= 0.0 return= [4. 4.]
 max_values[ 2.5 ]= 10.0
 min_values[ 2.5 ]= 4.0
normalized= [[1.  1. ]
 [0.5 0.9]
 [0.9 0.5]
 [0.4 0.4]]
Multiplier= 3.5
scenario= CC common_pot= 8 return= [14. 14.]
scenario= CD common_pot= 4 return= [ 7. 11.]
scenario= DC common_pot= 4 return= [11.  7.]
scenario= DD common_pot= 0.0 return= [4. 4.]
 max_values[ 3.5 ]= 14.0
 min_values[ 3.5 ]= 4.0
normalized= [[1.         1.        ]
 [0.5        0.78571429]
 [0.78571429 0.5       ]
 [0.28571429 0.28571429]]
Agent 0
is communicating?: 0
is listening?: 0
uncertainty: 0.0
gmm= 0
Agent 1
is communicating?: 0
is listening?: 0
uncertainty: 0.0
gmm= 0
Traceback (most recent call last):
  File "caller_given_params.py", line 60, in <module>
    training_function(args)
  File "/home/nicole/new_stuff/marl-emecom/src/experiments/train_given_params.py", line 204, in training_function
    train(args, repo_name)
  File "/home/nicole/new_stuff/marl-emecom/src/experiments/train_given_params.py", line 98, in train
    actions[agent] = agents[agent].select_action(m_val=mf.numpy()[0]) # m value is given only to compute metrics
  File "/home/nicole/new_stuff/marl-emecom/src/algos/Reinforce.py", line 95, in select_action
    action, action_logprob, entropy = self.act(self.policy_act, self.state_to_act, self.ent)
AttributeError: 'Reinforce' object has no attribute 'state_to_act'